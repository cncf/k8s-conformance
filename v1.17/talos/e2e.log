I1212 21:39:41.462281      22 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-860659511
I1212 21:39:41.462309      22 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1212 21:39:41.462428      22 e2e.go:109] Starting e2e run "d21fd6bb-ab5a-4057-b909-8ff7d7a1ba45" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576186779 - Will randomize all specs
Will run 280 of 4814 specs

Dec 12 21:39:41.475: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 21:39:41.478: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 12 21:39:41.497: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 12 21:39:41.536: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 12 21:39:41.536: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 12 21:39:41.536: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 12 21:39:41.547: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-apiserver' (0 seconds elapsed)
Dec 12 21:39:41.547: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
Dec 12 21:39:41.547: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 12 21:39:41.547: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'pod-checkpointer' (0 seconds elapsed)
Dec 12 21:39:41.547: INFO: e2e test version: v1.17.0
Dec 12 21:39:41.549: INFO: kube-apiserver version: v1.17.0
Dec 12 21:39:41.549: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 21:39:41.556: INFO: Cluster IP family: ipv4
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:39:41.557: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
Dec 12 21:39:41.621: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec 12 21:39:41.636: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1672
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 21:39:41.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1963'
Dec 12 21:39:42.278: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 21:39:42.278: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Dec 12 21:39:42.289: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 12 21:39:42.297: INFO: scanned /root for discovery docs: <nil>
Dec 12 21:39:42.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1963'
Dec 12 21:40:10.243: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 12 21:40:10.243: INFO: stdout: "Created e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce\nScaling up e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 12 21:40:10.243: INFO: stdout: "Created e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce\nScaling up e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 12 21:40:10.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-1963'
Dec 12 21:40:10.332: INFO: stderr: ""
Dec 12 21:40:10.332: INFO: stdout: "e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce-l5jrl "
Dec 12 21:40:10.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce-l5jrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1963'
Dec 12 21:40:10.418: INFO: stderr: ""
Dec 12 21:40:10.418: INFO: stdout: "true"
Dec 12 21:40:10.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce-l5jrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1963'
Dec 12 21:40:10.505: INFO: stderr: ""
Dec 12 21:40:10.505: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 12 21:40:10.505: INFO: e2e-test-httpd-rc-3b960656c1b2ed3886217eee98750bce-l5jrl is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1678
Dec 12 21:40:10.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete rc e2e-test-httpd-rc --namespace=kubectl-1963'
Dec 12 21:40:10.598: INFO: stderr: ""
Dec 12 21:40:10.598: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:40:10.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1963" for this suite.

• [SLOW TEST:29.056 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1667
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":1,"skipped":8,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:40:10.616: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-0ea78760-64e0-485b-9d99-4c23ca1e2c5f
STEP: Creating a pod to test consume secrets
Dec 12 21:40:10.798: INFO: Waiting up to 5m0s for pod "pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54" in namespace "secrets-5581" to be "success or failure"
Dec 12 21:40:10.804: INFO: Pod "pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.206025ms
Dec 12 21:40:12.808: INFO: Pod "pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010091831s
Dec 12 21:40:14.814: INFO: Pod "pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015905625s
STEP: Saw pod success
Dec 12 21:40:14.814: INFO: Pod "pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54" satisfied condition "success or failure"
Dec 12 21:40:14.818: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 21:40:14.860: INFO: Waiting for pod pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54 to disappear
Dec 12 21:40:14.863: INFO: Pod pod-secrets-6150bc7d-2669-41ce-be1f-7f543d535c54 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:40:14.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5581" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":2,"skipped":18,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:40:14.884: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:40:15.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-3690'
Dec 12 21:40:15.397: INFO: stderr: ""
Dec 12 21:40:15.397: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Dec 12 21:40:15.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-3690'
Dec 12 21:40:15.664: INFO: stderr: ""
Dec 12 21:40:15.664: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 12 21:40:16.669: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:16.669: INFO: Found 0 / 1
Dec 12 21:40:17.670: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:17.670: INFO: Found 0 / 1
Dec 12 21:40:18.669: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:18.669: INFO: Found 0 / 1
Dec 12 21:40:19.671: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:19.671: INFO: Found 0 / 1
Dec 12 21:40:20.672: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:20.672: INFO: Found 0 / 1
Dec 12 21:40:21.671: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:21.671: INFO: Found 0 / 1
Dec 12 21:40:22.671: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:22.671: INFO: Found 1 / 1
Dec 12 21:40:22.671: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 12 21:40:22.676: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 21:40:22.676: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 12 21:40:22.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 describe pod agnhost-master-km6kh --namespace=kubectl-3690'
Dec 12 21:40:22.783: INFO: stderr: ""
Dec 12 21:40:22.783: INFO: stdout: "Name:         agnhost-master-km6kh\nNamespace:    kubectl-3690\nPriority:     0\nNode:         talos-0-3-0-beta-0-gcp-workers-nt7qc/10.128.15.219\nStart Time:   Thu, 12 Dec 2019 21:40:15 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.244.4.5\nIPs:\n  IP:           10.244.4.5\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://b922f5e1eb7332928b095ab8eb0ae7d2cce0ee35832a901c9d9a42545f73ee4f\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Dec 2019 21:40:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-h6b7l (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-h6b7l:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-h6b7l\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                           Message\n  ----    ------     ----       ----                                           -------\n  Normal  Scheduled  <unknown>  default-scheduler                              Successfully assigned kubectl-3690/agnhost-master-km6kh to talos-0-3-0-beta-0-gcp-workers-nt7qc\n  Normal  Pulling    5s         kubelet, talos-0-3-0-beta-0-gcp-workers-nt7qc  Pulling image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\"\n  Normal  Pulled     3s         kubelet, talos-0-3-0-beta-0-gcp-workers-nt7qc  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\"\n  Normal  Created    1s         kubelet, talos-0-3-0-beta-0-gcp-workers-nt7qc  Created container agnhost-master\n  Normal  Started    1s         kubelet, talos-0-3-0-beta-0-gcp-workers-nt7qc  Started container agnhost-master\n"
Dec 12 21:40:22.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 describe rc agnhost-master --namespace=kubectl-3690'
Dec 12 21:40:22.898: INFO: stderr: ""
Dec 12 21:40:22.898: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-3690\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: agnhost-master-km6kh\n"
Dec 12 21:40:22.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 describe service agnhost-master --namespace=kubectl-3690'
Dec 12 21:40:23.001: INFO: stderr: ""
Dec 12 21:40:23.001: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-3690\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.96.79.152\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.4.5:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 12 21:40:23.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 describe node talos-0-3-0-beta-0-gcp-controlplane-0'
Dec 12 21:40:23.129: INFO: stderr: ""
Dec 12 21:40:23.129: INFO: stdout: "Name:               talos-0-3-0-beta-0-gcp-controlplane-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=talos-0-3-0-beta-0-gcp-controlplane-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"a2:d5:2c:eb:5a:8d\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.128.0.23\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 12 Dec 2019 21:35:43 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  talos-0-3-0-beta-0-gcp-controlplane-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 12 Dec 2019 21:40:20 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 12 Dec 2019 21:39:16 +0000   Thu, 12 Dec 2019 21:35:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 12 Dec 2019 21:39:16 +0000   Thu, 12 Dec 2019 21:35:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 12 Dec 2019 21:39:16 +0000   Thu, 12 Dec 2019 21:35:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 12 Dec 2019 21:39:16 +0000   Thu, 12 Dec 2019 21:36:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.128.0.23\n  Hostname:    talos-0-3-0-beta-0-gcp-controlplane-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  104329856Ki\n  hugepages-2Mi:      0\n  memory:             7626304Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  96150395131\n  hugepages-2Mi:      0\n  memory:             7523904Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 311936e2303b034fe7ef70182235b8cb\n  System UUID:                af29ccad-3725-0913-413d-d03f01898912\n  Boot ID:                    e7187b4e-7138-432d-9130-ccbc73dd795d\n  Kernel Version:             5.3.15-talos\n  OS Image:                   Talos (v0.3.0-beta.0)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.2\n  Kubelet Version:            v1.17.0\n  Kube-Proxy Version:         v1.17.0\nPodCIDR:                      10.244.2.0/24\nPodCIDRs:                     10.244.2.0/24\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                            ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-apiserver-9sv5v                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m20s\n  kube-system                 kube-flannel-69ftd                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m40s\n  kube-system                 kube-proxy-bkbtw                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m40s\n  kube-system                 pod-checkpointer-wbndz                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m20s\n  kube-system                 pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m8s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\nEvents:\n  Type     Reason                   Age                    From                                               Message\n  ----     ------                   ----                   ----                                               -------\n  Normal   Starting                 4m40s                  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     Starting kubelet.\n  Warning  InvalidDiskCapacity      4m40s                  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  4m40s (x2 over 4m40s)  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     Node talos-0-3-0-beta-0-gcp-controlplane-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    4m40s (x2 over 4m40s)  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     Node talos-0-3-0-beta-0-gcp-controlplane-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     4m40s (x2 over 4m40s)  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     Node talos-0-3-0-beta-0-gcp-controlplane-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  4m40s                  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     Updated Node Allocatable limit across pods\n  Normal   Starting                 4m38s                  kube-proxy, talos-0-3-0-beta-0-gcp-controlplane-0  Starting kube-proxy.\n  Normal   NodeReady                4m20s                  kubelet, talos-0-3-0-beta-0-gcp-controlplane-0     Node talos-0-3-0-beta-0-gcp-controlplane-0 status is now: NodeReady\n"
Dec 12 21:40:23.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 describe namespace kubectl-3690'
Dec 12 21:40:23.231: INFO: stderr: ""
Dec 12 21:40:23.231: INFO: stdout: "Name:         kubectl-3690\nLabels:       e2e-framework=kubectl\n              e2e-run=d21fd6bb-ab5a-4057-b909-8ff7d7a1ba45\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:40:23.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3690" for this suite.

• [SLOW TEST:8.364 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":3,"skipped":18,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:40:23.248: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:40:23.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-245" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":4,"skipped":30,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:40:23.449: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 21:40:23.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b" in namespace "downward-api-3032" to be "success or failure"
Dec 12 21:40:23.646: INFO: Pod "downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.596723ms
Dec 12 21:40:25.650: INFO: Pod "downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008127428s
Dec 12 21:40:27.656: INFO: Pod "downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013946923s
STEP: Saw pod success
Dec 12 21:40:27.656: INFO: Pod "downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b" satisfied condition "success or failure"
Dec 12 21:40:27.660: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b container client-container: <nil>
STEP: delete the pod
Dec 12 21:40:27.687: INFO: Waiting for pod downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b to disappear
Dec 12 21:40:27.691: INFO: Pod downwardapi-volume-9161857d-eaa7-4f2a-be6d-e222875e5e0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:40:27.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3032" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":5,"skipped":34,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:40:27.730: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
STEP: creating an pod
Dec 12 21:40:27.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-8939 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 12 21:40:28.021: INFO: stderr: ""
Dec 12 21:40:28.022: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Dec 12 21:40:28.022: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 12 21:40:28.022: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8939" to be "running and ready, or succeeded"
Dec 12 21:40:28.029: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.327227ms
Dec 12 21:40:30.033: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011782066s
Dec 12 21:40:32.038: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015982461s
Dec 12 21:40:34.042: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020217381s
Dec 12 21:40:36.046: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.024656014s
Dec 12 21:40:36.046: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 12 21:40:36.046: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 12 21:40:36.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs logs-generator logs-generator --namespace=kubectl-8939'
Dec 12 21:40:36.153: INFO: stderr: ""
Dec 12 21:40:36.153: INFO: stdout: "I1212 21:40:34.176697       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/4cd9 442\nI1212 21:40:34.376844       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ls5h 476\nI1212 21:40:34.576851       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/6n9x 536\nI1212 21:40:34.776846       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mlv 319\nI1212 21:40:34.976814       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vskb 272\nI1212 21:40:35.176837       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/q52 567\nI1212 21:40:35.376891       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/rmt2 285\nI1212 21:40:35.576847       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/gr5p 255\nI1212 21:40:35.776830       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/fzjx 538\nI1212 21:40:35.976840       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/fd5l 399\n"
STEP: limiting log lines
Dec 12 21:40:36.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs logs-generator logs-generator --namespace=kubectl-8939 --tail=1'
Dec 12 21:40:36.259: INFO: stderr: ""
Dec 12 21:40:36.259: INFO: stdout: "I1212 21:40:36.176823       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/jgk4 492\n"
Dec 12 21:40:36.259: INFO: got output "I1212 21:40:36.176823       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/jgk4 492\n"
STEP: limiting log bytes
Dec 12 21:40:36.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs logs-generator logs-generator --namespace=kubectl-8939 --limit-bytes=1'
Dec 12 21:40:36.359: INFO: stderr: ""
Dec 12 21:40:36.359: INFO: stdout: "I"
Dec 12 21:40:36.359: INFO: got output "I"
STEP: exposing timestamps
Dec 12 21:40:36.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs logs-generator logs-generator --namespace=kubectl-8939 --tail=1 --timestamps'
Dec 12 21:40:36.464: INFO: stderr: ""
Dec 12 21:40:36.464: INFO: stdout: "2019-12-12T21:40:36.376940324Z I1212 21:40:36.376816       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/r892 234\n"
Dec 12 21:40:36.464: INFO: got output "2019-12-12T21:40:36.376940324Z I1212 21:40:36.376816       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/r892 234\n"
STEP: restricting to a time range
Dec 12 21:40:38.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs logs-generator logs-generator --namespace=kubectl-8939 --since=1s'
Dec 12 21:40:39.060: INFO: stderr: ""
Dec 12 21:40:39.060: INFO: stdout: "I1212 21:40:38.176859       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/vmj2 314\nI1212 21:40:38.376874       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/h5fk 409\nI1212 21:40:38.576871       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/8qxh 591\nI1212 21:40:38.776879       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/7htj 484\nI1212 21:40:38.976826       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/6xmj 544\n"
Dec 12 21:40:39.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs logs-generator logs-generator --namespace=kubectl-8939 --since=24h'
Dec 12 21:40:39.183: INFO: stderr: ""
Dec 12 21:40:39.183: INFO: stdout: "I1212 21:40:34.176697       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/4cd9 442\nI1212 21:40:34.376844       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ls5h 476\nI1212 21:40:34.576851       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/6n9x 536\nI1212 21:40:34.776846       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mlv 319\nI1212 21:40:34.976814       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/vskb 272\nI1212 21:40:35.176837       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/q52 567\nI1212 21:40:35.376891       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/rmt2 285\nI1212 21:40:35.576847       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/gr5p 255\nI1212 21:40:35.776830       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/fzjx 538\nI1212 21:40:35.976840       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/fd5l 399\nI1212 21:40:36.176823       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/jgk4 492\nI1212 21:40:36.376816       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/r892 234\nI1212 21:40:36.576835       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8lnb 510\nI1212 21:40:36.776837       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/rw9 569\nI1212 21:40:36.976827       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/8sv8 319\nI1212 21:40:37.176859       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/95gj 544\nI1212 21:40:37.376902       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/pdkh 252\nI1212 21:40:37.576875       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/dlj6 466\nI1212 21:40:37.776845       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/j5w4 303\nI1212 21:40:37.976866       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/jwmz 393\nI1212 21:40:38.176859       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/vmj2 314\nI1212 21:40:38.376874       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/h5fk 409\nI1212 21:40:38.576871       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/8qxh 591\nI1212 21:40:38.776879       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/7htj 484\nI1212 21:40:38.976826       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/6xmj 544\nI1212 21:40:39.176879       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/jdd 457\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
Dec 12 21:40:39.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete pod logs-generator --namespace=kubectl-8939'
Dec 12 21:40:43.455: INFO: stderr: ""
Dec 12 21:40:43.455: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:40:43.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8939" for this suite.

• [SLOW TEST:15.743 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1440
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":6,"skipped":45,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:40:43.473: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:40:44.154: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 21:40:46.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783644, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783644, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783644, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783644, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:40:49.189: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:41:01.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4479" for this suite.
STEP: Destroying namespace "webhook-4479-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":7,"skipped":50,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:41:01.502: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-4stt
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 21:41:01.709: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4stt" in namespace "subpath-2959" to be "success or failure"
Dec 12 21:41:01.721: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Pending", Reason="", readiness=false. Elapsed: 11.687834ms
Dec 12 21:41:03.727: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017774231s
Dec 12 21:41:05.732: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 4.022194365s
Dec 12 21:41:07.736: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 6.0270938s
Dec 12 21:41:09.741: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 8.032107162s
Dec 12 21:41:11.746: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 10.036838363s
Dec 12 21:41:13.751: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 12.0419964s
Dec 12 21:41:15.756: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 14.046487425s
Dec 12 21:41:17.761: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 16.051375636s
Dec 12 21:41:19.765: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 18.056089525s
Dec 12 21:41:21.770: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 20.060889997s
Dec 12 21:41:23.775: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Running", Reason="", readiness=true. Elapsed: 22.065981118s
Dec 12 21:41:25.780: INFO: Pod "pod-subpath-test-configmap-4stt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.070631368s
STEP: Saw pod success
Dec 12 21:41:25.780: INFO: Pod "pod-subpath-test-configmap-4stt" satisfied condition "success or failure"
Dec 12 21:41:25.784: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-subpath-test-configmap-4stt container test-container-subpath-configmap-4stt: <nil>
STEP: delete the pod
Dec 12 21:41:25.806: INFO: Waiting for pod pod-subpath-test-configmap-4stt to disappear
Dec 12 21:41:25.810: INFO: Pod pod-subpath-test-configmap-4stt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4stt
Dec 12 21:41:25.810: INFO: Deleting pod "pod-subpath-test-configmap-4stt" in namespace "subpath-2959"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:41:25.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2959" for this suite.

• [SLOW TEST:24.322 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":8,"skipped":53,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:41:25.826: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 12 21:41:30.025: INFO: &Pod{ObjectMeta:{send-events-111e1c84-1abd-434d-908e-8d4058366b5c  events-879 /api/v1/namespaces/events-879/pods/send-events-111e1c84-1abd-434d-908e-8d4058366b5c f0165613-b938-452c-92f2-8171714ada92 2489 0 2019-12-12 21:41:26 +0000 UTC <nil> <nil> map[name:foo time:995935437] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jjgxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jjgxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jjgxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:10.244.4.9,StartTime:2019-12-12 21:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 21:41:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://6bc820b13c250a149f574909952635e89e9150cb74e5dca43bb6e5c8555f2e72,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 12 21:41:32.030: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 12 21:41:34.034: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:41:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-879" for this suite.

• [SLOW TEST:8.230 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":9,"skipped":62,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:41:34.056: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:41:35.075: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 21:41:37.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783695, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783695, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783695, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783695, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:41:40.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:41:40.110: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3037-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:41:41.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6800" for this suite.
STEP: Destroying namespace "webhook-6800-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":10,"skipped":63,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:41:41.606: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-a1a73656-b384-4c0c-88e5-1d602dc4e9e0
STEP: Creating a pod to test consume configMaps
Dec 12 21:41:41.881: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a" in namespace "projected-6399" to be "success or failure"
Dec 12 21:41:41.889: INFO: Pod "pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061939ms
Dec 12 21:41:43.893: INFO: Pod "pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012598458s
Dec 12 21:41:45.898: INFO: Pod "pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016843004s
STEP: Saw pod success
Dec 12 21:41:45.898: INFO: Pod "pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a" satisfied condition "success or failure"
Dec 12 21:41:45.901: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 21:41:45.925: INFO: Waiting for pod pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a to disappear
Dec 12 21:41:45.929: INFO: Pod pod-projected-configmaps-cf19c396-efc0-4fba-9bc9-0eb23656689a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:41:45.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6399" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":64,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:41:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6976
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 12 21:41:46.118: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2646 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 21:41:46.118: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2646 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 12 21:41:56.132: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2705 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 12 21:41:56.132: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2705 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 12 21:42:06.143: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2736 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 21:42:06.144: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2736 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 12 21:42:16.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2764 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 21:42:16.155: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-a 031d91ad-0aa6-433e-948c-1584db367b08 2764 0 2019-12-12 21:41:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 12 21:42:26.166: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-b 077a0244-e2b6-4aa9-9739-0296acf629e5 2799 0 2019-12-12 21:42:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 21:42:26.166: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-b 077a0244-e2b6-4aa9-9739-0296acf629e5 2799 0 2019-12-12 21:42:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 12 21:42:36.175: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-b 077a0244-e2b6-4aa9-9739-0296acf629e5 2829 0 2019-12-12 21:42:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 21:42:36.176: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976 /api/v1/namespaces/watch-6976/configmaps/e2e-watch-test-configmap-b 077a0244-e2b6-4aa9-9739-0296acf629e5 2829 0 2019-12-12 21:42:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:42:46.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6976" for this suite.

• [SLOW TEST:60.251 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":12,"skipped":66,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:42:46.196: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-7601/secret-test-456fbec1-5c27-4b26-a920-ecb820e4b098
STEP: Creating a pod to test consume secrets
Dec 12 21:42:46.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395" in namespace "secrets-7601" to be "success or failure"
Dec 12 21:42:46.423: INFO: Pod "pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395": Phase="Pending", Reason="", readiness=false. Elapsed: 6.986477ms
Dec 12 21:42:48.427: INFO: Pod "pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01079371s
Dec 12 21:42:50.430: INFO: Pod "pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014691156s
STEP: Saw pod success
Dec 12 21:42:50.431: INFO: Pod "pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395" satisfied condition "success or failure"
Dec 12 21:42:50.434: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395 container env-test: <nil>
STEP: delete the pod
Dec 12 21:42:50.456: INFO: Waiting for pod pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395 to disappear
Dec 12 21:42:50.460: INFO: Pod pod-configmaps-63e19302-3e2a-4eea-8e3a-cdfa812a2395 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:42:50.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7601" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":13,"skipped":73,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:42:50.476: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 12 21:42:58.709: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 21:42:58.714: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 21:43:00.714: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 21:43:00.719: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 21:43:02.714: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 21:43:02.720: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:43:02.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7737" for this suite.

• [SLOW TEST:12.260 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":85,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:43:02.737: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-54ce38b9-9cd3-457c-88b0-1604cba15772 in namespace container-probe-2000
Dec 12 21:43:06.953: INFO: Started pod liveness-54ce38b9-9cd3-457c-88b0-1604cba15772 in namespace container-probe-2000
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 21:43:06.957: INFO: Initial restart count of pod liveness-54ce38b9-9cd3-457c-88b0-1604cba15772 is 0
Dec 12 21:43:23.029: INFO: Restart count of pod container-probe-2000/liveness-54ce38b9-9cd3-457c-88b0-1604cba15772 is now 1 (16.071598178s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:43:23.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2000" for this suite.

• [SLOW TEST:20.320 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":15,"skipped":113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:43:23.060: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 21:43:27.285: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:43:27.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-778" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":16,"skipped":154,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:43:27.327: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:43:31.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7267" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":175,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:43:31.570: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2550
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:43:31.747: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Creating first CR 
Dec 12 21:43:32.351: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T21:43:32Z generation:1 name:name1 resourceVersion:3163 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a2e47ce4-2611-4a17-a954-abf3999794f8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 12 21:43:42.370: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T21:43:42Z generation:1 name:name2 resourceVersion:3211 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bee4bb23-1d21-4b18-bbdf-2299cc3e8435] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 12 21:43:52.379: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T21:43:32Z generation:2 name:name1 resourceVersion:3241 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a2e47ce4-2611-4a17-a954-abf3999794f8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 12 21:44:02.396: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T21:43:42Z generation:2 name:name2 resourceVersion:3271 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bee4bb23-1d21-4b18-bbdf-2299cc3e8435] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 12 21:44:12.408: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T21:43:32Z generation:2 name:name1 resourceVersion:3305 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a2e47ce4-2611-4a17-a954-abf3999794f8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 12 21:44:22.419: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T21:43:42Z generation:2 name:name2 resourceVersion:3339 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bee4bb23-1d21-4b18-bbdf-2299cc3e8435] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:44:32.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2550" for this suite.

• [SLOW TEST:61.378 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":18,"skipped":175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:44:32.956: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5180
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-6caa86ea-8766-4835-b22d-ca9900327e77
STEP: Creating a pod to test consume secrets
Dec 12 21:44:33.236: INFO: Waiting up to 5m0s for pod "pod-secrets-dc648daf-9391-4157-8195-82585afca6f2" in namespace "secrets-5180" to be "success or failure"
Dec 12 21:44:33.239: INFO: Pod "pod-secrets-dc648daf-9391-4157-8195-82585afca6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.654923ms
Dec 12 21:44:35.244: INFO: Pod "pod-secrets-dc648daf-9391-4157-8195-82585afca6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00808201s
Dec 12 21:44:37.248: INFO: Pod "pod-secrets-dc648daf-9391-4157-8195-82585afca6f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012261415s
STEP: Saw pod success
Dec 12 21:44:37.248: INFO: Pod "pod-secrets-dc648daf-9391-4157-8195-82585afca6f2" satisfied condition "success or failure"
Dec 12 21:44:37.252: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-secrets-dc648daf-9391-4157-8195-82585afca6f2 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 21:44:37.271: INFO: Waiting for pod pod-secrets-dc648daf-9391-4157-8195-82585afca6f2 to disappear
Dec 12 21:44:37.275: INFO: Pod pod-secrets-dc648daf-9391-4157-8195-82585afca6f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:44:37.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5180" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":19,"skipped":206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:44:37.292: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-8154
STEP: creating replication controller nodeport-test in namespace services-8154
I1212 21:44:37.488943      22 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-8154, replica count: 2
I1212 21:44:40.541200      22 runners.go:189] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 21:44:43.541473      22 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 21:44:43.541: INFO: Creating new exec pod
Dec 12 21:44:48.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-8154 execpod785lx -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 12 21:44:48.739: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 12 21:44:48.739: INFO: stdout: ""
Dec 12 21:44:48.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-8154 execpod785lx -- /bin/sh -x -c nc -zv -t -w 2 10.96.13.12 80'
Dec 12 21:44:48.912: INFO: stderr: "+ nc -zv -t -w 2 10.96.13.12 80\nConnection to 10.96.13.12 80 port [tcp/http] succeeded!\n"
Dec 12 21:44:48.912: INFO: stdout: ""
Dec 12 21:44:48.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-8154 execpod785lx -- /bin/sh -x -c nc -zv -t -w 2 10.128.0.23 31934'
Dec 12 21:44:49.072: INFO: stderr: "+ nc -zv -t -w 2 10.128.0.23 31934\nConnection to 10.128.0.23 31934 port [tcp/31934] succeeded!\n"
Dec 12 21:44:49.072: INFO: stdout: ""
Dec 12 21:44:49.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-8154 execpod785lx -- /bin/sh -x -c nc -zv -t -w 2 10.128.15.232 31934'
Dec 12 21:44:49.240: INFO: stderr: "+ nc -zv -t -w 2 10.128.15.232 31934\nConnection to 10.128.15.232 31934 port [tcp/31934] succeeded!\n"
Dec 12 21:44:49.240: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:44:49.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8154" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:11.960 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":20,"skipped":256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:44:49.254: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2561
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:44:49.413: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 12 21:44:52.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 create -f -'
Dec 12 21:44:53.229: INFO: stderr: ""
Dec 12 21:44:53.229: INFO: stdout: "e2e-test-crd-publish-openapi-2874-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 12 21:44:53.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 delete e2e-test-crd-publish-openapi-2874-crds test-foo'
Dec 12 21:44:53.327: INFO: stderr: ""
Dec 12 21:44:53.327: INFO: stdout: "e2e-test-crd-publish-openapi-2874-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 12 21:44:53.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 apply -f -'
Dec 12 21:44:53.662: INFO: stderr: ""
Dec 12 21:44:53.662: INFO: stdout: "e2e-test-crd-publish-openapi-2874-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 12 21:44:53.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 delete e2e-test-crd-publish-openapi-2874-crds test-foo'
Dec 12 21:44:53.764: INFO: stderr: ""
Dec 12 21:44:53.764: INFO: stdout: "e2e-test-crd-publish-openapi-2874-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 12 21:44:53.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 create -f -'
Dec 12 21:44:53.996: INFO: rc: 1
Dec 12 21:44:53.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 apply -f -'
Dec 12 21:44:54.229: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 12 21:44:54.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 create -f -'
Dec 12 21:44:54.495: INFO: rc: 1
Dec 12 21:44:54.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-2561 apply -f -'
Dec 12 21:44:54.743: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 12 21:44:54.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-2874-crds'
Dec 12 21:44:54.943: INFO: stderr: ""
Dec 12 21:44:54.943: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 12 21:44:54.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-2874-crds.metadata'
Dec 12 21:44:55.176: INFO: stderr: ""
Dec 12 21:44:55.177: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 12 21:44:55.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-2874-crds.spec'
Dec 12 21:44:55.425: INFO: stderr: ""
Dec 12 21:44:55.425: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 12 21:44:55.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-2874-crds.spec.bars'
Dec 12 21:44:55.671: INFO: stderr: ""
Dec 12 21:44:55.671: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 12 21:44:55.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-2874-crds.spec.bars2'
Dec 12 21:44:55.905: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:44:58.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2561" for this suite.

• [SLOW TEST:9.546 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":21,"skipped":282,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:44:58.801: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:44:59.424: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 21:45:01.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 21:45:03.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783899, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:45:06.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:45:06.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8306" for this suite.
STEP: Destroying namespace "webhook-8306-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.876 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":22,"skipped":286,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:45:06.682: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Dec 12 21:45:07.370: INFO: created pod pod-service-account-defaultsa
Dec 12 21:45:07.370: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 12 21:45:07.383: INFO: created pod pod-service-account-mountsa
Dec 12 21:45:07.383: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 12 21:45:07.397: INFO: created pod pod-service-account-nomountsa
Dec 12 21:45:07.397: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 12 21:45:07.409: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 12 21:45:07.409: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 12 21:45:07.423: INFO: created pod pod-service-account-mountsa-mountspec
Dec 12 21:45:07.423: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 12 21:45:07.433: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 12 21:45:07.434: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 12 21:45:07.442: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 12 21:45:07.442: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 12 21:45:07.454: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 12 21:45:07.454: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 12 21:45:07.478: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 12 21:45:07.478: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:45:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3153" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":23,"skipped":298,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:45:07.500: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:45:08.243: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 12 21:45:10.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 21:45:12.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711783908, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:45:15.272: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:45:15.276: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:45:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2371" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:9.367 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":24,"skipped":308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:45:16.868: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7558
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7558
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7558
Dec 12 21:45:17.089: INFO: Found 0 stateful pods, waiting for 1
Dec 12 21:45:27.095: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 12 21:45:27.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:45:27.373: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:45:27.373: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:45:27.373: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:45:27.379: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 12 21:45:37.386: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:45:37.386: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:45:37.408: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999312s
Dec 12 21:45:38.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995568416s
Dec 12 21:45:39.421: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988134537s
Dec 12 21:45:40.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981881351s
Dec 12 21:45:41.431: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976744702s
Dec 12 21:45:42.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971915959s
Dec 12 21:45:43.442: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.9662259s
Dec 12 21:45:44.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.961195156s
Dec 12 21:45:45.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.956299662s
Dec 12 21:45:46.458: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.298555ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7558
Dec 12 21:45:47.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:45:47.636: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 21:45:47.636: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:45:47.636: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:45:47.642: INFO: Found 1 stateful pods, waiting for 3
Dec 12 21:45:57.653: INFO: Found 2 stateful pods, waiting for 3
Dec 12 21:46:07.648: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:46:07.648: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:46:07.648: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 21:46:17.648: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:46:17.648: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:46:17.648: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 21:46:27.648: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:46:27.648: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:46:27.648: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 12 21:46:27.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:46:27.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:46:27.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:46:27.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:46:27.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:46:27.987: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:46:27.987: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:46:27.987: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:46:27.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:46:28.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:46:28.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:46:28.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:46:28.247: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:46:28.252: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 12 21:46:38.263: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:46:38.263: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:46:38.263: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:46:38.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999461s
Dec 12 21:46:39.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995925389s
Dec 12 21:46:40.307: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990007716s
Dec 12 21:46:41.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96963971s
Dec 12 21:46:42.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964413677s
Dec 12 21:46:43.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.959439987s
Dec 12 21:46:44.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953882197s
Dec 12 21:46:45.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948829354s
Dec 12 21:46:46.348: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944067699s
Dec 12 21:46:47.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.245023ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7558
Dec 12 21:46:48.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:46:48.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 21:46:48.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:46:48.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:46:48.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:46:48.681: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 21:46:48.681: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:46:48.681: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:46:48.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-7558 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:46:48.839: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 21:46:48.839: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:46:48.839: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:46:48.839: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 12 21:47:18.857: INFO: Deleting all statefulset in ns statefulset-7558
Dec 12 21:47:18.865: INFO: Scaling statefulset ss to 0
Dec 12 21:47:18.880: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:47:18.883: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:47:18.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7558" for this suite.

• [SLOW TEST:122.041 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":25,"skipped":352,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:47:18.910: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-a9e619de-c8bb-45e6-9c23-dbe162f080f5
STEP: Creating a pod to test consume secrets
Dec 12 21:47:19.094: INFO: Waiting up to 5m0s for pod "pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5" in namespace "secrets-470" to be "success or failure"
Dec 12 21:47:19.106: INFO: Pod "pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.658275ms
Dec 12 21:47:21.111: INFO: Pod "pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01687087s
Dec 12 21:47:23.117: INFO: Pod "pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022295779s
STEP: Saw pod success
Dec 12 21:47:23.117: INFO: Pod "pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5" satisfied condition "success or failure"
Dec 12 21:47:23.121: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5 container secret-env-test: <nil>
STEP: delete the pod
Dec 12 21:47:23.165: INFO: Waiting for pod pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5 to disappear
Dec 12 21:47:23.168: INFO: Pod pod-secrets-68078988-1474-488b-bcc0-07c3478d3cc5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:47:23.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-470" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":361,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:47:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-81c366aa-1dca-42bc-a057-602bc57cd96e
STEP: Creating a pod to test consume configMaps
Dec 12 21:47:23.363: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a" in namespace "projected-2568" to be "success or failure"
Dec 12 21:47:23.373: INFO: Pod "pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.509426ms
Dec 12 21:47:25.378: INFO: Pod "pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01484943s
Dec 12 21:47:27.382: INFO: Pod "pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019573652s
STEP: Saw pod success
Dec 12 21:47:27.383: INFO: Pod "pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a" satisfied condition "success or failure"
Dec 12 21:47:27.386: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 21:47:27.450: INFO: Waiting for pod pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a to disappear
Dec 12 21:47:27.454: INFO: Pod pod-projected-configmaps-a51b0fd1-941d-4df1-88ee-2ee8a6af358a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:47:27.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2568" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":361,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:47:27.467: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:47:28.460: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 21:47:30.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784048, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784048, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784048, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784048, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:47:33.521: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:47:33.526: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:47:34.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-137" for this suite.
STEP: Destroying namespace "webhook-137-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.348 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":28,"skipped":363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:47:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6337.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6337.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 21:47:57.115: INFO: DNS probes using dns-6337/dns-test-6add3272-10d2-409c-88f8-fd7056aff060 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:47:57.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6337" for this suite.

• [SLOW TEST:22.336 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":29,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:47:57.155: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:47:57.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8159" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":30,"skipped":451,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:47:57.375: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 12 21:48:02.167: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2355 pod-service-account-bf997af8-6512-47b4-89d0-5d9580b1b050 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 12 21:48:02.328: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2355 pod-service-account-bf997af8-6512-47b4-89d0-5d9580b1b050 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 12 21:48:02.522: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2355 pod-service-account-bf997af8-6512-47b4-89d0-5d9580b1b050 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:48:02.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2355" for this suite.

• [SLOW TEST:5.248 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":31,"skipped":462,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:48:02.699: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3834
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-6e9cc736-a86d-483f-ae2a-d0ad440c9208
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-6e9cc736-a86d-483f-ae2a-d0ad440c9208
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:29.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3834" for this suite.

• [SLOW TEST:86.643 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":32,"skipped":466,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:29.342: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:29.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9400" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 12 21:49:29.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9452 /api/v1/namespaces/watch-9452/configmaps/e2e-watch-test-resource-version e8319a18-b38d-4558-8622-3488013fcf85 5085 0 2019-12-12 21:49:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 21:49:29.840: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9452 /api/v1/namespaces/watch-9452/configmaps/e2e-watch-test-resource-version e8319a18-b38d-4558-8622-3488013fcf85 5086 0 2019-12-12 21:49:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:29.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9452" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":34,"skipped":530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:29.854: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 12 21:49:38.121: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 12 21:49:38.125: INFO: Pod pod-with-prestop-http-hook still exists
Dec 12 21:49:40.126: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 12 21:49:40.130: INFO: Pod pod-with-prestop-http-hook still exists
Dec 12 21:49:42.126: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 12 21:49:42.131: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:42.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8583" for this suite.

• [SLOW TEST:12.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":579,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:42.155: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7024
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 12 21:49:48.354: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7024 PodName:pod-sharedvolume-a516f866-ec63-4fd5-954f-9a4d213451cb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 21:49:48.354: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 21:49:48.427: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:48.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7024" for this suite.

• [SLOW TEST:6.286 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":36,"skipped":586,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:48.442: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:52.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3204" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":37,"skipped":590,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:52.650: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:49:53.492: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 21:49:55.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784193, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784193, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784193, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784193, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:49:58.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:49:58.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1313" for this suite.
STEP: Destroying namespace "webhook-1313-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.135 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":38,"skipped":596,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:49:58.786: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Dec 12 21:50:03.037: INFO: Pod pod-hostip-83afc108-e0c4-4ad5-a77e-a2792bc6eee3 has hostIP: 10.128.15.231
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:50:03.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-834" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":39,"skipped":599,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:50:03.049: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7188
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7188
STEP: Creating statefulset with conflicting port in namespace statefulset-7188
STEP: Waiting until pod test-pod will start running in namespace statefulset-7188
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7188
Dec 12 21:50:23.266: INFO: Observed stateful pod in namespace: statefulset-7188, name: ss-0, uid: 7d167d51-64d4-4ae1-bbe3-e5060b6e8924, status phase: Pending. Waiting for statefulset controller to delete.
Dec 12 21:50:23.431: INFO: Observed stateful pod in namespace: statefulset-7188, name: ss-0, uid: 7d167d51-64d4-4ae1-bbe3-e5060b6e8924, status phase: Failed. Waiting for statefulset controller to delete.
Dec 12 21:50:23.442: INFO: Observed stateful pod in namespace: statefulset-7188, name: ss-0, uid: 7d167d51-64d4-4ae1-bbe3-e5060b6e8924, status phase: Failed. Waiting for statefulset controller to delete.
Dec 12 21:50:23.453: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7188
STEP: Removing pod with conflicting port in namespace statefulset-7188
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7188 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 12 21:50:27.492: INFO: Deleting all statefulset in ns statefulset-7188
Dec 12 21:50:27.498: INFO: Scaling statefulset ss to 0
Dec 12 21:50:37.517: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:50:37.521: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:50:37.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7188" for this suite.

• [SLOW TEST:34.505 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":40,"skipped":611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:50:37.558: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:51:08.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3869" for this suite.

• [SLOW TEST:30.489 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":640,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:51:08.047: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 12 21:51:16.252: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-860659511 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 12 21:51:21.376: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:51:21.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8051" for this suite.

• [SLOW TEST:13.349 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":42,"skipped":641,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:51:21.396: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:51:21.574: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-6972985f-f08a-4545-b1f5-84ba148abdb6" in namespace "security-context-test-2955" to be "success or failure"
Dec 12 21:51:21.578: INFO: Pod "alpine-nnp-false-6972985f-f08a-4545-b1f5-84ba148abdb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.17815ms
Dec 12 21:51:23.582: INFO: Pod "alpine-nnp-false-6972985f-f08a-4545-b1f5-84ba148abdb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008546906s
Dec 12 21:51:25.587: INFO: Pod "alpine-nnp-false-6972985f-f08a-4545-b1f5-84ba148abdb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013231839s
Dec 12 21:51:25.587: INFO: Pod "alpine-nnp-false-6972985f-f08a-4545-b1f5-84ba148abdb6" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:51:25.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2955" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":43,"skipped":653,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:51:25.608: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:51:25.815: INFO: (0) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 19.542764ms)
Dec 12 21:51:25.823: INFO: (1) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 7.877197ms)
Dec 12 21:51:25.827: INFO: (2) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.937759ms)
Dec 12 21:51:25.833: INFO: (3) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.166654ms)
Dec 12 21:51:25.837: INFO: (4) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.464195ms)
Dec 12 21:51:25.842: INFO: (5) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.04502ms)
Dec 12 21:51:25.847: INFO: (6) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.319474ms)
Dec 12 21:51:25.852: INFO: (7) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.09422ms)
Dec 12 21:51:25.857: INFO: (8) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.05028ms)
Dec 12 21:51:25.863: INFO: (9) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.74735ms)
Dec 12 21:51:25.868: INFO: (10) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.770175ms)
Dec 12 21:51:25.873: INFO: (11) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.442758ms)
Dec 12 21:51:25.878: INFO: (12) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.464261ms)
Dec 12 21:51:25.883: INFO: (13) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.555243ms)
Dec 12 21:51:25.889: INFO: (14) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.075658ms)
Dec 12 21:51:25.893: INFO: (15) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.468661ms)
Dec 12 21:51:25.899: INFO: (16) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.127546ms)
Dec 12 21:51:25.903: INFO: (17) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.960404ms)
Dec 12 21:51:25.907: INFO: (18) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.919524ms)
Dec 12 21:51:25.912: INFO: (19) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.05635ms)
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:51:25.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8439" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":44,"skipped":659,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:51:25.926: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 12 21:51:27.160: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1212 21:51:27.160500      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:51:27.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2589" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":45,"skipped":663,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:51:27.175: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7424.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7424.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7424.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7424.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 21:51:31.383: INFO: DNS probes using dns-test-c934f583-2346-46da-a316-1175a313732b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7424.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7424.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7424.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7424.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 21:51:35.444: INFO: File wheezy_udp@dns-test-service-3.dns-7424.svc.cluster.local from pod  dns-7424/dns-test-1d25aa46-6ca7-466b-a7f1-7ec543350802 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 12 21:51:35.450: INFO: Lookups using dns-7424/dns-test-1d25aa46-6ca7-466b-a7f1-7ec543350802 failed for: [wheezy_udp@dns-test-service-3.dns-7424.svc.cluster.local]

Dec 12 21:51:40.462: INFO: DNS probes using dns-test-1d25aa46-6ca7-466b-a7f1-7ec543350802 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7424.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7424.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7424.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7424.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 21:52:14.590: INFO: DNS probes using dns-test-4c404d18-24f2-424f-b013-ffca3153daad succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:14.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7424" for this suite.

• [SLOW TEST:47.473 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":46,"skipped":663,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:14.653: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:52:15.755: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 21:52:17.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784335, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784335, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784335, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784335, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:52:20.784: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1721" for this suite.
STEP: Destroying namespace "webhook-1721-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.436 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":47,"skipped":677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:21.094: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-6d5dc307-2e82-44dc-828b-f7a15b339f08
STEP: Creating a pod to test consume secrets
Dec 12 21:52:21.284: INFO: Waiting up to 5m0s for pod "pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4" in namespace "secrets-5950" to be "success or failure"
Dec 12 21:52:21.292: INFO: Pod "pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.829505ms
Dec 12 21:52:23.296: INFO: Pod "pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012153103s
Dec 12 21:52:25.301: INFO: Pod "pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016813068s
STEP: Saw pod success
Dec 12 21:52:25.301: INFO: Pod "pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4" satisfied condition "success or failure"
Dec 12 21:52:25.304: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 21:52:25.336: INFO: Waiting for pod pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4 to disappear
Dec 12 21:52:25.340: INFO: Pod pod-secrets-8a171b6b-97e5-4739-b6f4-baee4d1fa0f4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:25.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5950" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:25.360: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 21:52:28.585: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:28.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4987" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":49,"skipped":777,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:28.615: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1469
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Dec 12 21:52:28.770: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-860659511 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:28.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1469" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":50,"skipped":777,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:28.857: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1768
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 21:52:29.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2365'
Dec 12 21:52:29.143: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 21:52:29.143: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1773
Dec 12 21:52:29.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete jobs e2e-test-httpd-job --namespace=kubectl-2365'
Dec 12 21:52:29.250: INFO: stderr: ""
Dec 12 21:52:29.250: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:29.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2365" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":51,"skipped":778,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:29.271: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5488
STEP: Creating secret with name secret-test-6746a9de-f661-458b-90aa-22134b77ffd1
STEP: Creating a pod to test consume secrets
Dec 12 21:52:29.631: INFO: Waiting up to 5m0s for pod "pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353" in namespace "secrets-5587" to be "success or failure"
Dec 12 21:52:29.636: INFO: Pod "pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33922ms
Dec 12 21:52:31.640: INFO: Pod "pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008552654s
Dec 12 21:52:33.658: INFO: Pod "pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027022429s
STEP: Saw pod success
Dec 12 21:52:33.658: INFO: Pod "pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353" satisfied condition "success or failure"
Dec 12 21:52:33.666: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 21:52:33.688: INFO: Waiting for pod pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353 to disappear
Dec 12 21:52:33.691: INFO: Pod pod-secrets-96fe736a-098a-438d-b0e1-1500b9c20353 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:33.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5587" for this suite.
STEP: Destroying namespace "secret-namespace-5488" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":794,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:33.720: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 12 21:52:33.910: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:37.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3972" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":53,"skipped":797,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:37.562: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-7879
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7879 to expose endpoints map[]
Dec 12 21:52:37.737: INFO: Get endpoints failed (4.690332ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 12 21:52:38.742: INFO: successfully validated that service endpoint-test2 in namespace services-7879 exposes endpoints map[] (1.009968099s elapsed)
STEP: Creating pod pod1 in namespace services-7879
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7879 to expose endpoints map[pod1:[80]]
Dec 12 21:52:41.782: INFO: successfully validated that service endpoint-test2 in namespace services-7879 exposes endpoints map[pod1:[80]] (3.02969014s elapsed)
STEP: Creating pod pod2 in namespace services-7879
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7879 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 12 21:52:44.832: INFO: successfully validated that service endpoint-test2 in namespace services-7879 exposes endpoints map[pod1:[80] pod2:[80]] (3.042403525s elapsed)
STEP: Deleting pod pod1 in namespace services-7879
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7879 to expose endpoints map[pod2:[80]]
Dec 12 21:52:45.854: INFO: successfully validated that service endpoint-test2 in namespace services-7879 exposes endpoints map[pod2:[80]] (1.015620844s elapsed)
STEP: Deleting pod pod2 in namespace services-7879
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7879 to expose endpoints map[]
Dec 12 21:52:46.870: INFO: successfully validated that service endpoint-test2 in namespace services-7879 exposes endpoints map[] (1.008532179s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:46.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7879" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.355 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":54,"skipped":799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:46.925: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 21:52:47.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f" in namespace "downward-api-9970" to be "success or failure"
Dec 12 21:52:47.112: INFO: Pod "downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.456685ms
Dec 12 21:52:49.119: INFO: Pod "downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015686414s
Dec 12 21:52:51.123: INFO: Pod "downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019847563s
STEP: Saw pod success
Dec 12 21:52:51.123: INFO: Pod "downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f" satisfied condition "success or failure"
Dec 12 21:52:51.126: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f container client-container: <nil>
STEP: delete the pod
Dec 12 21:52:51.155: INFO: Waiting for pod downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f to disappear
Dec 12 21:52:51.158: INFO: Pod downwardapi-volume-b4a17bf6-cb7e-4fa0-86b3-6f48e6e22f7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:51.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9970" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":829,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:51.170: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-6266ebb8-4e13-48bd-b3ea-1fb3fbb3e164
STEP: Creating a pod to test consume configMaps
Dec 12 21:52:51.353: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8" in namespace "projected-1021" to be "success or failure"
Dec 12 21:52:51.356: INFO: Pod "pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428168ms
Dec 12 21:52:53.360: INFO: Pod "pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007496914s
Dec 12 21:52:55.365: INFO: Pod "pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011963368s
STEP: Saw pod success
Dec 12 21:52:55.365: INFO: Pod "pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8" satisfied condition "success or failure"
Dec 12 21:52:55.368: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 21:52:55.387: INFO: Waiting for pod pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8 to disappear
Dec 12 21:52:55.391: INFO: Pod pod-projected-configmaps-2c38d8af-1d1b-4b24-bbd3-218ac813d8b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:52:55.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1021" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":56,"skipped":832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:52:55.406: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2638
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-efaf8e81-b7eb-4bcd-944e-9de163d9ac69
STEP: Creating configMap with name cm-test-opt-upd-574cf281-ef4d-4351-a29c-e7d6f233d53d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-efaf8e81-b7eb-4bcd-944e-9de163d9ac69
STEP: Updating configmap cm-test-opt-upd-574cf281-ef4d-4351-a29c-e7d6f233d53d
STEP: Creating configMap with name cm-test-opt-create-2fb62877-62b5-4c46-8863-4f8bb0907428
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:54:32.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2638" for this suite.

• [SLOW TEST:96.761 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:54:32.169: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:54:33.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 21:54:35.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784473, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784473, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784473, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784473, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:54:38.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:54:38.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-135" for this suite.
STEP: Destroying namespace "webhook-135-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":58,"skipped":968,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:54:38.239: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:54:49.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2237" for this suite.

• [SLOW TEST:11.264 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":59,"skipped":985,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:54:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1508.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1508.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 209.209.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.209.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.209.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.209.209_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1508.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1508.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1508.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 209.209.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.209.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.209.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.209.209_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 21:54:53.735: INFO: Unable to read wheezy_udp@dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.742: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.749: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.756: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.804: INFO: Unable to read jessie_udp@dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.809: INFO: Unable to read jessie_tcp@dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.814: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.820: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local from pod dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b: the server could not find the requested resource (get pods dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b)
Dec 12 21:54:53.856: INFO: Lookups using dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b failed for: [wheezy_udp@dns-test-service.dns-1508.svc.cluster.local wheezy_tcp@dns-test-service.dns-1508.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local jessie_udp@dns-test-service.dns-1508.svc.cluster.local jessie_tcp@dns-test-service.dns-1508.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1508.svc.cluster.local]

Dec 12 21:54:58.973: INFO: DNS probes using dns-1508/dns-test-d84f097b-2b49-42b5-9c6d-5cb009db885b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:54:59.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1508" for this suite.

• [SLOW TEST:9.570 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":60,"skipped":991,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:54:59.077: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3488.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3488.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3488.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3488.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 21:55:03.318: INFO: DNS probes using dns-3488/dns-test-d4f6ae9d-28ef-480e-8910-e74f2af59020 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:55:03.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3488" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":61,"skipped":1000,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:55:03.378: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 21:55:03.544: INFO: Creating deployment "test-recreate-deployment"
Dec 12 21:55:03.552: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 12 21:55:03.562: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Dec 12 21:55:05.572: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 12 21:55:05.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784503, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784503, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784503, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784503, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 21:55:07.580: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 12 21:55:07.592: INFO: Updating deployment test-recreate-deployment
Dec 12 21:55:07.592: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 12 21:55:07.738: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9751 /apis/apps/v1/namespaces/deployment-9751/deployments/test-recreate-deployment bc549abd-3aff-4bda-964f-5524d77c2d0e 7527 2 2019-12-12 21:55:03 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a9de28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-12 21:55:07 +0000 UTC,LastTransitionTime:2019-12-12 21:55:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-12 21:55:07 +0000 UTC,LastTransitionTime:2019-12-12 21:55:03 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 12 21:55:07.743: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-9751 /apis/apps/v1/namespaces/deployment-9751/replicasets/test-recreate-deployment-5f94c574ff 8f0450ce-bb36-47cd-99f5-2eec5270e173 7524 1 2019-12-12 21:55:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bc549abd-3aff-4bda-964f-5524d77c2d0e 0xc00510a1d7 0xc00510a1d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00510a238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 21:55:07.743: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 12 21:55:07.744: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-9751 /apis/apps/v1/namespaces/deployment-9751/replicasets/test-recreate-deployment-799c574856 11a6e006-e87d-4b92-b911-b0b20447b725 7514 2 2019-12-12 21:55:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bc549abd-3aff-4bda-964f-5524d77c2d0e 0xc00510a297 0xc00510a298}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00510a308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 21:55:07.749: INFO: Pod "test-recreate-deployment-5f94c574ff-jlgtp" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-jlgtp test-recreate-deployment-5f94c574ff- deployment-9751 /api/v1/namespaces/deployment-9751/pods/test-recreate-deployment-5f94c574ff-jlgtp a16f4d2b-b359-4ad6-a687-0c9fd58dbf7a 7523 0 2019-12-12 21:55:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 8f0450ce-bb36-47cd-99f5-2eec5270e173 0xc00510a777 0xc00510a778}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-48xkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-48xkt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-48xkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:55:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:55:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:55:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 21:55:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:,StartTime:2019-12-12 21:55:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:55:07.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9751" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":62,"skipped":1002,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:55:07.772: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4806
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Dec 12 21:55:07.993: INFO: Found 0 stateful pods, waiting for 3
Dec 12 21:55:18.001: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:55:18.001: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:55:18.001: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 21:55:28.001: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:55:28.001: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:55:28.001: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 12 21:55:28.034: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 12 21:55:38.074: INFO: Updating stateful set ss2
Dec 12 21:55:38.082: INFO: Waiting for Pod statefulset-4806/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 12 21:55:48.214: INFO: Found 2 stateful pods, waiting for 3
Dec 12 21:55:58.221: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:55:58.221: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:55:58.221: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 21:56:08.219: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:56:08.219: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:56:08.219: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 12 21:56:08.248: INFO: Updating stateful set ss2
Dec 12 21:56:08.255: INFO: Waiting for Pod statefulset-4806/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 21:56:18.289: INFO: Updating stateful set ss2
Dec 12 21:56:18.300: INFO: Waiting for StatefulSet statefulset-4806/ss2 to complete update
Dec 12 21:56:18.300: INFO: Waiting for Pod statefulset-4806/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 21:56:28.310: INFO: Waiting for StatefulSet statefulset-4806/ss2 to complete update
Dec 12 21:56:28.310: INFO: Waiting for Pod statefulset-4806/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 21:56:38.310: INFO: Waiting for StatefulSet statefulset-4806/ss2 to complete update
Dec 12 21:56:48.311: INFO: Waiting for StatefulSet statefulset-4806/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 12 21:56:58.309: INFO: Deleting all statefulset in ns statefulset-4806
Dec 12 21:56:58.312: INFO: Scaling statefulset ss2 to 0
Dec 12 21:57:18.334: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:57:18.337: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:57:18.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4806" for this suite.

• [SLOW TEST:130.593 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":63,"skipped":1013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:57:18.366: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:57:31.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3410" for this suite.

• [SLOW TEST:13.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":64,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:57:31.637: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 12 21:57:41.840: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:57:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1212 21:57:41.840022      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2705" for this suite.

• [SLOW TEST:10.218 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":65,"skipped":1052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:57:41.859: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 21:57:42.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae" in namespace "downward-api-5654" to be "success or failure"
Dec 12 21:57:42.045: INFO: Pod "downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae": Phase="Pending", Reason="", readiness=false. Elapsed: 10.776685ms
Dec 12 21:57:44.050: INFO: Pod "downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015542526s
Dec 12 21:57:46.055: INFO: Pod "downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020317731s
STEP: Saw pod success
Dec 12 21:57:46.055: INFO: Pod "downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae" satisfied condition "success or failure"
Dec 12 21:57:46.058: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae container client-container: <nil>
STEP: delete the pod
Dec 12 21:57:46.094: INFO: Waiting for pod downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae to disappear
Dec 12 21:57:46.098: INFO: Pod downwardapi-volume-1a32c37f-72d5-48ae-93ed-6981d8b1ecae no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:57:46.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5654" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":1077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:57:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 21:57:46.286: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2" in namespace "downward-api-9634" to be "success or failure"
Dec 12 21:57:46.290: INFO: Pod "downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.847581ms
Dec 12 21:57:48.295: INFO: Pod "downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009057778s
Dec 12 21:57:50.299: INFO: Pod "downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013282297s
STEP: Saw pod success
Dec 12 21:57:50.300: INFO: Pod "downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2" satisfied condition "success or failure"
Dec 12 21:57:50.303: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2 container client-container: <nil>
STEP: delete the pod
Dec 12 21:57:50.328: INFO: Waiting for pod downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2 to disappear
Dec 12 21:57:50.331: INFO: Pod downwardapi-volume-1c4612b9-aff8-4dc6-ac10-0859c47178c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:57:50.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9634" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1116,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:57:50.347: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-czkz
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 21:57:50.529: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-czkz" in namespace "subpath-1877" to be "success or failure"
Dec 12 21:57:50.540: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.644698ms
Dec 12 21:57:52.544: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014861448s
Dec 12 21:57:54.548: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 4.018879894s
Dec 12 21:57:56.553: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 6.024153054s
Dec 12 21:57:58.558: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 8.028939044s
Dec 12 21:58:00.593: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 10.064385406s
Dec 12 21:58:02.597: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 12.068220189s
Dec 12 21:58:04.601: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 14.072505634s
Dec 12 21:58:06.605: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 16.076362636s
Dec 12 21:58:08.609: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 18.080339314s
Dec 12 21:58:10.614: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 20.084869714s
Dec 12 21:58:12.618: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Running", Reason="", readiness=true. Elapsed: 22.08916304s
Dec 12 21:58:14.623: INFO: Pod "pod-subpath-test-configmap-czkz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.094163956s
STEP: Saw pod success
Dec 12 21:58:14.623: INFO: Pod "pod-subpath-test-configmap-czkz" satisfied condition "success or failure"
Dec 12 21:58:14.626: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-subpath-test-configmap-czkz container test-container-subpath-configmap-czkz: <nil>
STEP: delete the pod
Dec 12 21:58:14.655: INFO: Waiting for pod pod-subpath-test-configmap-czkz to disappear
Dec 12 21:58:14.659: INFO: Pod pod-subpath-test-configmap-czkz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-czkz
Dec 12 21:58:14.659: INFO: Deleting pod "pod-subpath-test-configmap-czkz" in namespace "subpath-1877"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:58:14.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1877" for this suite.

• [SLOW TEST:24.324 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":68,"skipped":1123,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:58:14.673: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1115
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-1115
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1115
Dec 12 21:58:14.861: INFO: Found 0 stateful pods, waiting for 1
Dec 12 21:58:24.865: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 12 21:58:24.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:58:25.563: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:58:25.563: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:58:25.563: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:58:25.567: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 12 21:58:35.574: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:58:35.574: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:58:35.592: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:58:35.592: INFO: ss-0  talos-0-3-0-beta-0-gcp-workers-nt7qc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:14 +0000 UTC  }]
Dec 12 21:58:35.592: INFO: 
Dec 12 21:58:35.592: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 12 21:58:36.597: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996452608s
Dec 12 21:58:37.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99186879s
Dec 12 21:58:38.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986842814s
Dec 12 21:58:39.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982606475s
Dec 12 21:58:40.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977193363s
Dec 12 21:58:41.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972693576s
Dec 12 21:58:42.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96823872s
Dec 12 21:58:43.630: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963691691s
Dec 12 21:58:44.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.40089ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1115
Dec 12 21:58:45.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:58:45.802: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 21:58:45.803: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:58:45.803: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:58:45.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:58:45.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 12 21:58:45.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:58:45.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:58:45.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 21:58:46.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 12 21:58:46.128: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 21:58:46.128: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 21:58:46.133: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:58:46.133: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 21:58:46.133: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 12 21:58:46.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:58:46.287: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:58:46.287: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:58:46.287: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:58:46.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:58:46.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:58:46.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:58:46.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:58:46.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1115 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 21:58:46.697: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 21:58:46.697: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 21:58:46.697: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 21:58:46.697: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:58:46.702: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 12 21:58:56.710: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:58:56.710: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:58:56.710: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 21:58:56.722: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:58:56.722: INFO: ss-0  talos-0-3-0-beta-0-gcp-workers-nt7qc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:14 +0000 UTC  }]
Dec 12 21:58:56.723: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:56.723: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:56.723: INFO: 
Dec 12 21:58:56.723: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 12 21:58:57.728: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:58:57.728: INFO: ss-0  talos-0-3-0-beta-0-gcp-workers-nt7qc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:14 +0000 UTC  }]
Dec 12 21:58:57.728: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:57.728: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:57.728: INFO: 
Dec 12 21:58:57.728: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 12 21:58:58.733: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:58:58.733: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:58.733: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:58.734: INFO: 
Dec 12 21:58:58.734: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 21:58:59.738: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:58:59.738: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:59.738: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:58:59.738: INFO: 
Dec 12 21:58:59.738: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 21:59:00.742: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:59:00.742: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:59:00.743: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:59:00.743: INFO: 
Dec 12 21:59:00.743: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 21:59:01.749: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:59:01.750: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:59:01.750: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:59:01.751: INFO: 
Dec 12 21:59:01.751: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 21:59:02.764: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec 12 21:59:02.764: INFO: ss-1  talos-0-3-0-beta-0-gcp-workers-ntvmn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:59:02.764: INFO: ss-2  talos-0-3-0-beta-0-gcp-workers-4kj7s  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 21:58:35 +0000 UTC  }]
Dec 12 21:59:02.764: INFO: 
Dec 12 21:59:02.764: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 21:59:03.768: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.95483097s
Dec 12 21:59:04.773: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.950731921s
Dec 12 21:59:05.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 945.825563ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1115
Dec 12 21:59:06.782: INFO: Scaling statefulset ss to 0
Dec 12 21:59:06.791: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 12 21:59:06.795: INFO: Deleting all statefulset in ns statefulset-1115
Dec 12 21:59:06.797: INFO: Scaling statefulset ss to 0
Dec 12 21:59:06.806: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 21:59:06.808: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:59:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1115" for this suite.

• [SLOW TEST:52.157 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":69,"skipped":1132,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:59:06.831: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 21:59:07.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 21:59:09.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784747, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784747, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784748, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784747, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 21:59:11.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784747, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784747, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784748, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711784747, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 21:59:14.932: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:59:25.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3504" for this suite.
STEP: Destroying namespace "webhook-3504-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.358 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":70,"skipped":1139,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:59:25.191: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 12 21:59:25.413: INFO: Waiting up to 5m0s for pod "pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff" in namespace "emptydir-4863" to be "success or failure"
Dec 12 21:59:25.417: INFO: Pod "pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8059ms
Dec 12 21:59:27.422: INFO: Pod "pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008638257s
Dec 12 21:59:29.427: INFO: Pod "pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013081688s
STEP: Saw pod success
Dec 12 21:59:29.427: INFO: Pod "pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff" satisfied condition "success or failure"
Dec 12 21:59:29.431: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff container test-container: <nil>
STEP: delete the pod
Dec 12 21:59:29.454: INFO: Waiting for pod pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff to disappear
Dec 12 21:59:29.459: INFO: Pod pod-16ca8fa0-38de-40c5-ad8d-e525249ca7ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:59:29.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4863" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":71,"skipped":1146,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:59:29.472: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-5c8873d0-2659-4b2f-b52d-15df086170c3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:59:29.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7432" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":72,"skipped":1146,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:59:29.665: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Dec 12 21:59:29.838: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6512" to be "success or failure"
Dec 12 21:59:29.852: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.535901ms
Dec 12 21:59:31.857: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019265126s
Dec 12 21:59:33.863: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024898781s
STEP: Saw pod success
Dec 12 21:59:33.863: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 12 21:59:33.867: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 12 21:59:33.910: INFO: Waiting for pod pod-host-path-test to disappear
Dec 12 21:59:33.916: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:59:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6512" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":73,"skipped":1164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:59:33.933: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 12 21:59:34.109: INFO: Waiting up to 5m0s for pod "pod-58945957-78ca-4a63-930c-b5b6c30bde1b" in namespace "emptydir-9507" to be "success or failure"
Dec 12 21:59:34.113: INFO: Pod "pod-58945957-78ca-4a63-930c-b5b6c30bde1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75935ms
Dec 12 21:59:36.118: INFO: Pod "pod-58945957-78ca-4a63-930c-b5b6c30bde1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008876849s
Dec 12 21:59:38.123: INFO: Pod "pod-58945957-78ca-4a63-930c-b5b6c30bde1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013759341s
STEP: Saw pod success
Dec 12 21:59:38.123: INFO: Pod "pod-58945957-78ca-4a63-930c-b5b6c30bde1b" satisfied condition "success or failure"
Dec 12 21:59:38.127: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-58945957-78ca-4a63-930c-b5b6c30bde1b container test-container: <nil>
STEP: delete the pod
Dec 12 21:59:38.151: INFO: Waiting for pod pod-58945957-78ca-4a63-930c-b5b6c30bde1b to disappear
Dec 12 21:59:38.154: INFO: Pod pod-58945957-78ca-4a63-930c-b5b6c30bde1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 21:59:38.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9507" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":1198,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 21:59:38.168: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-154
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3747
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:00:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7694" for this suite.
STEP: Destroying namespace "nsdeletetest-154" for this suite.
Dec 12 22:00:09.834: INFO: Namespace nsdeletetest-154 was already deleted
STEP: Destroying namespace "nsdeletetest-3747" for this suite.

• [SLOW TEST:31.673 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":75,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:00:09.842: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:01:10.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7061" for this suite.

• [SLOW TEST:60.207 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":76,"skipped":1228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:01:10.050: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1098
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 12 22:01:10.236: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:01:13.134: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:01:23.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1098" for this suite.

• [SLOW TEST:13.063 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":77,"skipped":1271,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:01:23.116: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-f42e034f-9929-4a92-8686-0bbddb391928 in namespace container-probe-3647
Dec 12 22:01:27.325: INFO: Started pod busybox-f42e034f-9929-4a92-8686-0bbddb391928 in namespace container-probe-3647
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 22:01:27.328: INFO: Initial restart count of pod busybox-f42e034f-9929-4a92-8686-0bbddb391928 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:05:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3647" for this suite.

• [SLOW TEST:244.834 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":78,"skipped":1292,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:05:28.032: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 12 22:05:28.195: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 12 22:05:28.210: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 22:05:28.214: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-0 before test
Dec 12 22:05:28.234: INFO: kube-apiserver-9sv5v from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.234: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 12 22:05:28.234: INFO: pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.234: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:05:28.234: INFO: kube-flannel-69ftd from kube-system started at 2019-12-12 21:35:43 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.234: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:05:28.234: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:05:28.234: INFO: kube-proxy-bkbtw from kube-system started at 2019-12-12 21:35:43 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.234: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:05:28.235: INFO: pod-checkpointer-wbndz from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.235: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:05:28.235: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-1 before test
Dec 12 22:05:28.260: INFO: coredns-5bcc66f5bd-6pzdf from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.260: INFO: 	Container coredns ready: true, restart count 0
Dec 12 22:05:28.260: INFO: kube-scheduler-6db5bfb7bc-tx7bk from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.260: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 12 22:05:28.260: INFO: kube-apiserver-j24rk from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.260: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:05:28.260: INFO: pod-checkpointer-8ftvl from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.260: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:05:28.260: INFO: kube-scheduler-6db5bfb7bc-dplt8 from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.260: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 12 22:05:28.260: INFO: kube-controller-manager-5c547b548-hxgrq from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.260: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 12 22:05:28.260: INFO: kube-controller-manager-5c547b548-cqqln from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.261: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 12 22:05:28.261: INFO: pod-checkpointer-8ftvl-talos-0-3-0-beta-0-gcp-controlplane-1 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.261: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:05:28.261: INFO: kube-flannel-cq4nw from kube-system started at 2019-12-12 21:35:36 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.261: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:05:28.261: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:05:28.261: INFO: kube-proxy-mb48g from kube-system started at 2019-12-12 21:35:36 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.261: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:05:28.261: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-2 before test
Dec 12 22:05:28.282: INFO: kube-proxy-9wbfd from kube-system started at 2019-12-12 21:35:29 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.282: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:05:28.282: INFO: kube-flannel-9hznp from kube-system started at 2019-12-12 21:35:29 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.282: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:05:28.282: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:05:28.282: INFO: pod-checkpointer-5mnr9 from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.282: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:05:28.282: INFO: kube-apiserver-kf44j from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.282: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:05:28.282: INFO: pod-checkpointer-5mnr9-talos-0-3-0-beta-0-gcp-controlplane-2 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.282: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:05:28.282: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-4kj7s before test
Dec 12 22:05:28.306: INFO: kube-flannel-bz9ht from kube-system started at 2019-12-12 21:36:55 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.306: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:05:28.306: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:05:28.306: INFO: kube-proxy-xh54v from kube-system started at 2019-12-12 21:36:55 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.306: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:05:28.306: INFO: sonobuoy-e2e-job-01c7b380be144838 from sonobuoy started at 2019-12-12 21:39:37 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.306: INFO: 	Container e2e ready: true, restart count 0
Dec 12 22:05:28.306: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 12 22:05:28.306: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-nt7qc before test
Dec 12 22:05:28.328: INFO: kube-flannel-p79j9 from kube-system started at 2019-12-12 21:37:10 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.328: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:05:28.328: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:05:28.328: INFO: kube-proxy-jtlmf from kube-system started at 2019-12-12 21:37:10 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.328: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:05:28.328: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-ntvmn before test
Dec 12 22:05:28.350: INFO: kube-proxy-bvldz from kube-system started at 2019-12-12 21:37:28 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.350: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:05:28.350: INFO: kube-flannel-gk4cx from kube-system started at 2019-12-12 21:37:28 +0000 UTC (2 container statuses recorded)
Dec 12 22:05:28.350: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:05:28.350: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:05:28.350: INFO: sonobuoy from sonobuoy started at 2019-12-12 21:39:34 +0000 UTC (1 container statuses recorded)
Dec 12 22:05:28.350: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4aae1a4d-bb6d-463a-ad5d-eeb12549cc9b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-4aae1a4d-bb6d-463a-ad5d-eeb12549cc9b off the node talos-0-3-0-beta-0-gcp-workers-nt7qc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4aae1a4d-bb6d-463a-ad5d-eeb12549cc9b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:05:44.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-685" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:16.556 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":79,"skipped":1292,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:05:44.590: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1211
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Dec 12 22:05:44.767: INFO: Found 0 stateful pods, waiting for 3
Dec 12 22:05:54.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:05:54.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:05:54.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 22:06:05.628: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:06:05.628: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:06:05.628: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 22:06:14.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:06:14.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:06:14.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 22:06:14.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1211 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 22:06:14.978: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 22:06:14.978: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 22:06:14.978: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 12 22:06:25.016: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 12 22:06:35.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1211 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 22:06:35.196: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 22:06:35.196: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 22:06:35.196: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Dec 12 22:06:55.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1211 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 22:06:55.463: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 22:06:55.463: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 22:06:55.463: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 22:07:05.525: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 12 22:07:15.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=statefulset-1211 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 22:07:15.743: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 22:07:15.743: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 22:07:15.743: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 22:07:25.771: INFO: Waiting for StatefulSet statefulset-1211/ss2 to complete update
Dec 12 22:07:25.771: INFO: Waiting for Pod statefulset-1211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 22:07:25.771: INFO: Waiting for Pod statefulset-1211/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 22:07:25.771: INFO: Waiting for Pod statefulset-1211/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 22:07:35.782: INFO: Waiting for StatefulSet statefulset-1211/ss2 to complete update
Dec 12 22:07:35.782: INFO: Waiting for Pod statefulset-1211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 12 22:07:45.783: INFO: Deleting all statefulset in ns statefulset-1211
Dec 12 22:07:45.787: INFO: Scaling statefulset ss2 to 0
Dec 12 22:08:15.812: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 22:08:15.815: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:15.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1211" for this suite.

• [SLOW TEST:151.256 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":80,"skipped":1294,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:15.848: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:16.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1257" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":81,"skipped":1313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:16.043: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5788
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:08:16.212: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 12 22:08:19.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-5788 create -f -'
Dec 12 22:08:19.838: INFO: stderr: ""
Dec 12 22:08:19.838: INFO: stdout: "e2e-test-crd-publish-openapi-1658-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 12 22:08:19.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-5788 delete e2e-test-crd-publish-openapi-1658-crds test-cr'
Dec 12 22:08:19.938: INFO: stderr: ""
Dec 12 22:08:19.938: INFO: stdout: "e2e-test-crd-publish-openapi-1658-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 12 22:08:19.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-5788 apply -f -'
Dec 12 22:08:20.279: INFO: stderr: ""
Dec 12 22:08:20.279: INFO: stdout: "e2e-test-crd-publish-openapi-1658-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 12 22:08:20.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-5788 delete e2e-test-crd-publish-openapi-1658-crds test-cr'
Dec 12 22:08:20.459: INFO: stderr: ""
Dec 12 22:08:20.459: INFO: stdout: "e2e-test-crd-publish-openapi-1658-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 12 22:08:20.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-1658-crds'
Dec 12 22:08:20.720: INFO: stderr: ""
Dec 12 22:08:20.720: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1658-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:23.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5788" for this suite.

• [SLOW TEST:7.595 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":82,"skipped":1364,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:23.638: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 12 22:08:23.857: INFO: Waiting up to 5m0s for pod "pod-788b3d20-d45b-406f-b823-d7d7c751fb82" in namespace "emptydir-8309" to be "success or failure"
Dec 12 22:08:23.862: INFO: Pod "pod-788b3d20-d45b-406f-b823-d7d7c751fb82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519585ms
Dec 12 22:08:25.884: INFO: Pod "pod-788b3d20-d45b-406f-b823-d7d7c751fb82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027277006s
Dec 12 22:08:27.891: INFO: Pod "pod-788b3d20-d45b-406f-b823-d7d7c751fb82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033754944s
STEP: Saw pod success
Dec 12 22:08:27.891: INFO: Pod "pod-788b3d20-d45b-406f-b823-d7d7c751fb82" satisfied condition "success or failure"
Dec 12 22:08:27.896: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-788b3d20-d45b-406f-b823-d7d7c751fb82 container test-container: <nil>
STEP: delete the pod
Dec 12 22:08:27.943: INFO: Waiting for pod pod-788b3d20-d45b-406f-b823-d7d7c751fb82 to disappear
Dec 12 22:08:27.947: INFO: Pod pod-788b3d20-d45b-406f-b823-d7d7c751fb82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:27.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8309" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1367,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:27.966: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-8494
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:28.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8494" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":84,"skipped":1387,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:08:28.415: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 12 22:08:33.421: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 12 22:08:33.422: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 12 22:08:35.427: INFO: Creating deployment "test-rollover-deployment"
Dec 12 22:08:35.443: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 12 22:08:37.451: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 12 22:08:37.461: INFO: Ensure that both replica sets have 1 created replica
Dec 12 22:08:37.467: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 12 22:08:37.479: INFO: Updating deployment test-rollover-deployment
Dec 12 22:08:37.479: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 12 22:08:39.508: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 12 22:08:39.517: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 12 22:08:39.527: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 22:08:39.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785317, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:08:41.536: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 22:08:41.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785320, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:08:43.537: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 22:08:43.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785320, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:08:45.536: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 22:08:45.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785320, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:08:47.536: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 22:08:47.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785320, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:08:49.535: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 22:08:49.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785320, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785315, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:08:51.539: INFO: 
Dec 12 22:08:51.539: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 12 22:08:51.550: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7551 /apis/apps/v1/namespaces/deployment-7551/deployments/test-rollover-deployment 057a28e8-3e41-4c79-be26-2b8ee4f4dd7a 11473 2 2019-12-12 22:08:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031df398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-12 22:08:35 +0000 UTC,LastTransitionTime:2019-12-12 22:08:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2019-12-12 22:08:50 +0000 UTC,LastTransitionTime:2019-12-12 22:08:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 12 22:08:51.554: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-7551 /apis/apps/v1/namespaces/deployment-7551/replicasets/test-rollover-deployment-574d6dfbff 5892332d-112c-41f2-bb0c-00b48b140a9b 11457 2 2019-12-12 22:08:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 057a28e8-3e41-4c79-be26-2b8ee4f4dd7a 0xc0031dfa57 0xc0031dfa58}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031dfb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:08:51.554: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 12 22:08:51.555: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7551 /apis/apps/v1/namespaces/deployment-7551/replicasets/test-rollover-controller 2b747962-bf44-454e-a55d-8123079bd2f1 11470 2 2019-12-12 22:08:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 057a28e8-3e41-4c79-be26-2b8ee4f4dd7a 0xc0031df937 0xc0031df938}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031df9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:08:51.556: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-7551 /apis/apps/v1/namespaces/deployment-7551/replicasets/test-rollover-deployment-f6c94f66c 1f4285c3-220e-44f4-8f2a-75c99d8d20bf 11412 2 2019-12-12 22:08:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 057a28e8-3e41-4c79-be26-2b8ee4f4dd7a 0xc0031dfbb0 0xc0031dfbb1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031dfc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:08:51.562: INFO: Pod "test-rollover-deployment-574d6dfbff-pb5ck" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-pb5ck test-rollover-deployment-574d6dfbff- deployment-7551 /api/v1/namespaces/deployment-7551/pods/test-rollover-deployment-574d6dfbff-pb5ck 0a856f75-97ad-42ee-acb3-05bc9ff700c8 11427 0 2019-12-12 22:08:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 5892332d-112c-41f2-bb0c-00b48b140a9b 0xc00320a557 0xc00320a558}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dcnt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dcnt8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dcnt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:10.244.4.78,StartTime:2019-12-12 22:08:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:08:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://f986bd564f0f81d4195543531d1eeb0e13c13fe47647547e695ba432384e3e82,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:51.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7551" for this suite.

• [SLOW TEST:23.342 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":85,"skipped":1390,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:51.579: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:08:51.771: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 12 22:08:56.775: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 12 22:08:56.775: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 12 22:08:56.802: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3699 /apis/apps/v1/namespaces/deployment-3699/deployments/test-cleanup-deployment e5ee64f8-32dd-41a2-99f4-34dbbbf486f4 11526 1 2019-12-12 22:08:56 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003254888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 12 22:08:56.806: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Dec 12 22:08:56.806: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 12 22:08:56.807: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3699 /apis/apps/v1/namespaces/deployment-3699/replicasets/test-cleanup-controller 4ab1bc65-da42-4194-ba72-3e5eaaae7848 11528 1 2019-12-12 22:08:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e5ee64f8-32dd-41a2-99f4-34dbbbf486f4 0xc003254da7 0xc003254da8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003254e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:08:56.814: INFO: Pod "test-cleanup-controller-7zm2w" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7zm2w test-cleanup-controller- deployment-3699 /api/v1/namespaces/deployment-3699/pods/test-cleanup-controller-7zm2w 06dcd95d-5d5b-47ad-80ec-bca5ae66893e 11503 0 2019-12-12 22:08:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 4ab1bc65-da42-4194-ba72-3e5eaaae7848 0xc003255407 0xc003255408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6nrql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6nrql,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6nrql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:08:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:10.244.5.29,StartTime:2019-12-12 22:08:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:08:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://df36ce25d2fc7f8b8bb28ffb788bee68b9d248dd07ef71197d997a8e0a1849cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:08:56.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3699" for this suite.

• [SLOW TEST:5.254 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":86,"skipped":1397,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:08:56.837: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:08:57.608: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 22:08:59.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785337, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785337, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785337, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711785337, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:09:02.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:02.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6617" for this suite.
STEP: Destroying namespace "webhook-6617-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.238 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":87,"skipped":1400,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:03.087: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6334
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-897bb2e2-faaa-48ca-a365-c1f996d3115a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:07.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6334" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:07.341: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 12 22:09:07.514: INFO: Created pod &Pod{ObjectMeta:{dns-6822  dns-6822 /api/v1/namespaces/dns-6822/pods/dns-6822 f838fef3-8e9b-49e7-98ec-d63c099f1bcd 11703 0 2019-12-12 22:09:07 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ls4nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ls4nt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ls4nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 12 22:09:11.523: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6822 PodName:dns-6822 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:09:11.523: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Verifying customized DNS server is configured on pod...
Dec 12 22:09:11.610: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6822 PodName:dns-6822 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:09:11.610: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:09:11.688: INFO: Deleting pod dns-6822...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:11.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6822" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":89,"skipped":1435,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:11.728: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8999 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8999;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8999 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8999;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8999.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8999.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8999.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8999.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8999.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8999.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8999.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8999.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8999.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 40.50.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.50.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.50.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.50.40_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8999 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8999;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8999 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8999;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8999.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8999.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8999.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8999.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8999.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8999.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8999.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8999.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8999.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8999.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 40.50.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.50.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.50.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.50.40_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 22:09:35.966: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:35.975: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:35.986: INFO: Unable to read wheezy_udp@dns-test-service.dns-8999 from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:35.993: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8999 from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:35.998: INFO: Unable to read wheezy_udp@dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.005: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.011: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.018: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.063: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.069: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.074: INFO: Unable to read jessie_udp@dns-test-service.dns-8999 from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-8999 from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.086: INFO: Unable to read jessie_udp@dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.097: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.103: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8999.svc from pod dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda: the server could not find the requested resource (get pods dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda)
Dec 12 22:09:36.144: INFO: Lookups using dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8999 wheezy_tcp@dns-test-service.dns-8999 wheezy_udp@dns-test-service.dns-8999.svc wheezy_tcp@dns-test-service.dns-8999.svc wheezy_udp@_http._tcp.dns-test-service.dns-8999.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8999.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8999 jessie_tcp@dns-test-service.dns-8999 jessie_udp@dns-test-service.dns-8999.svc jessie_tcp@dns-test-service.dns-8999.svc jessie_udp@_http._tcp.dns-test-service.dns-8999.svc jessie_tcp@_http._tcp.dns-test-service.dns-8999.svc]

Dec 12 22:09:41.322: INFO: DNS probes using dns-8999/dns-test-cdf3a9f2-eeb6-4de0-b542-6061fd5e1dda succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:41.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8999" for this suite.

• [SLOW TEST:29.707 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":90,"skipped":1438,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:41.433: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3430.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3430.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3430.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3430.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 22:09:45.696: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.703: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.708: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.713: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.738: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.744: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.750: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.754: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3430.svc.cluster.local from pod dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d: the server could not find the requested resource (get pods dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d)
Dec 12 22:09:45.765: INFO: Lookups using dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3430.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3430.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3430.svc.cluster.local jessie_udp@dns-test-service-2.dns-3430.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3430.svc.cluster.local]

Dec 12 22:09:50.840: INFO: DNS probes using dns-3430/dns-test-34fea6a0-b4aa-49d5-85de-14a48d49be1d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:50.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3430" for this suite.

• [SLOW TEST:9.462 seconds]
[sig-network] DNS
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":91,"skipped":1438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-05844e49-d8fc-4a37-86e4-10f62d01a22e
STEP: Creating a pod to test consume secrets
Dec 12 22:09:51.110: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54" in namespace "projected-8564" to be "success or failure"
Dec 12 22:09:51.117: INFO: Pod "pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.314144ms
Dec 12 22:09:53.122: INFO: Pod "pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011090959s
Dec 12 22:09:55.127: INFO: Pod "pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016568641s
STEP: Saw pod success
Dec 12 22:09:55.127: INFO: Pod "pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54" satisfied condition "success or failure"
Dec 12 22:09:55.132: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:09:55.161: INFO: Waiting for pod pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54 to disappear
Dec 12 22:09:55.164: INFO: Pod pod-projected-secrets-ba6bf1b7-74f6-4693-9d72-11560a159d54 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:55.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8564" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1463,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:55.181: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:09:55.397: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707" in namespace "downward-api-9235" to be "success or failure"
Dec 12 22:09:55.402: INFO: Pod "downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707": Phase="Pending", Reason="", readiness=false. Elapsed: 4.888173ms
Dec 12 22:09:57.407: INFO: Pod "downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009647479s
Dec 12 22:09:59.411: INFO: Pod "downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013886197s
STEP: Saw pod success
Dec 12 22:09:59.411: INFO: Pod "downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707" satisfied condition "success or failure"
Dec 12 22:09:59.415: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707 container client-container: <nil>
STEP: delete the pod
Dec 12 22:09:59.436: INFO: Waiting for pod downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707 to disappear
Dec 12 22:09:59.439: INFO: Pod downwardapi-volume-4d66845c-b9d6-4587-b503-44fe98903707 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9235" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":93,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:59.468: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:09:59.659: INFO: (0) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 20.052931ms)
Dec 12 22:09:59.665: INFO: (1) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.79763ms)
Dec 12 22:09:59.670: INFO: (2) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.201095ms)
Dec 12 22:09:59.676: INFO: (3) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.29123ms)
Dec 12 22:09:59.681: INFO: (4) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.060855ms)
Dec 12 22:09:59.686: INFO: (5) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.812701ms)
Dec 12 22:09:59.691: INFO: (6) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.896195ms)
Dec 12 22:09:59.697: INFO: (7) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.547662ms)
Dec 12 22:09:59.703: INFO: (8) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.690766ms)
Dec 12 22:09:59.708: INFO: (9) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.21716ms)
Dec 12 22:09:59.713: INFO: (10) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.408622ms)
Dec 12 22:09:59.719: INFO: (11) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.469226ms)
Dec 12 22:09:59.724: INFO: (12) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.10947ms)
Dec 12 22:09:59.731: INFO: (13) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.86644ms)
Dec 12 22:09:59.737: INFO: (14) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.292438ms)
Dec 12 22:09:59.743: INFO: (15) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.157922ms)
Dec 12 22:09:59.749: INFO: (16) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.941233ms)
Dec 12 22:09:59.754: INFO: (17) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.778081ms)
Dec 12 22:09:59.759: INFO: (18) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.734748ms)
Dec 12 22:09:59.765: INFO: (19) /api/v1/nodes/talos-0-3-0-beta-0-gcp-controlplane-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.756534ms)
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:09:59.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2870" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":94,"skipped":1482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:09:59.784: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2078
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2078
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 22:09:59.965: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 12 22:10:30.162: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.85:8080/dial?request=hostname&protocol=udp&host=10.244.2.3&port=8081&tries=1'] Namespace:pod-network-test-2078 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:10:30.162: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:10:30.250: INFO: Waiting for responses: map[]
Dec 12 22:10:30.256: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.85:8080/dial?request=hostname&protocol=udp&host=10.244.1.9&port=8081&tries=1'] Namespace:pod-network-test-2078 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:10:30.256: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:10:30.324: INFO: Waiting for responses: map[]
Dec 12 22:10:30.328: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.85:8080/dial?request=hostname&protocol=udp&host=10.244.0.3&port=8081&tries=1'] Namespace:pod-network-test-2078 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:10:30.328: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:10:30.404: INFO: Waiting for responses: map[]
Dec 12 22:10:30.408: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.85:8080/dial?request=hostname&protocol=udp&host=10.244.3.14&port=8081&tries=1'] Namespace:pod-network-test-2078 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:10:30.408: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:10:30.478: INFO: Waiting for responses: map[]
Dec 12 22:10:30.482: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.85:8080/dial?request=hostname&protocol=udp&host=10.244.4.84&port=8081&tries=1'] Namespace:pod-network-test-2078 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:10:30.482: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:10:30.547: INFO: Waiting for responses: map[]
Dec 12 22:10:30.570: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.85:8080/dial?request=hostname&protocol=udp&host=10.244.5.32&port=8081&tries=1'] Namespace:pod-network-test-2078 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:10:30.570: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:10:30.640: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:10:30.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2078" for this suite.

• [SLOW TEST:30.872 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:10:30.656: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 12 22:10:30.892: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:10:36.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4858" for this suite.

• [SLOW TEST:5.991 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":96,"skipped":1564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:10:36.648: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 12 22:10:36.893: INFO: Waiting up to 5m0s for pod "pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f" in namespace "emptydir-7325" to be "success or failure"
Dec 12 22:10:36.898: INFO: Pod "pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.357913ms
Dec 12 22:10:38.904: INFO: Pod "pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011123361s
Dec 12 22:10:40.909: INFO: Pod "pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016151055s
STEP: Saw pod success
Dec 12 22:10:40.909: INFO: Pod "pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f" satisfied condition "success or failure"
Dec 12 22:10:40.913: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f container test-container: <nil>
STEP: delete the pod
Dec 12 22:10:40.956: INFO: Waiting for pod pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f to disappear
Dec 12 22:10:40.960: INFO: Pod pod-32c1bee5-34e7-448e-9ee3-cab278e04f0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:10:40.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7325" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":97,"skipped":1593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:10:40.982: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:10:41.193: INFO: Create a RollingUpdate DaemonSet
Dec 12 22:10:41.200: INFO: Check that daemon pods launch on every node of the cluster
Dec 12 22:10:41.213: INFO: Number of nodes with available pods: 0
Dec 12 22:10:41.213: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:10:42.222: INFO: Number of nodes with available pods: 0
Dec 12 22:10:42.222: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:10:43.241: INFO: Number of nodes with available pods: 0
Dec 12 22:10:43.241: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:10:44.223: INFO: Number of nodes with available pods: 2
Dec 12 22:10:44.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:10:45.223: INFO: Number of nodes with available pods: 5
Dec 12 22:10:45.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:46.223: INFO: Number of nodes with available pods: 5
Dec 12 22:10:46.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:47.226: INFO: Number of nodes with available pods: 5
Dec 12 22:10:47.226: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:48.223: INFO: Number of nodes with available pods: 5
Dec 12 22:10:48.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:49.224: INFO: Number of nodes with available pods: 5
Dec 12 22:10:49.224: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:50.224: INFO: Number of nodes with available pods: 5
Dec 12 22:10:50.224: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:51.228: INFO: Number of nodes with available pods: 5
Dec 12 22:10:51.228: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:52.223: INFO: Number of nodes with available pods: 5
Dec 12 22:10:52.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:53.223: INFO: Number of nodes with available pods: 5
Dec 12 22:10:53.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:54.222: INFO: Number of nodes with available pods: 5
Dec 12 22:10:54.222: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:55.223: INFO: Number of nodes with available pods: 5
Dec 12 22:10:55.223: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:56.224: INFO: Number of nodes with available pods: 5
Dec 12 22:10:56.224: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:57.225: INFO: Number of nodes with available pods: 5
Dec 12 22:10:57.225: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:10:58.224: INFO: Number of nodes with available pods: 6
Dec 12 22:10:58.224: INFO: Number of running nodes: 6, number of available pods: 6
Dec 12 22:10:58.224: INFO: Update the DaemonSet to trigger a rollout
Dec 12 22:10:58.236: INFO: Updating DaemonSet daemon-set
Dec 12 22:11:06.255: INFO: Roll back the DaemonSet before rollout is complete
Dec 12 22:11:06.268: INFO: Updating DaemonSet daemon-set
Dec 12 22:11:06.268: INFO: Make sure DaemonSet rollback is complete
Dec 12 22:11:06.273: INFO: Wrong image for pod: daemon-set-7d7s8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 22:11:06.273: INFO: Pod daemon-set-7d7s8 is not available
Dec 12 22:11:07.285: INFO: Wrong image for pod: daemon-set-7d7s8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 22:11:07.285: INFO: Pod daemon-set-7d7s8 is not available
Dec 12 22:11:08.286: INFO: Wrong image for pod: daemon-set-7d7s8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 22:11:08.286: INFO: Pod daemon-set-7d7s8 is not available
Dec 12 22:11:09.286: INFO: Wrong image for pod: daemon-set-7d7s8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 22:11:09.286: INFO: Pod daemon-set-7d7s8 is not available
Dec 12 22:11:10.286: INFO: Wrong image for pod: daemon-set-7d7s8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 22:11:10.286: INFO: Pod daemon-set-7d7s8 is not available
Dec 12 22:11:11.286: INFO: Pod daemon-set-pbvfg is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8923, will wait for the garbage collector to delete the pods
Dec 12 22:11:11.363: INFO: Deleting DaemonSet.extensions daemon-set took: 11.925416ms
Dec 12 22:11:11.763: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.464923ms
Dec 12 22:11:24.368: INFO: Number of nodes with available pods: 0
Dec 12 22:11:24.368: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 22:11:24.373: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8923/daemonsets","resourceVersion":"12698"},"items":null}

Dec 12 22:11:24.377: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8923/pods","resourceVersion":"12698"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:24.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8923" for this suite.

• [SLOW TEST:43.435 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":98,"skipped":1627,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:24.418: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:28.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1935" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1639,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:28.631: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-520d353a-ca5c-4271-8048-93c526e1a001
STEP: Creating a pod to test consume secrets
Dec 12 22:11:28.858: INFO: Waiting up to 5m0s for pod "pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95" in namespace "secrets-4652" to be "success or failure"
Dec 12 22:11:28.863: INFO: Pod "pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096033ms
Dec 12 22:11:30.868: INFO: Pod "pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009845655s
Dec 12 22:11:32.874: INFO: Pod "pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015358159s
STEP: Saw pod success
Dec 12 22:11:32.874: INFO: Pod "pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95" satisfied condition "success or failure"
Dec 12 22:11:32.878: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:11:32.901: INFO: Waiting for pod pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95 to disappear
Dec 12 22:11:32.905: INFO: Pod pod-secrets-3e670294-4f2b-4ef5-bc6b-7795e9364a95 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:32.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4652" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:32.920: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:11:33.106: INFO: Creating deployment "webserver-deployment"
Dec 12 22:11:33.116: INFO: Waiting for observed generation 1
Dec 12 22:11:35.128: INFO: Waiting for all required pods to come up
Dec 12 22:11:35.134: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 12 22:11:37.155: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 12 22:11:37.161: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 12 22:11:37.172: INFO: Updating deployment webserver-deployment
Dec 12 22:11:37.172: INFO: Waiting for observed generation 2
Dec 12 22:11:39.181: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 12 22:11:39.184: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 12 22:11:39.187: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 12 22:11:39.198: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 12 22:11:39.198: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 12 22:11:39.202: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 12 22:11:39.210: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 12 22:11:39.210: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 12 22:11:39.272: INFO: Updating deployment webserver-deployment
Dec 12 22:11:39.272: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 12 22:11:39.282: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 12 22:11:39.285: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 12 22:11:41.324: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-440 /apis/apps/v1/namespaces/deployment-440/deployments/webserver-deployment 072b593a-ff9d-44db-a25a-1e268547727a 13115 3 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003357f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-12 22:11:39 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-12 22:11:39 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 12 22:11:41.329: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-440 /apis/apps/v1/namespaces/deployment-440/replicasets/webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 13106 3 2019-12-12 22:11:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 072b593a-ff9d-44db-a25a-1e268547727a 0xc002824477 0xc002824478}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0028246f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:11:41.329: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 12 22:11:41.329: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-440 /apis/apps/v1/namespaces/deployment-440/replicasets/webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 13092 3 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 072b593a-ff9d-44db-a25a-1e268547727a 0xc0028243b7 0xc0028243b8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002824418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:11:41.338: INFO: Pod "webserver-deployment-595b5b9587-6rwwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6rwwc webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-6rwwc 96125342-d2be-4cde-a546-12118b96e500 13107 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc0028251d7 0xc0028251d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.230,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.339: INFO: Pod "webserver-deployment-595b5b9587-78qjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-78qjw webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-78qjw dde22f7a-ce2f-4bbe-a874-19a0a50f91b0 13112 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc002825327 0xc002825328}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.23,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.340: INFO: Pod "webserver-deployment-595b5b9587-bl757" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bl757 webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-bl757 7f515f8a-e215-44cb-a53f-b576778d7f8d 12936 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc002825567 0xc002825568}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.230,PodIP:10.244.0.6,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a27764f9add6756bced748ded5d1fb5e93f56c6b903b90ae687841fb8f3faaff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.340: INFO: Pod "webserver-deployment-595b5b9587-f6wvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f6wvs webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-f6wvs c975021d-4170-4213-bae3-ee8c09dcba19 13086 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc0028256e0 0xc0028256e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-4kj7s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.232,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.341: INFO: Pod "webserver-deployment-595b5b9587-mlpsh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mlpsh webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-mlpsh 331dbfd7-0f66-42b3-b4e5-e28333497086 13040 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc002825907 0xc002825908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.23,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.341: INFO: Pod "webserver-deployment-595b5b9587-p7686" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p7686 webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-p7686 6f069580-af00-40bc-9a21-ee3bfcc1a81d 12947 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc002825c17 0xc002825c18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-4kj7s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.232,PodIP:10.244.3.17,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a082f814b2b2e4aa95d70140ee6275ab75f97227a185775bf746d2cf1b56b9db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.342: INFO: Pod "webserver-deployment-595b5b9587-pdqqq" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pdqqq webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-pdqqq 8c387554-e9e5-4376-9d3f-ae591cd18886 13122 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc002825ef0 0xc002825ef1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.342: INFO: Pod "webserver-deployment-595b5b9587-pnvv2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pnvv2 webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-pnvv2 7cfe479c-2e80-43c5-ac05-0bd4d6e00016 12939 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a0c7 0xc00452a0c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.218,PodIP:10.244.1.13,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f71599e9e5e0a34706da2456e9ff5320005b9eeadb6632e7329c71e471c4f513,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.342: INFO: Pod "webserver-deployment-595b5b9587-r28w8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r28w8 webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-r28w8 420399ed-db42-4769-a19b-f1b0c217df9f 13114 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a230 0xc00452a231}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.343: INFO: Pod "webserver-deployment-595b5b9587-r678b" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r678b webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-r678b 09949cdd-51ee-4bc7-bb92-c6edb57f6323 13050 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a377 0xc00452a378}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.343: INFO: Pod "webserver-deployment-595b5b9587-rfz8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rfz8s webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-rfz8s cf9e676c-06a8-4a24-89b4-ef60e398774c 13110 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a4e7 0xc00452a4e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.343: INFO: Pod "webserver-deployment-595b5b9587-rpmm8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rpmm8 webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-rpmm8 f230cc15-a751-47bf-ab5f-3764da2599d0 13039 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a637 0xc00452a638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.343: INFO: Pod "webserver-deployment-595b5b9587-rvgpd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rvgpd webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-rvgpd c0eb5cef-a09f-44b5-84c1-22902aaa1aa4 12933 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a787 0xc00452a788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.230,PodIP:10.244.0.5,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://d57b480d862efd5266797a6bb63e5d0dffcc147e59ba10776be24e7deb272e9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.344: INFO: Pod "webserver-deployment-595b5b9587-shlnb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-shlnb webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-shlnb 9d9e378a-b780-4de5-addc-ab7da2268b7c 12927 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452a8f0 0xc00452a8f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.23,PodIP:10.244.2.5,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4034af8227a9c14e7a29c1731524e6a7b577a30f351d3a2f8baef84eceb4e37c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.344: INFO: Pod "webserver-deployment-595b5b9587-tl4zp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tl4zp webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-tl4zp 0f20851e-a84c-428f-94da-af2a412fd289 13057 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452aa50 0xc00452aa51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.218,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.344: INFO: Pod "webserver-deployment-595b5b9587-v8snp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-v8snp webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-v8snp d4408dc2-70a5-4bff-89d7-4a4c452edd1e 13120 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452ab97 0xc00452ab98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.345: INFO: Pod "webserver-deployment-595b5b9587-vskdm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vskdm webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-vskdm fd6baa38-460b-4964-89b9-9c470e825b06 13113 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452ace7 0xc00452ace8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.218,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.345: INFO: Pod "webserver-deployment-595b5b9587-whsq5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-whsq5 webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-whsq5 725b9bb2-249d-4c8d-b1ff-a8978396a3bd 12944 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452ae37 0xc00452ae38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:10.244.4.92,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a4b1234d067605359b60f8b8631b494d2f0c579127dcf97da2313478ff4a782f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.345: INFO: Pod "webserver-deployment-595b5b9587-x8nzr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-x8nzr webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-x8nzr 6f3cfdb0-d32b-4e74-8a73-834c43e16580 12940 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452afa0 0xc00452afa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:10.244.4.91,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://fe1449fdec9ee2eda84e2657e74362b2af45649f3755de8235cb2531e0d6f5c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.345: INFO: Pod "webserver-deployment-595b5b9587-xct8v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xct8v webserver-deployment-595b5b9587- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-595b5b9587-xct8v d0904038-82f4-46e5-9ccc-aa9c98887cee 12909 0 2019-12-12 22:11:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f630831d-0c12-4bac-bd1d-84d0c20d776e 0xc00452b100 0xc00452b101}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-4kj7s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.232,PodIP:10.244.3.16,StartTime:2019-12-12 22:11:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:11:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://d161eb356fe1cfd906e194623bc859eed517b95b38628dcc2225635f4700ff37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.346: INFO: Pod "webserver-deployment-c7997dcc8-5bvwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5bvwp webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-5bvwp c5474085-fc1e-41d6-90be-2900a2a45b24 13163 0 2019-12-12 22:11:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452b260 0xc00452b261}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:10.244.4.93,StartTime:2019-12-12 22:11:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.346: INFO: Pod "webserver-deployment-c7997dcc8-8cn2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8cn2d webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-8cn2d 34c657f9-1c4c-4ce5-b526-e7ba95141de9 13147 0 2019-12-12 22:11:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452b4e0 0xc00452b4e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.23,PodIP:10.244.2.6,StartTime:2019-12-12 22:11:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.346: INFO: Pod "webserver-deployment-c7997dcc8-ddvh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ddvh8 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-ddvh8 6aeb987d-6e16-43f8-a09e-1fe1c6ea6197 13038 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452b6f0 0xc00452b6f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.218,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.347: INFO: Pod "webserver-deployment-c7997dcc8-gsj46" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gsj46 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-gsj46 09665623-cff7-4f9a-9eb7-021485689c65 13167 0 2019-12-12 22:11:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452b857 0xc00452b858}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-4kj7s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.232,PodIP:10.244.3.18,StartTime:2019-12-12 22:11:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.347: INFO: Pod "webserver-deployment-c7997dcc8-kksq7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kksq7 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-kksq7 15781d7c-c926-4ed3-9489-2c057165207c 13117 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452b9f0 0xc00452b9f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.230,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.347: INFO: Pod "webserver-deployment-c7997dcc8-ncn67" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ncn67 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-ncn67 ffd8c09b-50a3-4bfa-889f-2520323c7eb8 13093 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452bb57 0xc00452bb58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.347: INFO: Pod "webserver-deployment-c7997dcc8-ql5n7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ql5n7 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-ql5n7 686ef346-de83-48a4-81d3-d79a523bfef5 13104 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452bcc7 0xc00452bcc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-nt7qc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.219,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.348: INFO: Pod "webserver-deployment-c7997dcc8-qpqf9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qpqf9 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-qpqf9 003e777e-5de2-44b9-b2a6-943068b2662c 13128 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452be37 0xc00452be38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.348: INFO: Pod "webserver-deployment-c7997dcc8-r25mz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-r25mz webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-r25mz 5710ee9c-0e54-4d5f-a9cb-46cc771749d3 13124 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc00452bfa7 0xc00452bfa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.218,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.348: INFO: Pod "webserver-deployment-c7997dcc8-rq995" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rq995 webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-rq995 633bf815-a961-4121-af0e-2cc54ee1ade8 13123 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc004470117 0xc004470118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.23,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.348: INFO: Pod "webserver-deployment-c7997dcc8-stzcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-stzcv webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-stzcv fc5115d3-329d-4784-8280-bdc6f4ab6805 13146 0 2019-12-12 22:11:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc004470280 0xc004470281}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:10.244.5.36,StartTime:2019-12-12 22:11:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.365: INFO: Pod "webserver-deployment-c7997dcc8-tpw5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tpw5s webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-tpw5s 4c166738-b1a4-41ba-830b-5c6120866d99 13111 0 2019-12-12 22:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc004470410 0xc004470411}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-4kj7s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.232,PodIP:,StartTime:2019-12-12 22:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 22:11:41.365: INFO: Pod "webserver-deployment-c7997dcc8-vbk7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vbk7k webserver-deployment-c7997dcc8- deployment-440 /api/v1/namespaces/deployment-440/pods/webserver-deployment-c7997dcc8-vbk7k e14bac39-8c03-4e35-a9fd-3f6e78372206 13157 0 2019-12-12 22:11:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 5499e421-8648-455b-b6f3-247d1d6de0c9 0xc004470577 0xc004470578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8f74c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8f74c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8f74c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-controlplane-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:11:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.230,PodIP:10.244.0.7,StartTime:2019-12-12 22:11:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:41.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-440" for this suite.

• [SLOW TEST:8.461 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":101,"skipped":1680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:41.383: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-6447/configmap-test-3cd772a4-8e72-4b5f-91cc-d9de38c84f47
STEP: Creating a pod to test consume configMaps
Dec 12 22:11:41.597: INFO: Waiting up to 5m0s for pod "pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594" in namespace "configmap-6447" to be "success or failure"
Dec 12 22:11:41.638: INFO: Pod "pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594": Phase="Pending", Reason="", readiness=false. Elapsed: 40.770051ms
Dec 12 22:11:43.644: INFO: Pod "pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046421663s
Dec 12 22:11:45.649: INFO: Pod "pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051607191s
STEP: Saw pod success
Dec 12 22:11:45.649: INFO: Pod "pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594" satisfied condition "success or failure"
Dec 12 22:11:45.654: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594 container env-test: <nil>
STEP: delete the pod
Dec 12 22:11:45.681: INFO: Waiting for pod pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594 to disappear
Dec 12 22:11:45.685: INFO: Pod pod-configmaps-51d1cf4f-80b5-4007-a92f-de5a41d55594 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:45.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6447" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1723,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 12 22:11:45.899: INFO: Waiting up to 5m0s for pod "pod-d37215d8-e671-40cb-a98c-e61bdf461ffc" in namespace "emptydir-3529" to be "success or failure"
Dec 12 22:11:45.902: INFO: Pod "pod-d37215d8-e671-40cb-a98c-e61bdf461ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976261ms
Dec 12 22:11:47.906: INFO: Pod "pod-d37215d8-e671-40cb-a98c-e61bdf461ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007568639s
Dec 12 22:11:49.911: INFO: Pod "pod-d37215d8-e671-40cb-a98c-e61bdf461ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01285142s
STEP: Saw pod success
Dec 12 22:11:49.911: INFO: Pod "pod-d37215d8-e671-40cb-a98c-e61bdf461ffc" satisfied condition "success or failure"
Dec 12 22:11:49.916: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-4kj7s pod pod-d37215d8-e671-40cb-a98c-e61bdf461ffc container test-container: <nil>
STEP: delete the pod
Dec 12 22:11:49.961: INFO: Waiting for pod pod-d37215d8-e671-40cb-a98c-e61bdf461ffc to disappear
Dec 12 22:11:49.966: INFO: Pod pod-d37215d8-e671-40cb-a98c-e61bdf461ffc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:49.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3529" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1732,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:49.979: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-0df664ea-7235-4265-b9f4-0c3f174c7a6c
STEP: Creating a pod to test consume configMaps
Dec 12 22:11:50.167: INFO: Waiting up to 5m0s for pod "pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1" in namespace "configmap-6937" to be "success or failure"
Dec 12 22:11:50.175: INFO: Pod "pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.895415ms
Dec 12 22:11:52.180: INFO: Pod "pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013151892s
Dec 12 22:11:54.187: INFO: Pod "pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019557369s
STEP: Saw pod success
Dec 12 22:11:54.187: INFO: Pod "pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1" satisfied condition "success or failure"
Dec 12 22:11:54.192: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:11:54.223: INFO: Waiting for pod pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1 to disappear
Dec 12 22:11:54.229: INFO: Pod pod-configmaps-310fd6d4-9f73-4a73-94bf-d46bedc1efe1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:11:54.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6937" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1735,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:11:54.251: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1595, will wait for the garbage collector to delete the pods
Dec 12 22:11:58.500: INFO: Deleting Job.batch foo took: 11.384836ms
Dec 12 22:11:58.601: INFO: Terminating Job.batch foo pods took: 100.282401ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:12:33.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1595" for this suite.

• [SLOW TEST:39.367 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":105,"skipped":1751,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:12:33.619: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:12:33.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382" in namespace "projected-8493" to be "success or failure"
Dec 12 22:12:33.803: INFO: Pod "downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382": Phase="Pending", Reason="", readiness=false. Elapsed: 4.495616ms
Dec 12 22:12:35.808: INFO: Pod "downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009290579s
Dec 12 22:12:37.813: INFO: Pod "downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014633808s
STEP: Saw pod success
Dec 12 22:12:37.813: INFO: Pod "downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382" satisfied condition "success or failure"
Dec 12 22:12:37.817: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382 container client-container: <nil>
STEP: delete the pod
Dec 12 22:12:37.843: INFO: Waiting for pod downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382 to disappear
Dec 12 22:12:37.847: INFO: Pod downwardapi-volume-e1900431-4eda-4589-ba06-c65ca8f36382 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:12:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8493" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1755,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:12:37.859: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 12 22:12:38.059: INFO: Waiting up to 5m0s for pod "pod-894bf709-5662-45f7-86df-6bae98d425ec" in namespace "emptydir-3846" to be "success or failure"
Dec 12 22:12:38.066: INFO: Pod "pod-894bf709-5662-45f7-86df-6bae98d425ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.386664ms
Dec 12 22:12:40.071: INFO: Pod "pod-894bf709-5662-45f7-86df-6bae98d425ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011848378s
Dec 12 22:12:42.076: INFO: Pod "pod-894bf709-5662-45f7-86df-6bae98d425ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01691338s
STEP: Saw pod success
Dec 12 22:12:42.076: INFO: Pod "pod-894bf709-5662-45f7-86df-6bae98d425ec" satisfied condition "success or failure"
Dec 12 22:12:42.080: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-894bf709-5662-45f7-86df-6bae98d425ec container test-container: <nil>
STEP: delete the pod
Dec 12 22:12:42.099: INFO: Waiting for pod pod-894bf709-5662-45f7-86df-6bae98d425ec to disappear
Dec 12 22:12:42.104: INFO: Pod pod-894bf709-5662-45f7-86df-6bae98d425ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:12:42.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3846" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1759,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:12:42.120: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 12 22:12:42.291: INFO: Waiting up to 5m0s for pod "downward-api-15761390-8895-4f81-b227-d8fa42f8f223" in namespace "downward-api-7635" to be "success or failure"
Dec 12 22:12:42.295: INFO: Pod "downward-api-15761390-8895-4f81-b227-d8fa42f8f223": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555189ms
Dec 12 22:12:44.300: INFO: Pod "downward-api-15761390-8895-4f81-b227-d8fa42f8f223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009146473s
Dec 12 22:12:46.305: INFO: Pod "downward-api-15761390-8895-4f81-b227-d8fa42f8f223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014697909s
STEP: Saw pod success
Dec 12 22:12:46.306: INFO: Pod "downward-api-15761390-8895-4f81-b227-d8fa42f8f223" satisfied condition "success or failure"
Dec 12 22:12:46.309: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downward-api-15761390-8895-4f81-b227-d8fa42f8f223 container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:12:46.365: INFO: Waiting for pod downward-api-15761390-8895-4f81-b227-d8fa42f8f223 to disappear
Dec 12 22:12:46.368: INFO: Pod downward-api-15761390-8895-4f81-b227-d8fa42f8f223 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:12:46.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7635" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":108,"skipped":1776,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:12:46.383: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:12:46.574: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d7bb62db-a4fe-4394-bf5e-42074ad12ee4" in namespace "security-context-test-6250" to be "success or failure"
Dec 12 22:12:46.578: INFO: Pod "busybox-privileged-false-d7bb62db-a4fe-4394-bf5e-42074ad12ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.693942ms
Dec 12 22:12:48.583: INFO: Pod "busybox-privileged-false-d7bb62db-a4fe-4394-bf5e-42074ad12ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009072463s
Dec 12 22:12:50.587: INFO: Pod "busybox-privileged-false-d7bb62db-a4fe-4394-bf5e-42074ad12ee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013668838s
Dec 12 22:12:50.587: INFO: Pod "busybox-privileged-false-d7bb62db-a4fe-4394-bf5e-42074ad12ee4" satisfied condition "success or failure"
Dec 12 22:12:50.595: INFO: Got logs for pod "busybox-privileged-false-d7bb62db-a4fe-4394-bf5e-42074ad12ee4": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:12:50.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6250" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":109,"skipped":1786,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:12:50.608: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5493
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-5493
Dec 12 22:12:50.856: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 22:13:00.861: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Dec 12 22:13:00.886: INFO: Deleting all statefulset in ns statefulset-5493
Dec 12 22:13:00.889: INFO: Scaling statefulset ss to 0
Dec 12 22:13:20.915: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 22:13:20.919: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:20.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5493" for this suite.

• [SLOW TEST:30.342 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":110,"skipped":1789,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5199
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4964
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:27.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5363" for this suite.
STEP: Destroying namespace "nsdeletetest-5199" for this suite.
Dec 12 22:13:27.575: INFO: Namespace nsdeletetest-5199 was already deleted
STEP: Destroying namespace "nsdeletetest-4964" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":111,"skipped":1795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:27.586: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-8477c444-5747-48b8-8c86-0130220ee196
STEP: Creating a pod to test consume configMaps
Dec 12 22:13:27.789: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b" in namespace "projected-9221" to be "success or failure"
Dec 12 22:13:27.793: INFO: Pod "pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.641881ms
Dec 12 22:13:29.799: INFO: Pod "pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01075261s
Dec 12 22:13:31.804: INFO: Pod "pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015746446s
STEP: Saw pod success
Dec 12 22:13:31.804: INFO: Pod "pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b" satisfied condition "success or failure"
Dec 12 22:13:31.809: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:13:31.836: INFO: Waiting for pod pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b to disappear
Dec 12 22:13:31.840: INFO: Pod pod-projected-configmaps-7284aa6d-3e40-4429-851a-bcf458032c3b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:31.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9221" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":112,"skipped":1842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:31.855: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:13:32.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a" in namespace "downward-api-8844" to be "success or failure"
Dec 12 22:13:32.044: INFO: Pod "downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272969ms
Dec 12 22:13:34.048: INFO: Pod "downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009749283s
STEP: Saw pod success
Dec 12 22:13:34.049: INFO: Pod "downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a" satisfied condition "success or failure"
Dec 12 22:13:34.053: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a container client-container: <nil>
STEP: delete the pod
Dec 12 22:13:34.076: INFO: Waiting for pod downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a to disappear
Dec 12 22:13:34.082: INFO: Pod downwardapi-volume-0c44668d-4a37-4471-980f-0313ffce252a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:34.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8844" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":1867,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:34.096: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Dec 12 22:13:34.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-1239'
Dec 12 22:13:34.651: INFO: stderr: ""
Dec 12 22:13:34.652: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 22:13:34.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1239'
Dec 12 22:13:34.741: INFO: stderr: ""
Dec 12 22:13:34.741: INFO: stdout: "update-demo-nautilus-j67z5 update-demo-nautilus-rdwps "
Dec 12 22:13:34.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-j67z5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1239'
Dec 12 22:13:34.833: INFO: stderr: ""
Dec 12 22:13:34.833: INFO: stdout: ""
Dec 12 22:13:34.833: INFO: update-demo-nautilus-j67z5 is created but not running
Dec 12 22:13:39.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1239'
Dec 12 22:13:39.924: INFO: stderr: ""
Dec 12 22:13:39.924: INFO: stdout: "update-demo-nautilus-j67z5 update-demo-nautilus-rdwps "
Dec 12 22:13:39.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-j67z5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1239'
Dec 12 22:13:40.010: INFO: stderr: ""
Dec 12 22:13:40.010: INFO: stdout: "true"
Dec 12 22:13:40.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-j67z5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1239'
Dec 12 22:13:40.102: INFO: stderr: ""
Dec 12 22:13:40.102: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:13:40.102: INFO: validating pod update-demo-nautilus-j67z5
Dec 12 22:13:40.110: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:13:40.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:13:40.110: INFO: update-demo-nautilus-j67z5 is verified up and running
Dec 12 22:13:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-rdwps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1239'
Dec 12 22:13:40.199: INFO: stderr: ""
Dec 12 22:13:40.199: INFO: stdout: "true"
Dec 12 22:13:40.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-rdwps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1239'
Dec 12 22:13:40.285: INFO: stderr: ""
Dec 12 22:13:40.285: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:13:40.285: INFO: validating pod update-demo-nautilus-rdwps
Dec 12 22:13:40.294: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:13:40.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:13:40.294: INFO: update-demo-nautilus-rdwps is verified up and running
STEP: using delete to clean up resources
Dec 12 22:13:40.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-1239'
Dec 12 22:13:40.389: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:13:40.389: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 12 22:13:40.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1239'
Dec 12 22:13:40.482: INFO: stderr: "No resources found in kubectl-1239 namespace.\n"
Dec 12 22:13:40.482: INFO: stdout: ""
Dec 12 22:13:40.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -l name=update-demo --namespace=kubectl-1239 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 22:13:40.569: INFO: stderr: ""
Dec 12 22:13:40.569: INFO: stdout: "update-demo-nautilus-j67z5\nupdate-demo-nautilus-rdwps\n"
Dec 12 22:13:41.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1239'
Dec 12 22:13:41.161: INFO: stderr: "No resources found in kubectl-1239 namespace.\n"
Dec 12 22:13:41.161: INFO: stdout: ""
Dec 12 22:13:41.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -l name=update-demo --namespace=kubectl-1239 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 22:13:41.287: INFO: stderr: ""
Dec 12 22:13:41.287: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:41.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1239" for this suite.

• [SLOW TEST:7.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":114,"skipped":1869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:41.307: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 22:13:41.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2434'
Dec 12 22:13:41.598: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 22:13:41.598: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 12 22:13:41.612: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-rvv7c]
Dec 12 22:13:41.612: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-rvv7c" in namespace "kubectl-2434" to be "running and ready"
Dec 12 22:13:41.615: INFO: Pod "e2e-test-httpd-rc-rvv7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.983612ms
Dec 12 22:13:43.619: INFO: Pod "e2e-test-httpd-rc-rvv7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007193546s
Dec 12 22:13:45.624: INFO: Pod "e2e-test-httpd-rc-rvv7c": Phase="Running", Reason="", readiness=true. Elapsed: 4.01169265s
Dec 12 22:13:45.624: INFO: Pod "e2e-test-httpd-rc-rvv7c" satisfied condition "running and ready"
Dec 12 22:13:45.624: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-rvv7c]
Dec 12 22:13:45.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs rc/e2e-test-httpd-rc --namespace=kubectl-2434'
Dec 12 22:13:45.761: INFO: stderr: ""
Dec 12 22:13:45.761: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.4.108. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.4.108. Set the 'ServerName' directive globally to suppress this message\n[Thu Dec 12 22:13:43.489455 2019] [mpm_event:notice] [pid 1:tid 140670564997992] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Thu Dec 12 22:13:43.489525 2019] [core:notice] [pid 1:tid 140670564997992] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Dec 12 22:13:45.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete rc e2e-test-httpd-rc --namespace=kubectl-2434'
Dec 12 22:13:45.862: INFO: stderr: ""
Dec 12 22:13:45.862: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:45.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2434" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":115,"skipped":1895,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:45.904: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1841
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 22:13:46.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7029'
Dec 12 22:13:46.173: INFO: stderr: ""
Dec 12 22:13:46.173: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1846
Dec 12 22:13:46.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete pods e2e-test-httpd-pod --namespace=kubectl-7029'
Dec 12 22:13:53.557: INFO: stderr: ""
Dec 12 22:13:53.557: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7029" for this suite.

• [SLOW TEST:7.667 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1837
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":116,"skipped":1905,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 12 22:13:53.785: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 12 22:13:53.799: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 22:13:53.802: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-0 before test
Dec 12 22:13:53.830: INFO: pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.830: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:13:53.830: INFO: kube-flannel-69ftd from kube-system started at 2019-12-12 21:35:43 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.830: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:13:53.830: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:13:53.830: INFO: pod-checkpointer-wbndz from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.830: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:13:53.830: INFO: kube-proxy-bkbtw from kube-system started at 2019-12-12 21:35:43 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.830: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:13:53.830: INFO: kube-apiserver-9sv5v from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.830: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 12 22:13:53.830: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-1 before test
Dec 12 22:13:53.859: INFO: kube-apiserver-j24rk from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.859: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:13:53.859: INFO: kube-proxy-mb48g from kube-system started at 2019-12-12 21:35:36 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.859: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:13:53.859: INFO: kube-controller-manager-5c547b548-hxgrq from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.859: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 12 22:13:53.859: INFO: pod-checkpointer-8ftvl-talos-0-3-0-beta-0-gcp-controlplane-1 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:13:53.860: INFO: pod-checkpointer-8ftvl from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:13:53.860: INFO: coredns-5bcc66f5bd-6pzdf from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container coredns ready: true, restart count 0
Dec 12 22:13:53.860: INFO: kube-scheduler-6db5bfb7bc-tx7bk from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 12 22:13:53.860: INFO: kube-flannel-cq4nw from kube-system started at 2019-12-12 21:35:36 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:13:53.860: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:13:53.860: INFO: kube-scheduler-6db5bfb7bc-dplt8 from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 12 22:13:53.860: INFO: kube-controller-manager-5c547b548-cqqln from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.860: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 12 22:13:53.860: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-2 before test
Dec 12 22:13:53.881: INFO: kube-flannel-9hznp from kube-system started at 2019-12-12 21:35:29 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.881: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:13:53.881: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:13:53.881: INFO: kube-apiserver-kf44j from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.881: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:13:53.881: INFO: kube-proxy-9wbfd from kube-system started at 2019-12-12 21:35:29 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.881: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:13:53.881: INFO: pod-checkpointer-5mnr9-talos-0-3-0-beta-0-gcp-controlplane-2 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.881: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:13:53.881: INFO: pod-checkpointer-5mnr9 from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.881: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:13:53.881: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-4kj7s before test
Dec 12 22:13:53.899: INFO: kube-flannel-bz9ht from kube-system started at 2019-12-12 21:36:55 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.899: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:13:53.899: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:13:53.899: INFO: kube-proxy-xh54v from kube-system started at 2019-12-12 21:36:55 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.899: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:13:53.899: INFO: sonobuoy-e2e-job-01c7b380be144838 from sonobuoy started at 2019-12-12 21:39:37 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.899: INFO: 	Container e2e ready: true, restart count 0
Dec 12 22:13:53.899: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 12 22:13:53.899: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-nt7qc before test
Dec 12 22:13:53.910: INFO: kube-flannel-p79j9 from kube-system started at 2019-12-12 21:37:10 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.910: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:13:53.910: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:13:53.910: INFO: kube-proxy-jtlmf from kube-system started at 2019-12-12 21:37:10 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.910: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:13:53.910: INFO: e2e-test-httpd-rc-rvv7c from kubectl-2434 started at 2019-12-12 22:13:41 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.910: INFO: 	Container e2e-test-httpd-rc ready: false, restart count 0
Dec 12 22:13:53.910: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-ntvmn before test
Dec 12 22:13:53.931: INFO: kube-proxy-bvldz from kube-system started at 2019-12-12 21:37:28 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.931: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:13:53.931: INFO: kube-flannel-gk4cx from kube-system started at 2019-12-12 21:37:28 +0000 UTC (2 container statuses recorded)
Dec 12 22:13:53.931: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:13:53.931: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:13:53.931: INFO: sonobuoy from sonobuoy started at 2019-12-12 21:39:34 +0000 UTC (1 container statuses recorded)
Dec 12 22:13:53.931: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dfbf64bdb73bf2], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dfbf64be59c803], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:54.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6518" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":117,"skipped":1921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:54.974: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 12 22:13:55.159: INFO: Waiting up to 5m0s for pod "downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7" in namespace "downward-api-6502" to be "success or failure"
Dec 12 22:13:55.184: INFO: Pod "downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.124847ms
Dec 12 22:13:57.188: INFO: Pod "downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029217596s
Dec 12 22:13:59.194: INFO: Pod "downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034605469s
STEP: Saw pod success
Dec 12 22:13:59.194: INFO: Pod "downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7" satisfied condition "success or failure"
Dec 12 22:13:59.198: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7 container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:13:59.225: INFO: Waiting for pod downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7 to disappear
Dec 12 22:13:59.228: INFO: Pod downward-api-b9c84eb0-a6f1-4dc6-87fd-5aba9877edd7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:13:59.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6502" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":118,"skipped":1964,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:13:59.243: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:14:10.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6326" for this suite.

• [SLOW TEST:11.252 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":119,"skipped":1967,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:14:10.557: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:14:17.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3800" for this suite.

• [SLOW TEST:7.233 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":120,"skipped":1967,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:14:17.790: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 12 22:14:22.537: INFO: Successfully updated pod "annotationupdate0d94ab85-8543-4389-aa18-f675719c8b81"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:14:24.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2255" for this suite.

• [SLOW TEST:6.793 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":1969,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:14:24.583: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:14:44.786: INFO: Container started at 2019-12-12 22:14:27 +0000 UTC, pod became ready at 2019-12-12 22:14:44 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:14:44.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9719" for this suite.

• [SLOW TEST:20.215 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":122,"skipped":1970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:14:44.800: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:14:44.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba" in namespace "projected-9774" to be "success or failure"
Dec 12 22:14:44.986: INFO: Pod "downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328112ms
Dec 12 22:14:46.990: INFO: Pod "downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008857044s
Dec 12 22:14:48.995: INFO: Pod "downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013863098s
STEP: Saw pod success
Dec 12 22:14:48.995: INFO: Pod "downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba" satisfied condition "success or failure"
Dec 12 22:14:49.000: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba container client-container: <nil>
STEP: delete the pod
Dec 12 22:14:49.025: INFO: Waiting for pod downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba to disappear
Dec 12 22:14:49.029: INFO: Pod downwardapi-volume-0be30975-cc3a-4fdb-a087-3624ebc6bbba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:14:49.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9774" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":1992,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:14:49.041: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8651
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 12 22:14:49.204: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 12 22:15:00.526: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:15:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:15:14.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8651" for this suite.

• [SLOW TEST:25.834 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":124,"skipped":1999,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:15:14.876: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 12 22:15:15.103: INFO: Number of nodes with available pods: 0
Dec 12 22:15:15.103: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:16.126: INFO: Number of nodes with available pods: 0
Dec 12 22:15:16.126: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:17.120: INFO: Number of nodes with available pods: 0
Dec 12 22:15:17.120: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:18.114: INFO: Number of nodes with available pods: 1
Dec 12 22:15:18.114: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:19.114: INFO: Number of nodes with available pods: 6
Dec 12 22:15:19.114: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 12 22:15:19.152: INFO: Number of nodes with available pods: 5
Dec 12 22:15:19.152: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:20.191: INFO: Number of nodes with available pods: 5
Dec 12 22:15:20.191: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:21.164: INFO: Number of nodes with available pods: 5
Dec 12 22:15:21.164: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:22.165: INFO: Number of nodes with available pods: 5
Dec 12 22:15:22.165: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:15:23.166: INFO: Number of nodes with available pods: 6
Dec 12 22:15:23.166: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9639, will wait for the garbage collector to delete the pods
Dec 12 22:15:23.244: INFO: Deleting DaemonSet.extensions daemon-set took: 13.025857ms
Dec 12 22:15:23.644: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.227168ms
Dec 12 22:15:36.450: INFO: Number of nodes with available pods: 0
Dec 12 22:15:36.450: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 22:15:36.454: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9639/daemonsets","resourceVersion":"15033"},"items":null}

Dec 12 22:15:36.460: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9639/pods","resourceVersion":"15033"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:15:36.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9639" for this suite.

• [SLOW TEST:21.633 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":125,"skipped":2012,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:15:36.510: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5562
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Dec 12 22:15:36.677: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:15:53.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5562" for this suite.

• [SLOW TEST:16.677 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":126,"skipped":2022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:15:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 12 22:15:53.399: INFO: Waiting up to 5m0s for pod "pod-180d172f-6760-4f0d-ad61-baf5a38e2494" in namespace "emptydir-495" to be "success or failure"
Dec 12 22:15:53.408: INFO: Pod "pod-180d172f-6760-4f0d-ad61-baf5a38e2494": Phase="Pending", Reason="", readiness=false. Elapsed: 8.704593ms
Dec 12 22:15:55.415: INFO: Pod "pod-180d172f-6760-4f0d-ad61-baf5a38e2494": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015543471s
Dec 12 22:15:57.422: INFO: Pod "pod-180d172f-6760-4f0d-ad61-baf5a38e2494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022235782s
STEP: Saw pod success
Dec 12 22:15:57.422: INFO: Pod "pod-180d172f-6760-4f0d-ad61-baf5a38e2494" satisfied condition "success or failure"
Dec 12 22:15:57.426: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-180d172f-6760-4f0d-ad61-baf5a38e2494 container test-container: <nil>
STEP: delete the pod
Dec 12 22:15:57.490: INFO: Waiting for pod pod-180d172f-6760-4f0d-ad61-baf5a38e2494 to disappear
Dec 12 22:15:57.495: INFO: Pod pod-180d172f-6760-4f0d-ad61-baf5a38e2494 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:15:57.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-495" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2049,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:15:57.511: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6333
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-dfb7273e-a09f-45ae-bd3f-f52c0bbe7a1d
STEP: Creating secret with name s-test-opt-upd-391756a4-fcec-4ef3-bd2b-5cf32cf6f576
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dfb7273e-a09f-45ae-bd3f-f52c0bbe7a1d
STEP: Updating secret s-test-opt-upd-391756a4-fcec-4ef3-bd2b-5cf32cf6f576
STEP: Creating secret with name s-test-opt-create-30ce91cd-f0d7-480b-9573-c67782bd6bf7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:17:34.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6333" for this suite.

• [SLOW TEST:96.789 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":2056,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:17:34.303: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3085
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:17:34.488: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 12 22:17:36.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-3085 create -f -'
Dec 12 22:17:37.491: INFO: stderr: ""
Dec 12 22:17:37.491: INFO: stdout: "e2e-test-crd-publish-openapi-4510-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 12 22:17:37.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-3085 delete e2e-test-crd-publish-openapi-4510-crds test-cr'
Dec 12 22:17:37.594: INFO: stderr: ""
Dec 12 22:17:37.595: INFO: stdout: "e2e-test-crd-publish-openapi-4510-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 12 22:17:37.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-3085 apply -f -'
Dec 12 22:17:37.967: INFO: stderr: ""
Dec 12 22:17:37.967: INFO: stdout: "e2e-test-crd-publish-openapi-4510-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 12 22:17:37.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-3085 delete e2e-test-crd-publish-openapi-4510-crds test-cr'
Dec 12 22:17:38.073: INFO: stderr: ""
Dec 12 22:17:38.073: INFO: stdout: "e2e-test-crd-publish-openapi-4510-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 12 22:17:38.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-4510-crds'
Dec 12 22:17:38.254: INFO: stderr: ""
Dec 12 22:17:38.254: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4510-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:17:41.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3085" for this suite.

• [SLOW TEST:6.842 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":129,"skipped":2077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:17:41.146: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8638
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Dec 12 22:17:41.328: INFO: Waiting up to 5m0s for pod "var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd" in namespace "var-expansion-8638" to be "success or failure"
Dec 12 22:17:41.334: INFO: Pod "var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607463ms
Dec 12 22:17:43.339: INFO: Pod "var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010559542s
Dec 12 22:17:45.344: INFO: Pod "var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015562309s
STEP: Saw pod success
Dec 12 22:17:45.344: INFO: Pod "var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd" satisfied condition "success or failure"
Dec 12 22:17:45.347: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:17:45.375: INFO: Waiting for pod var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd to disappear
Dec 12 22:17:45.379: INFO: Pod var-expansion-2ebce710-99f5-48e4-9853-520deb516ecd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:17:45.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8638" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":130,"skipped":2106,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:17:45.398: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-c64ffe0b-5527-4e72-b027-c80cb6546add
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:17:45.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5003" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":131,"skipped":2123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:17:45.637: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9556
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-e591f6bb-835e-4bdb-b779-a91d3e2411b5
STEP: Creating configMap with name cm-test-opt-upd-a43b1878-0b7f-4569-97be-a56af2fefc28
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e591f6bb-835e-4bdb-b779-a91d3e2411b5
STEP: Updating configmap cm-test-opt-upd-a43b1878-0b7f-4569-97be-a56af2fefc28
STEP: Creating configMap with name cm-test-opt-create-3b5a1be6-3217-479a-a59f-e16a437cafc7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:18:52.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9556" for this suite.

• [SLOW TEST:66.767 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":132,"skipped":2168,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:18:52.407: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 12 22:18:52.588: INFO: Waiting up to 5m0s for pod "downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f" in namespace "downward-api-4799" to be "success or failure"
Dec 12 22:18:52.592: INFO: Pod "downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271816ms
Dec 12 22:18:54.607: INFO: Pod "downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019651133s
Dec 12 22:18:56.613: INFO: Pod "downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024938209s
STEP: Saw pod success
Dec 12 22:18:56.613: INFO: Pod "downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f" satisfied condition "success or failure"
Dec 12 22:18:56.617: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:18:56.657: INFO: Waiting for pod downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f to disappear
Dec 12 22:18:56.662: INFO: Pod downward-api-ed1f15e6-6ce4-4cce-99b8-532da5884f7f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:18:56.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4799" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":133,"skipped":2179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:18:56.680: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:18:56.860: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-676eec3b-c884-487c-a9db-f3d3d4268c9d" in namespace "security-context-test-9719" to be "success or failure"
Dec 12 22:18:56.864: INFO: Pod "busybox-readonly-false-676eec3b-c884-487c-a9db-f3d3d4268c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.983075ms
Dec 12 22:18:58.868: INFO: Pod "busybox-readonly-false-676eec3b-c884-487c-a9db-f3d3d4268c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008321099s
Dec 12 22:19:00.873: INFO: Pod "busybox-readonly-false-676eec3b-c884-487c-a9db-f3d3d4268c9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0124908s
Dec 12 22:19:00.873: INFO: Pod "busybox-readonly-false-676eec3b-c884-487c-a9db-f3d3d4268c9d" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:00.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9719" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2207,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:00.891: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-978ebcf3-7b72-4e0b-8d33-3b8db2dda099
STEP: Creating a pod to test consume configMaps
Dec 12 22:19:01.091: INFO: Waiting up to 5m0s for pod "pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925" in namespace "configmap-9812" to be "success or failure"
Dec 12 22:19:01.100: INFO: Pod "pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925": Phase="Pending", Reason="", readiness=false. Elapsed: 8.501232ms
Dec 12 22:19:03.105: INFO: Pod "pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013277598s
Dec 12 22:19:05.109: INFO: Pod "pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017405097s
STEP: Saw pod success
Dec 12 22:19:05.109: INFO: Pod "pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925" satisfied condition "success or failure"
Dec 12 22:19:05.112: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:19:05.138: INFO: Waiting for pod pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925 to disappear
Dec 12 22:19:05.142: INFO: Pod pod-configmaps-f6b195f8-643c-48bf-a07b-761ed1afb925 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:05.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9812" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:05.156: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1877
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 22:19:05.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7357'
Dec 12 22:19:05.430: INFO: stderr: ""
Dec 12 22:19:05.430: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 12 22:19:10.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pod e2e-test-httpd-pod --namespace=kubectl-7357 -o json'
Dec 12 22:19:10.582: INFO: stderr: ""
Dec 12 22:19:10.582: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-12-12T22:19:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7357\",\n        \"resourceVersion\": \"15986\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7357/pods/e2e-test-httpd-pod\",\n        \"uid\": \"2d18a4e6-727e-4975-b975-4cedcddd063a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-875mb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"talos-0-3-0-beta-0-gcp-workers-nt7qc\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-875mb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-875mb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T22:19:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T22:19:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T22:19:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T22:19:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f65c20880d3128b935c142bfbe80904df2f1e685feff3b3c0d2b7eb1fca050a4\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-12T22:19:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.15.219\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.118\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.4.118\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-12T22:19:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 12 22:19:10.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 replace -f - --namespace=kubectl-7357'
Dec 12 22:19:11.010: INFO: stderr: ""
Dec 12 22:19:11.010: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1882
Dec 12 22:19:11.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete pods e2e-test-httpd-pod --namespace=kubectl-7357'
Dec 12 22:19:12.738: INFO: stderr: ""
Dec 12 22:19:12.739: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:12.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7357" for this suite.

• [SLOW TEST:7.596 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1873
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":136,"skipped":2251,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:12.753: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 22:19:12.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3608'
Dec 12 22:19:13.079: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 22:19:13.079: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1582
Dec 12 22:19:13.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3608'
Dec 12 22:19:13.199: INFO: stderr: ""
Dec 12 22:19:13.199: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:13.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3608" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":137,"skipped":2260,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:13.215: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Dec 12 22:19:13.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-4166'
Dec 12 22:19:13.684: INFO: stderr: ""
Dec 12 22:19:13.684: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 12 22:19:14.689: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:19:14.689: INFO: Found 0 / 1
Dec 12 22:19:15.689: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:19:15.689: INFO: Found 0 / 1
Dec 12 22:19:16.689: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:19:16.689: INFO: Found 0 / 1
Dec 12 22:19:17.689: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:19:17.689: INFO: Found 1 / 1
Dec 12 22:19:17.689: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 12 22:19:17.694: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:19:17.694: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 12 22:19:17.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 patch pod agnhost-master-j7lc6 --namespace=kubectl-4166 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 12 22:19:17.794: INFO: stderr: ""
Dec 12 22:19:17.794: INFO: stdout: "pod/agnhost-master-j7lc6 patched\n"
STEP: checking annotations
Dec 12 22:19:17.798: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:19:17.798: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4166" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":138,"skipped":2261,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:17.814: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-26
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 12 22:19:17.983: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 12 22:19:18.010: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 22:19:18.015: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-0 before test
Dec 12 22:19:18.037: INFO: kube-proxy-bkbtw from kube-system started at 2019-12-12 21:35:43 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.037: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:19:18.037: INFO: kube-apiserver-9sv5v from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.037: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 12 22:19:18.037: INFO: pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.037: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:19:18.037: INFO: kube-flannel-69ftd from kube-system started at 2019-12-12 21:35:43 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.037: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:19:18.037: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:19:18.037: INFO: pod-checkpointer-wbndz from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.037: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:19:18.037: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-1 before test
Dec 12 22:19:18.061: INFO: kube-apiserver-j24rk from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.061: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:19:18.061: INFO: kube-controller-manager-5c547b548-hxgrq from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.061: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 12 22:19:18.061: INFO: pod-checkpointer-8ftvl-talos-0-3-0-beta-0-gcp-controlplane-1 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.061: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:19:18.061: INFO: kube-proxy-mb48g from kube-system started at 2019-12-12 21:35:36 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.061: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:19:18.061: INFO: coredns-5bcc66f5bd-6pzdf from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.061: INFO: 	Container coredns ready: true, restart count 0
Dec 12 22:19:18.061: INFO: kube-scheduler-6db5bfb7bc-tx7bk from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.061: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 12 22:19:18.061: INFO: pod-checkpointer-8ftvl from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.062: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:19:18.062: INFO: kube-scheduler-6db5bfb7bc-dplt8 from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.062: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 12 22:19:18.062: INFO: kube-controller-manager-5c547b548-cqqln from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.062: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 12 22:19:18.062: INFO: kube-flannel-cq4nw from kube-system started at 2019-12-12 21:35:36 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.062: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:19:18.062: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:19:18.062: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-2 before test
Dec 12 22:19:18.086: INFO: kube-proxy-9wbfd from kube-system started at 2019-12-12 21:35:29 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.086: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:19:18.086: INFO: kube-flannel-9hznp from kube-system started at 2019-12-12 21:35:29 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.086: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:19:18.086: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:19:18.086: INFO: kube-apiserver-kf44j from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.086: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:19:18.086: INFO: pod-checkpointer-5mnr9 from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.086: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:19:18.086: INFO: pod-checkpointer-5mnr9-talos-0-3-0-beta-0-gcp-controlplane-2 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.086: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:19:18.086: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-4kj7s before test
Dec 12 22:19:18.111: INFO: sonobuoy-e2e-job-01c7b380be144838 from sonobuoy started at 2019-12-12 21:39:37 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.111: INFO: 	Container e2e ready: true, restart count 0
Dec 12 22:19:18.111: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 12 22:19:18.111: INFO: kube-flannel-bz9ht from kube-system started at 2019-12-12 21:36:55 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.111: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:19:18.111: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:19:18.111: INFO: kube-proxy-xh54v from kube-system started at 2019-12-12 21:36:55 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.111: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:19:18.111: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-nt7qc before test
Dec 12 22:19:18.117: INFO: kube-flannel-p79j9 from kube-system started at 2019-12-12 21:37:10 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.117: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:19:18.117: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:19:18.117: INFO: kube-proxy-jtlmf from kube-system started at 2019-12-12 21:37:10 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.117: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:19:18.117: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-ntvmn before test
Dec 12 22:19:18.125: INFO: sonobuoy from sonobuoy started at 2019-12-12 21:39:34 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.125: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 12 22:19:18.125: INFO: agnhost-master-j7lc6 from kubectl-4166 started at 2019-12-12 22:19:13 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.125: INFO: 	Container agnhost-master ready: true, restart count 0
Dec 12 22:19:18.125: INFO: kube-proxy-bvldz from kube-system started at 2019-12-12 21:37:28 +0000 UTC (1 container statuses recorded)
Dec 12 22:19:18.125: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:19:18.125: INFO: kube-flannel-gk4cx from kube-system started at 2019-12-12 21:37:28 +0000 UTC (2 container statuses recorded)
Dec 12 22:19:18.125: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:19:18.125: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1184617a-0c63-4a64-9413-d97c955d0026 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-1184617a-0c63-4a64-9413-d97c955d0026 off the node talos-0-3-0-beta-0-gcp-workers-nt7qc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1184617a-0c63-4a64-9413-d97c955d0026
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:26.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-26" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.451 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":139,"skipped":2265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:26.268: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:19:26.430: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:30.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1060" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":140,"skipped":2304,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:30.503: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:19:30.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26" in namespace "projected-7112" to be "success or failure"
Dec 12 22:19:30.691: INFO: Pod "downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26": Phase="Pending", Reason="", readiness=false. Elapsed: 11.673056ms
Dec 12 22:19:32.696: INFO: Pod "downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016795135s
Dec 12 22:19:34.701: INFO: Pod "downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021518395s
STEP: Saw pod success
Dec 12 22:19:34.701: INFO: Pod "downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26" satisfied condition "success or failure"
Dec 12 22:19:34.706: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-4kj7s pod downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26 container client-container: <nil>
STEP: delete the pod
Dec 12 22:19:34.730: INFO: Waiting for pod downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26 to disappear
Dec 12 22:19:34.733: INFO: Pod downwardapi-volume-b5b86e69-f397-48f7-b789-e85918c73f26 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:34.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7112" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2318,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:34.745: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103
Dec 12 22:19:34.914: INFO: Pod name my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103: Found 0 pods out of 1
Dec 12 22:19:39.920: INFO: Pod name my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103: Found 1 pods out of 1
Dec 12 22:19:39.920: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103" are running
Dec 12 22:19:39.923: INFO: Pod "my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103-vcr7f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:19:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:19:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:19:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:19:34 +0000 UTC Reason: Message:}])
Dec 12 22:19:39.923: INFO: Trying to dial the pod
Dec 12 22:19:44.938: INFO: Controller my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103: Got expected result from replica 1 [my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103-vcr7f]: "my-hostname-basic-c7f06fd7-46c7-4f34-97cd-6aa78f751103-vcr7f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:44.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6400" for this suite.

• [SLOW TEST:10.205 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":142,"skipped":2319,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:44.951: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-608
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-608
STEP: Deleting pre-stop pod
Dec 12 22:19:58.182: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:19:58.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-608" for this suite.

• [SLOW TEST:13.273 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":143,"skipped":2321,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:19:58.225: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:19:58.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca" in namespace "projected-228" to be "success or failure"
Dec 12 22:19:58.407: INFO: Pod "downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365081ms
Dec 12 22:20:00.412: INFO: Pod "downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008522478s
Dec 12 22:20:02.416: INFO: Pod "downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012642475s
STEP: Saw pod success
Dec 12 22:20:02.416: INFO: Pod "downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca" satisfied condition "success or failure"
Dec 12 22:20:02.420: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca container client-container: <nil>
STEP: delete the pod
Dec 12 22:20:02.445: INFO: Waiting for pod downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca to disappear
Dec 12 22:20:02.448: INFO: Pod downwardapi-volume-3c061d10-36f2-484e-8f29-a501244161ca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:02.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-228" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":144,"skipped":2327,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:02.461: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 12 22:20:10.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 22:20:10.682: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 22:20:12.682: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 22:20:12.687: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 22:20:14.682: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 22:20:14.689: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 22:20:16.682: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 22:20:16.687: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:16.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2481" for this suite.

• [SLOW TEST:14.239 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2342,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:16.701: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-30
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:20:16.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa" in namespace "projected-30" to be "success or failure"
Dec 12 22:20:16.880: INFO: Pod "downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.265411ms
Dec 12 22:20:18.884: INFO: Pod "downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011530359s
Dec 12 22:20:20.889: INFO: Pod "downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015968396s
STEP: Saw pod success
Dec 12 22:20:20.889: INFO: Pod "downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa" satisfied condition "success or failure"
Dec 12 22:20:20.893: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa container client-container: <nil>
STEP: delete the pod
Dec 12 22:20:20.917: INFO: Waiting for pod downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa to disappear
Dec 12 22:20:20.921: INFO: Pod downwardapi-volume-b3322aeb-85cd-436b-8a6c-7262f0c6eafa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:20.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-30" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":146,"skipped":2355,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:20.935: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-42fada1c-fccd-4b18-81a7-806eb69ec48c
STEP: Creating a pod to test consume secrets
Dec 12 22:20:21.127: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09" in namespace "projected-2296" to be "success or failure"
Dec 12 22:20:21.135: INFO: Pod "pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.318448ms
Dec 12 22:20:23.139: INFO: Pod "pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012201252s
Dec 12 22:20:25.145: INFO: Pod "pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017492233s
STEP: Saw pod success
Dec 12 22:20:25.145: INFO: Pod "pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09" satisfied condition "success or failure"
Dec 12 22:20:25.148: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:20:25.189: INFO: Waiting for pod pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09 to disappear
Dec 12 22:20:25.194: INFO: Pod pod-projected-secrets-4855cd47-d396-42f0-9597-d4f792e3ea09 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:25.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2296" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2371,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:25.207: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 12 22:20:25.411: INFO: Waiting up to 5m0s for pod "downward-api-06f44df4-a154-4b13-bb84-0da037da26b3" in namespace "downward-api-341" to be "success or failure"
Dec 12 22:20:25.417: INFO: Pod "downward-api-06f44df4-a154-4b13-bb84-0da037da26b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110607ms
Dec 12 22:20:27.421: INFO: Pod "downward-api-06f44df4-a154-4b13-bb84-0da037da26b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010429708s
Dec 12 22:20:29.425: INFO: Pod "downward-api-06f44df4-a154-4b13-bb84-0da037da26b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014318861s
STEP: Saw pod success
Dec 12 22:20:29.425: INFO: Pod "downward-api-06f44df4-a154-4b13-bb84-0da037da26b3" satisfied condition "success or failure"
Dec 12 22:20:29.428: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downward-api-06f44df4-a154-4b13-bb84-0da037da26b3 container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:20:29.451: INFO: Waiting for pod downward-api-06f44df4-a154-4b13-bb84-0da037da26b3 to disappear
Dec 12 22:20:29.456: INFO: Pod downward-api-06f44df4-a154-4b13-bb84-0da037da26b3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:29.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-341" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:29.469: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-14d4fe01-0998-46ac-9fc1-75c082f3c350
STEP: Creating a pod to test consume configMaps
Dec 12 22:20:29.641: INFO: Waiting up to 5m0s for pod "pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee" in namespace "configmap-1076" to be "success or failure"
Dec 12 22:20:29.646: INFO: Pod "pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16648ms
Dec 12 22:20:31.650: INFO: Pod "pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008354274s
Dec 12 22:20:33.654: INFO: Pod "pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01297012s
STEP: Saw pod success
Dec 12 22:20:33.654: INFO: Pod "pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee" satisfied condition "success or failure"
Dec 12 22:20:33.659: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:20:33.693: INFO: Waiting for pod pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee to disappear
Dec 12 22:20:33.697: INFO: Pod pod-configmaps-355f5a6c-3614-4fcb-8ac1-37a958b621ee no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:33.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1076" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2424,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:33.715: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 12 22:20:38.446: INFO: Successfully updated pod "pod-update-6d4ac46e-6d23-4b58-b0ad-e5c76cfa2004"
STEP: verifying the updated pod is in kubernetes
Dec 12 22:20:38.454: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:38.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4059" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2430,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Dec 12 22:20:38.620: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-860659511 proxy --unix-socket=/tmp/kubectl-proxy-unix279864242/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:38.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7471" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":151,"skipped":2442,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:38.694: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4030.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4030.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4030.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4030.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4030.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4030.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 22:20:42.975: INFO: DNS probes using dns-4030/dns-test-7043aa42-8db6-4445-88b8-8aa0f5bab2d7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:43.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4030" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":152,"skipped":2456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:43.028: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:20:43.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837" in namespace "projected-3990" to be "success or failure"
Dec 12 22:20:43.215: INFO: Pod "downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.492473ms
Dec 12 22:20:45.220: INFO: Pod "downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009065044s
Dec 12 22:20:47.224: INFO: Pod "downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013899385s
STEP: Saw pod success
Dec 12 22:20:47.224: INFO: Pod "downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837" satisfied condition "success or failure"
Dec 12 22:20:47.228: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837 container client-container: <nil>
STEP: delete the pod
Dec 12 22:20:47.253: INFO: Waiting for pod downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837 to disappear
Dec 12 22:20:47.257: INFO: Pod downwardapi-volume-57c52faf-3919-43cd-84ca-bdfca4f91837 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:20:47.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3990" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:20:47.271: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 12 22:20:47.753: INFO: Pod name wrapped-volume-race-30ee9be7-ba83-4f5b-b7de-170c41089aaa: Found 0 pods out of 5
Dec 12 22:20:52.759: INFO: Pod name wrapped-volume-race-30ee9be7-ba83-4f5b-b7de-170c41089aaa: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-30ee9be7-ba83-4f5b-b7de-170c41089aaa in namespace emptydir-wrapper-5451, will wait for the garbage collector to delete the pods
Dec 12 22:21:04.846: INFO: Deleting ReplicationController wrapped-volume-race-30ee9be7-ba83-4f5b-b7de-170c41089aaa took: 11.472295ms
Dec 12 22:21:05.146: INFO: Terminating ReplicationController wrapped-volume-race-30ee9be7-ba83-4f5b-b7de-170c41089aaa pods took: 300.340926ms
STEP: Creating RC which spawns configmap-volume pods
Dec 12 22:21:10.269: INFO: Pod name wrapped-volume-race-5e084ff1-bd7f-45e4-8b99-6925e1dfa069: Found 0 pods out of 5
Dec 12 22:21:15.277: INFO: Pod name wrapped-volume-race-5e084ff1-bd7f-45e4-8b99-6925e1dfa069: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5e084ff1-bd7f-45e4-8b99-6925e1dfa069 in namespace emptydir-wrapper-5451, will wait for the garbage collector to delete the pods
Dec 12 22:21:27.398: INFO: Deleting ReplicationController wrapped-volume-race-5e084ff1-bd7f-45e4-8b99-6925e1dfa069 took: 12.499428ms
Dec 12 22:21:27.798: INFO: Terminating ReplicationController wrapped-volume-race-5e084ff1-bd7f-45e4-8b99-6925e1dfa069 pods took: 400.283335ms
STEP: Creating RC which spawns configmap-volume pods
Dec 12 22:21:36.523: INFO: Pod name wrapped-volume-race-1037c290-c1d7-47d8-b680-51782c5102fb: Found 0 pods out of 5
Dec 12 22:21:41.531: INFO: Pod name wrapped-volume-race-1037c290-c1d7-47d8-b680-51782c5102fb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1037c290-c1d7-47d8-b680-51782c5102fb in namespace emptydir-wrapper-5451, will wait for the garbage collector to delete the pods
Dec 12 22:21:53.634: INFO: Deleting ReplicationController wrapped-volume-race-1037c290-c1d7-47d8-b680-51782c5102fb took: 14.970666ms
Dec 12 22:21:53.934: INFO: Terminating ReplicationController wrapped-volume-race-1037c290-c1d7-47d8-b680-51782c5102fb pods took: 300.283696ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:21:59.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5451" for this suite.

• [SLOW TEST:72.340 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":154,"skipped":2523,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:21:59.612: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 12 22:22:04.328: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ff7bffd4-e12d-4d15-b92e-a9aeac53bcda"
Dec 12 22:22:04.329: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ff7bffd4-e12d-4d15-b92e-a9aeac53bcda" in namespace "pods-8099" to be "terminated due to deadline exceeded"
Dec 12 22:22:04.333: INFO: Pod "pod-update-activedeadlineseconds-ff7bffd4-e12d-4d15-b92e-a9aeac53bcda": Phase="Running", Reason="", readiness=true. Elapsed: 4.579422ms
Dec 12 22:22:06.340: INFO: Pod "pod-update-activedeadlineseconds-ff7bffd4-e12d-4d15-b92e-a9aeac53bcda": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010983971s
Dec 12 22:22:06.340: INFO: Pod "pod-update-activedeadlineseconds-ff7bffd4-e12d-4d15-b92e-a9aeac53bcda" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:22:06.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8099" for this suite.

• [SLOW TEST:6.748 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":155,"skipped":2531,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:22:06.360: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6851
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:22:06.556: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:22:11.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6851" for this suite.

• [SLOW TEST:5.626 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":156,"skipped":2543,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:22:11.987: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-q4lc
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 22:22:12.309: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q4lc" in namespace "subpath-6344" to be "success or failure"
Dec 12 22:22:12.322: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.88119ms
Dec 12 22:22:14.328: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018818468s
Dec 12 22:22:16.333: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 4.024528672s
Dec 12 22:22:18.339: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 6.030215318s
Dec 12 22:22:20.345: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 8.035785836s
Dec 12 22:22:22.350: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 10.040989162s
Dec 12 22:22:24.356: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 12.046751683s
Dec 12 22:22:26.360: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 14.05160856s
Dec 12 22:22:28.366: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 16.057369712s
Dec 12 22:22:30.372: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 18.063150435s
Dec 12 22:22:32.378: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 20.069717575s
Dec 12 22:22:34.384: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Running", Reason="", readiness=true. Elapsed: 22.074983764s
Dec 12 22:22:36.389: INFO: Pod "pod-subpath-test-projected-q4lc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.080530076s
STEP: Saw pod success
Dec 12 22:22:36.389: INFO: Pod "pod-subpath-test-projected-q4lc" satisfied condition "success or failure"
Dec 12 22:22:36.395: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-subpath-test-projected-q4lc container test-container-subpath-projected-q4lc: <nil>
STEP: delete the pod
Dec 12 22:22:36.449: INFO: Waiting for pod pod-subpath-test-projected-q4lc to disappear
Dec 12 22:22:36.456: INFO: Pod pod-subpath-test-projected-q4lc no longer exists
STEP: Deleting pod pod-subpath-test-projected-q4lc
Dec 12 22:22:36.456: INFO: Deleting pod "pod-subpath-test-projected-q4lc" in namespace "subpath-6344"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:22:36.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6344" for this suite.

• [SLOW TEST:24.494 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":157,"skipped":2553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:22:36.482: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Dec 12 22:22:36.670: INFO: Waiting up to 5m0s for pod "client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061" in namespace "containers-2115" to be "success or failure"
Dec 12 22:22:36.676: INFO: Pod "client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606354ms
Dec 12 22:22:38.681: INFO: Pod "client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011585366s
Dec 12 22:22:40.686: INFO: Pod "client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016445183s
STEP: Saw pod success
Dec 12 22:22:40.686: INFO: Pod "client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061" satisfied condition "success or failure"
Dec 12 22:22:40.690: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061 container test-container: <nil>
STEP: delete the pod
Dec 12 22:22:40.735: INFO: Waiting for pod client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061 to disappear
Dec 12 22:22:40.740: INFO: Pod client-containers-f852b2f8-9a08-4e00-98ff-710d1dc39061 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:22:40.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2115" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2588,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:22:40.755: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:22:40.968: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 12 22:22:40.987: INFO: Number of nodes with available pods: 0
Dec 12 22:22:40.987: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:22:42.006: INFO: Number of nodes with available pods: 0
Dec 12 22:22:42.006: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:22:43.001: INFO: Number of nodes with available pods: 0
Dec 12 22:22:43.001: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:22:44.026: INFO: Number of nodes with available pods: 0
Dec 12 22:22:44.026: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:22:45.001: INFO: Number of nodes with available pods: 5
Dec 12 22:22:45.001: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:22:45.999: INFO: Number of nodes with available pods: 6
Dec 12 22:22:45.999: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 12 22:22:46.048: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:46.048: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:46.048: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:46.048: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:46.048: INFO: Wrong image for pod: daemon-set-sg52r. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:46.048: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:47.058: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:47.058: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:47.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:47.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:47.058: INFO: Wrong image for pod: daemon-set-sg52r. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:47.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:48.059: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:48.059: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:48.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:48.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:48.059: INFO: Wrong image for pod: daemon-set-sg52r. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:48.059: INFO: Pod daemon-set-sg52r is not available
Dec 12 22:22:48.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:49.058: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:49.058: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:49.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:49.058: INFO: Pod daemon-set-lm6r4 is not available
Dec 12 22:22:49.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:49.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:50.057: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:50.057: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:50.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:50.057: INFO: Pod daemon-set-lm6r4 is not available
Dec 12 22:22:50.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:50.057: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:51.058: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:51.058: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:51.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:51.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:51.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:52.078: INFO: Wrong image for pod: daemon-set-9gnr6. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:52.078: INFO: Pod daemon-set-9gnr6 is not available
Dec 12 22:22:52.078: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:52.078: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:52.078: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:52.078: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:53.060: INFO: Pod daemon-set-49d68 is not available
Dec 12 22:22:53.060: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:53.060: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:53.060: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:53.060: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:54.085: INFO: Pod daemon-set-49d68 is not available
Dec 12 22:22:54.085: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:54.085: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:54.085: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:54.085: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:55.058: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:55.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:55.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:55.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:56.059: INFO: Wrong image for pod: daemon-set-jdt46. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:56.059: INFO: Pod daemon-set-jdt46 is not available
Dec 12 22:22:56.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:56.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:56.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:57.059: INFO: Pod daemon-set-7c82c is not available
Dec 12 22:22:57.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:57.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:57.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:58.060: INFO: Pod daemon-set-7c82c is not available
Dec 12 22:22:58.060: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:58.060: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:58.060: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:59.058: INFO: Pod daemon-set-7c82c is not available
Dec 12 22:22:59.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:59.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:22:59.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:00.059: INFO: Pod daemon-set-7c82c is not available
Dec 12 22:23:00.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:00.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:00.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:01.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:01.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:01.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:02.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:02.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:02.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:02.059: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:03.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:03.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:03.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:03.059: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:04.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:04.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:04.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:04.058: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:05.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:05.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:05.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:05.058: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:06.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:06.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:06.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:06.059: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:07.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:07.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:07.058: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:07.058: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:08.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:08.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:08.057: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:08.057: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:09.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:09.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:09.057: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:09.057: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:10.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:10.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:10.057: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:10.057: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:11.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:11.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:11.057: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:11.057: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:12.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:12.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:12.059: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:12.059: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:13.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:13.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:13.057: INFO: Wrong image for pod: daemon-set-tcsd8. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:13.057: INFO: Pod daemon-set-tcsd8 is not available
Dec 12 22:23:14.058: INFO: Pod daemon-set-cpsgl is not available
Dec 12 22:23:14.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:14.059: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:15.057: INFO: Pod daemon-set-cpsgl is not available
Dec 12 22:23:15.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:15.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:16.057: INFO: Pod daemon-set-cpsgl is not available
Dec 12 22:23:16.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:16.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:17.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:17.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:18.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:18.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:18.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:19.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:19.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:19.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:20.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:20.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:20.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:21.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:21.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:21.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:22.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:22.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:22.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:23.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:23.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:23.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:24.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:24.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:24.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:25.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:25.057: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:25.057: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:26.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:26.058: INFO: Wrong image for pod: daemon-set-qm2q9. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:26.058: INFO: Pod daemon-set-qm2q9 is not available
Dec 12 22:23:27.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:27.059: INFO: Pod daemon-set-vq7zv is not available
Dec 12 22:23:28.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:28.058: INFO: Pod daemon-set-vq7zv is not available
Dec 12 22:23:29.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:29.057: INFO: Pod daemon-set-vq7zv is not available
Dec 12 22:23:30.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:30.058: INFO: Pod daemon-set-kvzwv is not available
Dec 12 22:23:31.059: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:31.059: INFO: Pod daemon-set-kvzwv is not available
Dec 12 22:23:32.057: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:32.057: INFO: Pod daemon-set-kvzwv is not available
Dec 12 22:23:33.058: INFO: Wrong image for pod: daemon-set-kvzwv. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 22:23:33.058: INFO: Pod daemon-set-kvzwv is not available
Dec 12 22:23:34.057: INFO: Pod daemon-set-k5c9l is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 12 22:23:34.072: INFO: Number of nodes with available pods: 5
Dec 12 22:23:34.073: INFO: Node talos-0-3-0-beta-0-gcp-workers-4kj7s is running more than one daemon pod
Dec 12 22:23:35.083: INFO: Number of nodes with available pods: 5
Dec 12 22:23:35.084: INFO: Node talos-0-3-0-beta-0-gcp-workers-4kj7s is running more than one daemon pod
Dec 12 22:23:36.082: INFO: Number of nodes with available pods: 6
Dec 12 22:23:36.082: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9764, will wait for the garbage collector to delete the pods
Dec 12 22:23:36.167: INFO: Deleting DaemonSet.extensions daemon-set took: 12.135747ms
Dec 12 22:23:36.567: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.274348ms
Dec 12 22:23:44.373: INFO: Number of nodes with available pods: 0
Dec 12 22:23:44.373: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 22:23:44.379: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9764/daemonsets","resourceVersion":"18597"},"items":null}

Dec 12 22:23:44.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9764/pods","resourceVersion":"18597"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:23:44.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9764" for this suite.

• [SLOW TEST:63.672 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":159,"skipped":2615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:23:44.430: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-1362
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1362 to expose endpoints map[]
Dec 12 22:23:44.628: INFO: Get endpoints failed (5.556637ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 12 22:23:45.632: INFO: successfully validated that service multi-endpoint-test in namespace services-1362 exposes endpoints map[] (1.009662717s elapsed)
STEP: Creating pod pod1 in namespace services-1362
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1362 to expose endpoints map[pod1:[100]]
Dec 12 22:23:48.679: INFO: successfully validated that service multi-endpoint-test in namespace services-1362 exposes endpoints map[pod1:[100]] (3.034267104s elapsed)
STEP: Creating pod pod2 in namespace services-1362
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1362 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 12 22:23:51.736: INFO: successfully validated that service multi-endpoint-test in namespace services-1362 exposes endpoints map[pod1:[100] pod2:[101]] (3.049237131s elapsed)
STEP: Deleting pod pod1 in namespace services-1362
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1362 to expose endpoints map[pod2:[101]]
Dec 12 22:23:52.767: INFO: successfully validated that service multi-endpoint-test in namespace services-1362 exposes endpoints map[pod2:[101]] (1.016245318s elapsed)
STEP: Deleting pod pod2 in namespace services-1362
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1362 to expose endpoints map[]
Dec 12 22:23:53.799: INFO: successfully validated that service multi-endpoint-test in namespace services-1362 exposes endpoints map[] (1.020510872s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:23:53.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1362" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.414 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":160,"skipped":2643,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:23:53.853: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Dec 12 22:23:54.051: INFO: Waiting up to 5m0s for pod "downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9" in namespace "downward-api-3671" to be "success or failure"
Dec 12 22:23:54.058: INFO: Pod "downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.18171ms
Dec 12 22:23:56.064: INFO: Pod "downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012478438s
Dec 12 22:23:58.068: INFO: Pod "downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017260961s
STEP: Saw pod success
Dec 12 22:23:58.068: INFO: Pod "downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9" satisfied condition "success or failure"
Dec 12 22:23:58.072: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9 container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:23:58.096: INFO: Waiting for pod downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9 to disappear
Dec 12 22:23:58.100: INFO: Pod downward-api-9682cf55-b2ea-45ad-ab4c-94532d0b1fa9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:23:58.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3671" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2653,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:23:58.113: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 12 22:23:58.308: INFO: Waiting up to 5m0s for pod "pod-64a82549-f411-40a1-9230-aebe4e6bc71f" in namespace "emptydir-1892" to be "success or failure"
Dec 12 22:23:58.313: INFO: Pod "pod-64a82549-f411-40a1-9230-aebe4e6bc71f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.007853ms
Dec 12 22:24:00.318: INFO: Pod "pod-64a82549-f411-40a1-9230-aebe4e6bc71f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009896204s
Dec 12 22:24:02.322: INFO: Pod "pod-64a82549-f411-40a1-9230-aebe4e6bc71f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014503196s
STEP: Saw pod success
Dec 12 22:24:02.322: INFO: Pod "pod-64a82549-f411-40a1-9230-aebe4e6bc71f" satisfied condition "success or failure"
Dec 12 22:24:02.326: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-64a82549-f411-40a1-9230-aebe4e6bc71f container test-container: <nil>
STEP: delete the pod
Dec 12 22:24:02.377: INFO: Waiting for pod pod-64a82549-f411-40a1-9230-aebe4e6bc71f to disappear
Dec 12 22:24:02.381: INFO: Pod pod-64a82549-f411-40a1-9230-aebe4e6bc71f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:24:02.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1892" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2669,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:24:02.399: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-9220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Dec 12 22:24:02.565: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 12 22:25:02.588: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:25:02.595: INFO: Starting informer...
STEP: Starting pods...
Dec 12 22:25:02.826: INFO: Pod1 is running on talos-0-3-0-beta-0-gcp-workers-nt7qc. Tainting Node
Dec 12 22:25:07.061: INFO: Pod2 is running on talos-0-3-0-beta-0-gcp-workers-nt7qc. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 12 22:25:16.384: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 12 22:25:36.445: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:25:36.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9220" for this suite.

• [SLOW TEST:94.078 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":163,"skipped":2685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:25:36.482: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-c257d612-d905-4ca1-bab7-f6cfaa1d8b24 in namespace container-probe-7640
Dec 12 22:25:40.665: INFO: Started pod test-webserver-c257d612-d905-4ca1-bab7-f6cfaa1d8b24 in namespace container-probe-7640
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 22:25:40.669: INFO: Initial restart count of pod test-webserver-c257d612-d905-4ca1-bab7-f6cfaa1d8b24 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:29:41.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7640" for this suite.

• [SLOW TEST:244.820 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2741,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:29:41.303: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1362
STEP: creating the pod
Dec 12 22:29:41.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-6949'
Dec 12 22:29:42.350: INFO: stderr: ""
Dec 12 22:29:42.350: INFO: stdout: "pod/pause created\n"
Dec 12 22:29:42.350: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 12 22:29:42.350: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6949" to be "running and ready"
Dec 12 22:29:42.364: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.349691ms
Dec 12 22:29:44.368: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017947439s
Dec 12 22:29:46.373: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.022625761s
Dec 12 22:29:46.373: INFO: Pod "pause" satisfied condition "running and ready"
Dec 12 22:29:46.373: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 12 22:29:46.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 label pods pause testing-label=testing-label-value --namespace=kubectl-6949'
Dec 12 22:29:46.478: INFO: stderr: ""
Dec 12 22:29:46.478: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 12 22:29:46.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pod pause -L testing-label --namespace=kubectl-6949'
Dec 12 22:29:46.600: INFO: stderr: ""
Dec 12 22:29:46.600: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 12 22:29:46.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 label pods pause testing-label- --namespace=kubectl-6949'
Dec 12 22:29:46.705: INFO: stderr: ""
Dec 12 22:29:46.705: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 12 22:29:46.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pod pause -L testing-label --namespace=kubectl-6949'
Dec 12 22:29:46.790: INFO: stderr: ""
Dec 12 22:29:46.790: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1369
STEP: using delete to clean up resources
Dec 12 22:29:46.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-6949'
Dec 12 22:29:46.922: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:29:46.922: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 12 22:29:46.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get rc,svc -l name=pause --no-headers --namespace=kubectl-6949'
Dec 12 22:29:47.012: INFO: stderr: "No resources found in kubectl-6949 namespace.\n"
Dec 12 22:29:47.012: INFO: stdout: ""
Dec 12 22:29:47.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -l name=pause --namespace=kubectl-6949 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 22:29:47.095: INFO: stderr: ""
Dec 12 22:29:47.095: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:29:47.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6949" for this suite.

• [SLOW TEST:5.809 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":165,"skipped":2753,"failed":0}
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:29:47.113: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6894
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-2712628b-5af6-4125-9573-ae6c58172b25 in namespace container-probe-6894
Dec 12 22:29:51.333: INFO: Started pod busybox-2712628b-5af6-4125-9573-ae6c58172b25 in namespace container-probe-6894
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 22:29:51.338: INFO: Initial restart count of pod busybox-2712628b-5af6-4125-9573-ae6c58172b25 is 0
Dec 12 22:30:35.466: INFO: Restart count of pod container-probe-6894/busybox-2712628b-5af6-4125-9573-ae6c58172b25 is now 1 (44.127818117s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:30:35.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6894" for this suite.

• [SLOW TEST:48.380 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:30:35.497: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 12 22:30:35.707: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 12 22:30:40.711: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:30:41.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8868" for this suite.

• [SLOW TEST:6.258 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":167,"skipped":2782,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:30:41.757: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4301
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 12 22:30:46.586: INFO: Successfully updated pod "labelsupdated322b3b6-c4ce-4d9d-8177-4a36149af6d6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:30:48.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4301" for this suite.

• [SLOW TEST:6.869 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":168,"skipped":2784,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:30:48.715: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3702
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 22:30:48.900: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 12 22:31:17.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.159:8080/dial?request=hostname&protocol=http&host=10.244.2.15&port=8080&tries=1'] Namespace:pod-network-test-3702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:17.144: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:17.218: INFO: Waiting for responses: map[]
Dec 12 22:31:17.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.159:8080/dial?request=hostname&protocol=http&host=10.244.1.21&port=8080&tries=1'] Namespace:pod-network-test-3702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:17.222: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:17.295: INFO: Waiting for responses: map[]
Dec 12 22:31:17.302: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.159:8080/dial?request=hostname&protocol=http&host=10.244.0.13&port=8080&tries=1'] Namespace:pod-network-test-3702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:17.302: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:17.391: INFO: Waiting for responses: map[]
Dec 12 22:31:17.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.159:8080/dial?request=hostname&protocol=http&host=10.244.3.27&port=8080&tries=1'] Namespace:pod-network-test-3702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:17.396: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:17.474: INFO: Waiting for responses: map[]
Dec 12 22:31:17.480: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.159:8080/dial?request=hostname&protocol=http&host=10.244.4.158&port=8080&tries=1'] Namespace:pod-network-test-3702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:17.480: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:17.558: INFO: Waiting for responses: map[]
Dec 12 22:31:17.563: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.159:8080/dial?request=hostname&protocol=http&host=10.244.5.65&port=8080&tries=1'] Namespace:pod-network-test-3702 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:17.632: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:17.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3702" for this suite.

• [SLOW TEST:28.932 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2788,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:17.648: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-58d910aa-85bf-42cf-a212-144b70c74d9d
STEP: Creating a pod to test consume secrets
Dec 12 22:31:17.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad" in namespace "projected-837" to be "success or failure"
Dec 12 22:31:17.989: INFO: Pod "pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.290395ms
Dec 12 22:31:19.994: INFO: Pod "pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008330027s
Dec 12 22:31:21.998: INFO: Pod "pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013107159s
STEP: Saw pod success
Dec 12 22:31:21.998: INFO: Pod "pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad" satisfied condition "success or failure"
Dec 12 22:31:22.003: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:31:22.026: INFO: Waiting for pod pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad to disappear
Dec 12 22:31:22.031: INFO: Pod pod-projected-secrets-ee820298-8c25-469c-8280-2abf8430f9ad no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:22.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-837" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":2816,"failed":0}
SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:22.048: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 12 22:31:28.276: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.276: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.390: INFO: Exec stderr: ""
Dec 12 22:31:28.390: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.390: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.480: INFO: Exec stderr: ""
Dec 12 22:31:28.480: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.559: INFO: Exec stderr: ""
Dec 12 22:31:28.559: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.559: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.640: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 12 22:31:28.640: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.640: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.722: INFO: Exec stderr: ""
Dec 12 22:31:28.722: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.722: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.799: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 12 22:31:28.799: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.799: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.863: INFO: Exec stderr: ""
Dec 12 22:31:28.863: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.863: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.926: INFO: Exec stderr: ""
Dec 12 22:31:28.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.926: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:28.993: INFO: Exec stderr: ""
Dec 12 22:31:28.993: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6137 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:31:28.993: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:31:29.065: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:29.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6137" for this suite.

• [SLOW TEST:7.033 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":171,"skipped":2820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:29.083: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Dec 12 22:31:29.295: INFO: Waiting up to 5m0s for pod "var-expansion-56da957b-958f-4487-8fb9-d22af2096946" in namespace "var-expansion-5248" to be "success or failure"
Dec 12 22:31:29.300: INFO: Pod "var-expansion-56da957b-958f-4487-8fb9-d22af2096946": Phase="Pending", Reason="", readiness=false. Elapsed: 4.630085ms
Dec 12 22:31:31.307: INFO: Pod "var-expansion-56da957b-958f-4487-8fb9-d22af2096946": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011170393s
Dec 12 22:31:33.311: INFO: Pod "var-expansion-56da957b-958f-4487-8fb9-d22af2096946": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015725298s
Dec 12 22:31:35.316: INFO: Pod "var-expansion-56da957b-958f-4487-8fb9-d22af2096946": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020888556s
STEP: Saw pod success
Dec 12 22:31:35.316: INFO: Pod "var-expansion-56da957b-958f-4487-8fb9-d22af2096946" satisfied condition "success or failure"
Dec 12 22:31:35.320: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-4kj7s pod var-expansion-56da957b-958f-4487-8fb9-d22af2096946 container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:31:35.356: INFO: Waiting for pod var-expansion-56da957b-958f-4487-8fb9-d22af2096946 to disappear
Dec 12 22:31:35.364: INFO: Pod var-expansion-56da957b-958f-4487-8fb9-d22af2096946 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:35.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5248" for this suite.

• [SLOW TEST:6.302 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":2845,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-585
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:31:35.576: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:36.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-585" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":173,"skipped":2849,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:36.168: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1713
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 22:31:36.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-461'
Dec 12 22:31:36.521: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 22:31:36.521: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1718
Dec 12 22:31:40.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete deployment e2e-test-httpd-deployment --namespace=kubectl-461'
Dec 12 22:31:40.641: INFO: stderr: ""
Dec 12 22:31:40.641: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:40.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-461" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":174,"skipped":2851,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:40.663: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Dec 12 22:31:40.895: INFO: Waiting up to 5m0s for pod "client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd" in namespace "containers-7838" to be "success or failure"
Dec 12 22:31:40.907: INFO: Pod "client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.491767ms
Dec 12 22:31:42.915: INFO: Pod "client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019936528s
Dec 12 22:31:44.922: INFO: Pod "client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027191823s
STEP: Saw pod success
Dec 12 22:31:44.922: INFO: Pod "client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd" satisfied condition "success or failure"
Dec 12 22:31:44.926: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-controlplane-0 pod client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd container test-container: <nil>
STEP: delete the pod
Dec 12 22:31:44.980: INFO: Waiting for pod client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd to disappear
Dec 12 22:31:44.986: INFO: Pod client-containers-2f8bd1d9-6441-41a7-8185-1d59848be0cd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7838" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:45.002: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-991/configmap-test-95073c9b-87b4-41d0-b5d0-35d8f882aaca
STEP: Creating a pod to test consume configMaps
Dec 12 22:31:45.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67" in namespace "configmap-991" to be "success or failure"
Dec 12 22:31:45.265: INFO: Pod "pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.703571ms
Dec 12 22:31:47.270: INFO: Pod "pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010080252s
Dec 12 22:31:49.275: INFO: Pod "pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015289002s
STEP: Saw pod success
Dec 12 22:31:49.275: INFO: Pod "pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67" satisfied condition "success or failure"
Dec 12 22:31:49.279: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-4kj7s pod pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67 container env-test: <nil>
STEP: delete the pod
Dec 12 22:31:49.304: INFO: Waiting for pod pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67 to disappear
Dec 12 22:31:49.309: INFO: Pod pod-configmaps-ebb0e686-2f93-4009-bf0b-29348daa9a67 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:31:49.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-991" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:31:49.321: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 in namespace container-probe-2748
Dec 12 22:31:53.527: INFO: Started pod liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 in namespace container-probe-2748
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 22:31:53.531: INFO: Initial restart count of pod liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 is 0
Dec 12 22:32:09.576: INFO: Restart count of pod container-probe-2748/liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 is now 1 (16.045026029s elapsed)
Dec 12 22:32:29.628: INFO: Restart count of pod container-probe-2748/liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 is now 2 (36.097638358s elapsed)
Dec 12 22:32:49.700: INFO: Restart count of pod container-probe-2748/liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 is now 3 (56.168990563s elapsed)
Dec 12 22:33:09.749: INFO: Restart count of pod container-probe-2748/liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 is now 4 (1m16.217864006s elapsed)
Dec 12 22:34:15.916: INFO: Restart count of pod container-probe-2748/liveness-cf5bdd5c-425f-4be6-a491-9c4111f1d662 is now 5 (2m22.384807443s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:34:15.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2748" for this suite.

• [SLOW TEST:146.627 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":2929,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:34:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:34:16.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1657" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":178,"skipped":2936,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:34:16.176: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:34:16.332: INFO: Creating ReplicaSet my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928
Dec 12 22:34:16.342: INFO: Pod name my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928: Found 0 pods out of 1
Dec 12 22:34:21.347: INFO: Pod name my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928: Found 1 pods out of 1
Dec 12 22:34:21.347: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928" is running
Dec 12 22:34:21.351: INFO: Pod "my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928-575bv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:34:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:34:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:34:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 22:34:16 +0000 UTC Reason: Message:}])
Dec 12 22:34:21.351: INFO: Trying to dial the pod
Dec 12 22:34:26.365: INFO: Controller my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928: Got expected result from replica 1 [my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928-575bv]: "my-hostname-basic-d4736bf7-bfab-44f1-8e60-08c0393b4928-575bv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:34:26.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9819" for this suite.

• [SLOW TEST:10.230 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":179,"skipped":2938,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:34:26.409: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-mh9v
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 22:34:26.590: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mh9v" in namespace "subpath-8866" to be "success or failure"
Dec 12 22:34:26.598: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07357ms
Dec 12 22:34:28.602: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012029617s
Dec 12 22:34:30.607: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 4.017460068s
Dec 12 22:34:32.612: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 6.022253302s
Dec 12 22:34:34.616: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 8.026763052s
Dec 12 22:34:36.621: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 10.031709456s
Dec 12 22:34:38.626: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.036492371s
Dec 12 22:34:40.631: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.041028229s
Dec 12 22:34:42.642: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.05219161s
Dec 12 22:34:44.646: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.056437909s
Dec 12 22:34:46.651: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.061506984s
Dec 12 22:34:48.655: INFO: Pod "pod-subpath-test-downwardapi-mh9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.065125081s
STEP: Saw pod success
Dec 12 22:34:48.655: INFO: Pod "pod-subpath-test-downwardapi-mh9v" satisfied condition "success or failure"
Dec 12 22:34:48.658: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-subpath-test-downwardapi-mh9v container test-container-subpath-downwardapi-mh9v: <nil>
STEP: delete the pod
Dec 12 22:34:48.694: INFO: Waiting for pod pod-subpath-test-downwardapi-mh9v to disappear
Dec 12 22:34:48.697: INFO: Pod pod-subpath-test-downwardapi-mh9v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mh9v
Dec 12 22:34:48.697: INFO: Deleting pod "pod-subpath-test-downwardapi-mh9v" in namespace "subpath-8866"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:34:48.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8866" for this suite.

• [SLOW TEST:22.304 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":180,"skipped":2947,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:34:48.716: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:35:05.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5273" for this suite.

• [SLOW TEST:16.327 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":181,"skipped":2956,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:35:05.043: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-58
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 12 22:35:05.222: INFO: Waiting up to 5m0s for pod "pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca" in namespace "emptydir-58" to be "success or failure"
Dec 12 22:35:05.227: INFO: Pod "pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.8977ms
Dec 12 22:35:07.231: INFO: Pod "pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009774317s
Dec 12 22:35:09.236: INFO: Pod "pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014446016s
STEP: Saw pod success
Dec 12 22:35:09.236: INFO: Pod "pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca" satisfied condition "success or failure"
Dec 12 22:35:09.240: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca container test-container: <nil>
STEP: delete the pod
Dec 12 22:35:09.269: INFO: Waiting for pod pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca to disappear
Dec 12 22:35:09.272: INFO: Pod pod-5ba87493-cdd5-408b-895b-3b2cac0dbfca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:35:09.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-58" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":2958,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:35:09.286: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6336
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 22:35:09.470: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 12 22:35:33.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.17:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:35:33.648: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:35:33.730: INFO: Found all expected endpoints: [netserver-0]
Dec 12 22:35:33.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.22:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:35:33.735: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:35:33.809: INFO: Found all expected endpoints: [netserver-1]
Dec 12 22:35:33.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.14:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:35:33.814: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:35:33.898: INFO: Found all expected endpoints: [netserver-2]
Dec 12 22:35:33.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:35:33.903: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:35:33.977: INFO: Found all expected endpoints: [netserver-3]
Dec 12 22:35:33.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.167:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:35:33.981: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:35:34.083: INFO: Found all expected endpoints: [netserver-4]
Dec 12 22:35:34.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.66:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6336 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:35:34.087: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:35:34.171: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:35:34.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6336" for this suite.

• [SLOW TEST:24.899 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2966,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:35:34.187: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Dec 12 22:35:34.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-4474'
Dec 12 22:35:34.719: INFO: stderr: ""
Dec 12 22:35:34.719: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 22:35:34.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4474'
Dec 12 22:35:34.823: INFO: stderr: ""
Dec 12 22:35:34.823: INFO: stdout: "update-demo-nautilus-lm5f6 update-demo-nautilus-p9tp6 "
Dec 12 22:35:34.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-lm5f6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:35:34.908: INFO: stderr: ""
Dec 12 22:35:34.908: INFO: stdout: ""
Dec 12 22:35:34.909: INFO: update-demo-nautilus-lm5f6 is created but not running
Dec 12 22:35:39.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4474'
Dec 12 22:35:40.004: INFO: stderr: ""
Dec 12 22:35:40.004: INFO: stdout: "update-demo-nautilus-lm5f6 update-demo-nautilus-p9tp6 "
Dec 12 22:35:40.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-lm5f6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:35:40.093: INFO: stderr: ""
Dec 12 22:35:40.093: INFO: stdout: "true"
Dec 12 22:35:40.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-lm5f6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:35:40.177: INFO: stderr: ""
Dec 12 22:35:40.177: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:35:40.177: INFO: validating pod update-demo-nautilus-lm5f6
Dec 12 22:35:40.184: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:35:40.184: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:35:40.184: INFO: update-demo-nautilus-lm5f6 is verified up and running
Dec 12 22:35:40.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-p9tp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:35:40.273: INFO: stderr: ""
Dec 12 22:35:40.273: INFO: stdout: "true"
Dec 12 22:35:40.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-p9tp6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:35:40.361: INFO: stderr: ""
Dec 12 22:35:40.361: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:35:40.361: INFO: validating pod update-demo-nautilus-p9tp6
Dec 12 22:35:40.369: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:35:40.369: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:35:40.369: INFO: update-demo-nautilus-p9tp6 is verified up and running
STEP: rolling-update to new replication controller
Dec 12 22:35:40.372: INFO: scanned /root for discovery docs: <nil>
Dec 12 22:35:40.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4474'
Dec 12 22:36:03.896: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 12 22:36:03.896: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 22:36:03.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4474'
Dec 12 22:36:03.989: INFO: stderr: ""
Dec 12 22:36:03.989: INFO: stdout: "update-demo-kitten-6jcjd update-demo-kitten-8l9zp "
Dec 12 22:36:03.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-kitten-6jcjd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:36:04.075: INFO: stderr: ""
Dec 12 22:36:04.075: INFO: stdout: "true"
Dec 12 22:36:04.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-kitten-6jcjd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:36:04.163: INFO: stderr: ""
Dec 12 22:36:04.163: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 12 22:36:04.163: INFO: validating pod update-demo-kitten-6jcjd
Dec 12 22:36:04.169: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 12 22:36:04.169: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 12 22:36:04.169: INFO: update-demo-kitten-6jcjd is verified up and running
Dec 12 22:36:04.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-kitten-8l9zp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:36:04.259: INFO: stderr: ""
Dec 12 22:36:04.260: INFO: stdout: "true"
Dec 12 22:36:04.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-kitten-8l9zp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4474'
Dec 12 22:36:04.361: INFO: stderr: ""
Dec 12 22:36:04.361: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 12 22:36:04.361: INFO: validating pod update-demo-kitten-8l9zp
Dec 12 22:36:04.371: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 12 22:36:04.371: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 12 22:36:04.371: INFO: update-demo-kitten-8l9zp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:04.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4474" for this suite.

• [SLOW TEST:30.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":184,"skipped":2971,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:04.385: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-69ddc837-f108-4714-85ab-0ca4a979edc2
STEP: Creating a pod to test consume configMaps
Dec 12 22:36:04.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7" in namespace "configmap-6254" to be "success or failure"
Dec 12 22:36:04.627: INFO: Pod "pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.321115ms
Dec 12 22:36:06.633: INFO: Pod "pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020790892s
Dec 12 22:36:08.636: INFO: Pod "pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024382187s
STEP: Saw pod success
Dec 12 22:36:08.636: INFO: Pod "pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7" satisfied condition "success or failure"
Dec 12 22:36:08.639: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:36:08.663: INFO: Waiting for pod pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7 to disappear
Dec 12 22:36:08.666: INFO: Pod pod-configmaps-7cda6088-ee55-459e-a8af-74e265ca90b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:08.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6254" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2983,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:08.682: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4286
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:36:08.841: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:09.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4286" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":186,"skipped":2990,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:09.460: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1330
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Dec 12 22:36:15.787: INFO: 0 pods remaining
Dec 12 22:36:15.787: INFO: 0 pods has nil DeletionTimestamp
Dec 12 22:36:15.787: INFO: 
STEP: Gathering metrics
W1212 22:36:16.747421      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 22:36:16.747: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:16.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1330" for this suite.

• [SLOW TEST:7.299 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":187,"skipped":2994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:16.760: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:36:16.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470" in namespace "projected-495" to be "success or failure"
Dec 12 22:36:16.938: INFO: Pod "downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250959ms
Dec 12 22:36:18.942: INFO: Pod "downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008928074s
Dec 12 22:36:20.947: INFO: Pod "downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013676754s
STEP: Saw pod success
Dec 12 22:36:20.947: INFO: Pod "downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470" satisfied condition "success or failure"
Dec 12 22:36:20.951: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470 container client-container: <nil>
STEP: delete the pod
Dec 12 22:36:20.976: INFO: Waiting for pod downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470 to disappear
Dec 12 22:36:20.980: INFO: Pod downwardapi-volume-17682f3a-3b2c-49f9-ae2e-3650096ed470 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:20.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-495" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":188,"skipped":3020,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:20.992: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-77066bb7-4e0d-4ceb-b881-746a97fa66c3
STEP: Creating a pod to test consume configMaps
Dec 12 22:36:21.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60" in namespace "configmap-8406" to be "success or failure"
Dec 12 22:36:21.178: INFO: Pod "pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60": Phase="Pending", Reason="", readiness=false. Elapsed: 5.499768ms
Dec 12 22:36:23.183: INFO: Pod "pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349708s
Dec 12 22:36:25.187: INFO: Pod "pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014754133s
STEP: Saw pod success
Dec 12 22:36:25.187: INFO: Pod "pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60" satisfied condition "success or failure"
Dec 12 22:36:25.191: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:36:25.208: INFO: Waiting for pod pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60 to disappear
Dec 12 22:36:25.211: INFO: Pod pod-configmaps-f849f690-28ef-4eec-a680-3da63a95ec60 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:25.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8406" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":189,"skipped":3028,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:25.223: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 12 22:36:25.438: INFO: Waiting up to 5m0s for pod "pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658" in namespace "emptydir-4994" to be "success or failure"
Dec 12 22:36:25.444: INFO: Pod "pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658": Phase="Pending", Reason="", readiness=false. Elapsed: 5.902503ms
Dec 12 22:36:27.448: INFO: Pod "pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010562157s
Dec 12 22:36:29.453: INFO: Pod "pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015505792s
STEP: Saw pod success
Dec 12 22:36:29.453: INFO: Pod "pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658" satisfied condition "success or failure"
Dec 12 22:36:29.457: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658 container test-container: <nil>
STEP: delete the pod
Dec 12 22:36:29.479: INFO: Waiting for pod pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658 to disappear
Dec 12 22:36:29.483: INFO: Pod pod-6ec10d0d-8a84-49c4-9f7f-effd0ff34658 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:29.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4994" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3042,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-25
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:36:29.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195" in namespace "downward-api-25" to be "success or failure"
Dec 12 22:36:29.704: INFO: Pod "downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195": Phase="Pending", Reason="", readiness=false. Elapsed: 8.142306ms
Dec 12 22:36:31.709: INFO: Pod "downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0135328s
Dec 12 22:36:33.715: INFO: Pod "downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018855221s
STEP: Saw pod success
Dec 12 22:36:33.715: INFO: Pod "downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195" satisfied condition "success or failure"
Dec 12 22:36:33.720: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195 container client-container: <nil>
STEP: delete the pod
Dec 12 22:36:33.746: INFO: Waiting for pod downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195 to disappear
Dec 12 22:36:33.751: INFO: Pod downwardapi-volume-e3047ecb-c58c-409f-87cd-650566b6e195 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:33.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-25" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3060,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:33.763: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:36:45.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5187" for this suite.

• [SLOW TEST:11.261 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":192,"skipped":3062,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:36:45.024: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9314
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9314
STEP: creating replication controller externalsvc in namespace services-9314
I1212 22:36:45.239150      22 runners.go:189] Created replication controller with name: externalsvc, namespace: services-9314, replica count: 2
I1212 22:36:48.289580      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:36:51.289870      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 12 22:36:51.347: INFO: Creating new exec pod
Dec 12 22:36:55.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-9314 execpod4vlq4 -- /bin/sh -x -c nslookup clusterip-service'
Dec 12 22:36:55.568: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 12 22:36:55.568: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9314.svc.cluster.local\tcanonical name = externalsvc.services-9314.svc.cluster.local.\nName:\texternalsvc.services-9314.svc.cluster.local\nAddress: 10.96.226.115\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9314, will wait for the garbage collector to delete the pods
Dec 12 22:36:55.632: INFO: Deleting ReplicationController externalsvc took: 8.501786ms
Dec 12 22:36:55.933: INFO: Terminating ReplicationController externalsvc pods took: 300.314095ms
Dec 12 22:37:06.561: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:06.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9314" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:21.572 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":193,"skipped":3065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:06.601: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:37:06.820: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7e4676d0-09b8-4316-a177-304ff1bc07a8", Controller:(*bool)(0xc005753a4e), BlockOwnerDeletion:(*bool)(0xc005753a4f)}}
Dec 12 22:37:06.851: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"40bb4915-2041-4978-8473-26249d7b4366", Controller:(*bool)(0xc00431b22e), BlockOwnerDeletion:(*bool)(0xc00431b22f)}}
Dec 12 22:37:06.861: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6013eb92-ce59-4ea4-95e1-f6f939cbcef9", Controller:(*bool)(0xc005753c2e), BlockOwnerDeletion:(*bool)(0xc005753c2f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:11.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1205" for this suite.

• [SLOW TEST:5.289 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":194,"skipped":3096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:37:12.078: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:14.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5124" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":195,"skipped":3124,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:14.176: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:37:14.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223" in namespace "downward-api-2899" to be "success or failure"
Dec 12 22:37:14.355: INFO: Pod "downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223": Phase="Pending", Reason="", readiness=false. Elapsed: 4.190014ms
Dec 12 22:37:16.360: INFO: Pod "downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008805909s
Dec 12 22:37:18.364: INFO: Pod "downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01304224s
STEP: Saw pod success
Dec 12 22:37:18.365: INFO: Pod "downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223" satisfied condition "success or failure"
Dec 12 22:37:18.369: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223 container client-container: <nil>
STEP: delete the pod
Dec 12 22:37:18.401: INFO: Waiting for pod downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223 to disappear
Dec 12 22:37:18.404: INFO: Pod downwardapi-volume-23be1b2c-ba83-4b22-85ac-d2eada752223 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:18.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2899" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":196,"skipped":3127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:18.436: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6860
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:35.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6860" for this suite.

• [SLOW TEST:17.258 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":197,"skipped":3178,"failed":0}
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:35.694: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:37:35.945: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9f078b9f-26ac-4ec6-987a-a0464bbb2a7a" in namespace "security-context-test-8644" to be "success or failure"
Dec 12 22:37:35.956: INFO: Pod "busybox-user-65534-9f078b9f-26ac-4ec6-987a-a0464bbb2a7a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.959316ms
Dec 12 22:37:37.963: INFO: Pod "busybox-user-65534-9f078b9f-26ac-4ec6-987a-a0464bbb2a7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017414266s
Dec 12 22:37:39.970: INFO: Pod "busybox-user-65534-9f078b9f-26ac-4ec6-987a-a0464bbb2a7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02460647s
Dec 12 22:37:39.970: INFO: Pod "busybox-user-65534-9f078b9f-26ac-4ec6-987a-a0464bbb2a7a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:39.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8644" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":3178,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:39.990: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6409" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":199,"skipped":3198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:44.345: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Dec 12 22:37:44.520: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Dec 12 22:37:44.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-8141'
Dec 12 22:37:44.914: INFO: stderr: ""
Dec 12 22:37:44.914: INFO: stdout: "service/agnhost-slave created\n"
Dec 12 22:37:44.914: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Dec 12 22:37:44.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-8141'
Dec 12 22:37:45.237: INFO: stderr: ""
Dec 12 22:37:45.237: INFO: stdout: "service/agnhost-master created\n"
Dec 12 22:37:45.237: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 12 22:37:45.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-8141'
Dec 12 22:37:45.612: INFO: stderr: ""
Dec 12 22:37:45.612: INFO: stdout: "service/frontend created\n"
Dec 12 22:37:45.612: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 12 22:37:45.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-8141'
Dec 12 22:37:45.902: INFO: stderr: ""
Dec 12 22:37:45.902: INFO: stdout: "deployment.apps/frontend created\n"
Dec 12 22:37:45.902: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 12 22:37:45.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-8141'
Dec 12 22:37:46.209: INFO: stderr: ""
Dec 12 22:37:46.209: INFO: stdout: "deployment.apps/agnhost-master created\n"
Dec 12 22:37:46.210: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 12 22:37:46.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-8141'
Dec 12 22:37:46.435: INFO: stderr: ""
Dec 12 22:37:46.435: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Dec 12 22:37:46.435: INFO: Waiting for all frontend pods to be Running.
Dec 12 22:37:51.485: INFO: Waiting for frontend to serve content.
Dec 12 22:37:51.503: INFO: Trying to add a new entry to the guestbook.
Dec 12 22:37:51.521: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 12 22:37:51.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-8141'
Dec 12 22:37:51.656: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:37:51.656: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 22:37:51.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-8141'
Dec 12 22:37:51.796: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:37:51.796: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 22:37:51.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-8141'
Dec 12 22:37:51.954: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:37:51.954: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 22:37:51.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-8141'
Dec 12 22:37:52.066: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:37:52.066: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 22:37:52.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-8141'
Dec 12 22:37:52.170: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:37:52.170: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 22:37:52.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-8141'
Dec 12 22:37:52.268: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:37:52.268: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:52.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8141" for this suite.

• [SLOW TEST:7.941 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:385
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":200,"skipped":3220,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:52.286: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 12 22:37:52.500: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:37:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6987" for this suite.

• [SLOW TEST:5.881 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":201,"skipped":3222,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:37:58.168: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-fe093ee4-27d0-49f4-942b-3442caf8402b
STEP: Creating a pod to test consume configMaps
Dec 12 22:37:58.368: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614" in namespace "projected-2112" to be "success or failure"
Dec 12 22:37:58.376: INFO: Pod "pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614": Phase="Pending", Reason="", readiness=false. Elapsed: 7.707359ms
Dec 12 22:38:00.381: INFO: Pod "pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013153448s
Dec 12 22:38:02.389: INFO: Pod "pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020231576s
STEP: Saw pod success
Dec 12 22:38:02.389: INFO: Pod "pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614" satisfied condition "success or failure"
Dec 12 22:38:02.394: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:38:02.441: INFO: Waiting for pod pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614 to disappear
Dec 12 22:38:02.446: INFO: Pod pod-projected-configmaps-6f4f5df3-e7f7-437f-9ec0-95555b7e0614 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:38:02.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2112" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3239,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:38:02.471: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:38:03.373: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 22:38:05.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787083, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787083, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787083, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787083, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:38:08.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:38:08.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7945" for this suite.
STEP: Destroying namespace "webhook-7945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":203,"skipped":3254,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:38:08.610: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:38:09.187: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 22:38:11.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787089, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787089, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787089, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787089, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:38:14.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:38:14.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1406" for this suite.
STEP: Destroying namespace "webhook-1406-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.831 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":204,"skipped":3271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:38:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3495
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 22:38:14.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 12 22:38:38.985: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.20 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3495 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:38:38.985: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:38:40.068: INFO: Found all expected endpoints: [netserver-0]
Dec 12 22:38:40.072: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3495 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:38:40.072: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:38:41.146: INFO: Found all expected endpoints: [netserver-1]
Dec 12 22:38:41.152: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3495 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:38:41.152: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:38:42.221: INFO: Found all expected endpoints: [netserver-2]
Dec 12 22:38:42.225: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3495 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:38:42.225: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:38:43.305: INFO: Found all expected endpoints: [netserver-3]
Dec 12 22:38:43.310: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.187 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3495 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:38:43.310: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:38:44.400: INFO: Found all expected endpoints: [netserver-4]
Dec 12 22:38:44.404: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.80 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3495 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 22:38:44.404: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:38:45.483: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:38:45.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3495" for this suite.

• [SLOW TEST:31.054 seconds]
[sig-network] Networking
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":205,"skipped":3343,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:38:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-36fc739a-d167-4b8d-84f0-3fcaf6106b2c
STEP: Creating a pod to test consume configMaps
Dec 12 22:38:45.778: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26" in namespace "projected-6707" to be "success or failure"
Dec 12 22:38:45.789: INFO: Pod "pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26": Phase="Pending", Reason="", readiness=false. Elapsed: 10.935907ms
Dec 12 22:38:47.794: INFO: Pod "pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015799026s
Dec 12 22:38:49.800: INFO: Pod "pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02174531s
STEP: Saw pod success
Dec 12 22:38:49.800: INFO: Pod "pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26" satisfied condition "success or failure"
Dec 12 22:38:49.805: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:38:49.835: INFO: Waiting for pod pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26 to disappear
Dec 12 22:38:49.838: INFO: Pod pod-projected-configmaps-a9550193-9b23-4383-9d5b-cf8076e35c26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:38:49.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6707" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3362,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:38:49.855: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Dec 12 22:38:50.067: INFO: PodSpec: initContainers in spec.initContainers
Dec 12 22:39:36.745: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c4ad2163-af33-4e0d-af71-93c409d1f8aa", GenerateName:"", Namespace:"init-container-4575", SelfLink:"/api/v1/namespaces/init-container-4575/pods/pod-init-c4ad2163-af33-4e0d-af71-93c409d1f8aa", UID:"63ea58a5-bfeb-4d84-9ee9-650731ef39e6", ResourceVersion:"23901", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711787130, loc:(*time.Location)(0x7d421e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"67948130"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-d7g28", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0032f2300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-d7g28", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-d7g28", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-d7g28", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002f0f998), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"talos-0-3-0-beta-0-gcp-workers-ntvmn", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0025ba120), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002f0fb00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002f0fbd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002f0fbd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002f0fbdc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787130, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787130, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787130, loc:(*time.Location)(0x7d421e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787130, loc:(*time.Location)(0x7d421e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.15.231", PodIP:"10.244.5.82", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.5.82"}}, StartTime:(*v1.Time)(0xc004838300), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002fa4310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002fa4380)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://e1a57174baf0f934a5f0e817e0cf7a553ea4dd2209f52d97ad605b6c2146e346", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004838340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004838320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002f0fcff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:39:36.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4575" for this suite.

• [SLOW TEST:46.906 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":207,"skipped":3383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:39:36.762: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-5c09f52e-ee2c-44eb-90ad-70bb8f749ea9
STEP: Creating a pod to test consume configMaps
Dec 12 22:39:36.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c" in namespace "configmap-2382" to be "success or failure"
Dec 12 22:39:37.005: INFO: Pod "pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.388976ms
Dec 12 22:39:39.010: INFO: Pod "pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015508694s
Dec 12 22:39:41.014: INFO: Pod "pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020009705s
STEP: Saw pod success
Dec 12 22:39:41.014: INFO: Pod "pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c" satisfied condition "success or failure"
Dec 12 22:39:41.018: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:39:41.067: INFO: Waiting for pod pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c to disappear
Dec 12 22:39:41.071: INFO: Pod pod-configmaps-e85cdce8-2c7d-40ad-b507-f450965b2e1c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:39:41.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2382" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3413,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:39:41.087: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7198
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7198
I1212 22:39:41.295575      22 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7198, replica count: 2
I1212 22:39:44.346035      22 runners.go:189] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:39:47.346294      22 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 22:39:47.346: INFO: Creating new exec pod
Dec 12 22:39:52.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-7198 execpodh487j -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 12 22:39:52.948: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 12 22:39:52.948: INFO: stdout: ""
Dec 12 22:39:52.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-7198 execpodh487j -- /bin/sh -x -c nc -zv -t -w 2 10.96.109.73 80'
Dec 12 22:39:53.144: INFO: stderr: "+ nc -zv -t -w 2 10.96.109.73 80\nConnection to 10.96.109.73 80 port [tcp/http] succeeded!\n"
Dec 12 22:39:53.144: INFO: stdout: ""
Dec 12 22:39:53.144: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:39:53.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7198" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:12.098 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":209,"skipped":3416,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:39:53.186: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:40:09.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4416" for this suite.

• [SLOW TEST:16.350 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":210,"skipped":3429,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:40:09.538: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-rhvmg in namespace proxy-7046
I1212 22:40:09.744107      22 runners.go:189] Created replication controller with name: proxy-service-rhvmg, namespace: proxy-7046, replica count: 1
I1212 22:40:10.794546      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:40:11.794814      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:40:12.795114      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:40:13.795471      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:14.795692      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:15.795916      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:16.796159      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:17.796496      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:18.796774      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:19.797016      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 22:40:20.797300      22 runners.go:189] proxy-service-rhvmg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 22:40:20.801: INFO: setup took 11.082281979s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 12 22:40:20.818: INFO: (0) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.196948ms)
Dec 12 22:40:20.819: INFO: (0) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 17.147846ms)
Dec 12 22:40:20.819: INFO: (0) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 16.799818ms)
Dec 12 22:40:20.824: INFO: (0) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 22.237096ms)
Dec 12 22:40:20.825: INFO: (0) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 22.63002ms)
Dec 12 22:40:20.825: INFO: (0) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 23.371104ms)
Dec 12 22:40:20.825: INFO: (0) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 23.312339ms)
Dec 12 22:40:20.826: INFO: (0) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 24.688654ms)
Dec 12 22:40:20.826: INFO: (0) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 24.415814ms)
Dec 12 22:40:20.826: INFO: (0) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 24.927135ms)
Dec 12 22:40:20.826: INFO: (0) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 24.682331ms)
Dec 12 22:40:20.828: INFO: (0) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 25.6566ms)
Dec 12 22:40:20.828: INFO: (0) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 26.604579ms)
Dec 12 22:40:20.828: INFO: (0) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 26.099877ms)
Dec 12 22:40:20.828: INFO: (0) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 26.493284ms)
Dec 12 22:40:20.828: INFO: (0) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 26.735762ms)
Dec 12 22:40:20.839: INFO: (1) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 10.639933ms)
Dec 12 22:40:20.841: INFO: (1) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 13.081054ms)
Dec 12 22:40:20.841: INFO: (1) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 12.712926ms)
Dec 12 22:40:20.842: INFO: (1) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 12.776551ms)
Dec 12 22:40:20.843: INFO: (1) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 14.29635ms)
Dec 12 22:40:20.843: INFO: (1) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 13.83589ms)
Dec 12 22:40:20.843: INFO: (1) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 13.746532ms)
Dec 12 22:40:20.843: INFO: (1) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 14.301716ms)
Dec 12 22:40:20.844: INFO: (1) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 15.060263ms)
Dec 12 22:40:20.855: INFO: (1) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 25.462367ms)
Dec 12 22:40:20.855: INFO: (1) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 27.047078ms)
Dec 12 22:40:20.856: INFO: (1) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 26.224346ms)
Dec 12 22:40:20.856: INFO: (1) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 26.954457ms)
Dec 12 22:40:20.856: INFO: (1) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 26.920828ms)
Dec 12 22:40:20.856: INFO: (1) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 27.274526ms)
Dec 12 22:40:20.859: INFO: (1) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 30.365818ms)
Dec 12 22:40:20.867: INFO: (2) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 7.406118ms)
Dec 12 22:40:20.869: INFO: (2) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 8.339716ms)
Dec 12 22:40:20.869: INFO: (2) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 8.563467ms)
Dec 12 22:40:20.870: INFO: (2) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 10.261446ms)
Dec 12 22:40:20.875: INFO: (2) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 13.975313ms)
Dec 12 22:40:20.876: INFO: (2) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 15.547328ms)
Dec 12 22:40:20.877: INFO: (2) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 15.925071ms)
Dec 12 22:40:20.877: INFO: (2) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 17.034037ms)
Dec 12 22:40:20.877: INFO: (2) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 15.646268ms)
Dec 12 22:40:20.879: INFO: (2) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 17.507904ms)
Dec 12 22:40:20.882: INFO: (2) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 20.932963ms)
Dec 12 22:40:20.882: INFO: (2) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 20.597879ms)
Dec 12 22:40:20.882: INFO: (2) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 21.090794ms)
Dec 12 22:40:20.882: INFO: (2) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 20.910366ms)
Dec 12 22:40:20.882: INFO: (2) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 21.109047ms)
Dec 12 22:40:20.882: INFO: (2) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 21.315683ms)
Dec 12 22:40:20.889: INFO: (3) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 6.753915ms)
Dec 12 22:40:20.895: INFO: (3) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 11.886894ms)
Dec 12 22:40:20.895: INFO: (3) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 11.681159ms)
Dec 12 22:40:20.899: INFO: (3) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 15.595922ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 20.29241ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 20.766046ms)
Dec 12 22:40:20.905: INFO: (3) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 20.985332ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 20.675633ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 20.033291ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 20.127609ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 20.2811ms)
Dec 12 22:40:20.904: INFO: (3) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 20.319485ms)
Dec 12 22:40:20.905: INFO: (3) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 21.427432ms)
Dec 12 22:40:20.905: INFO: (3) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 21.529623ms)
Dec 12 22:40:20.905: INFO: (3) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 21.770004ms)
Dec 12 22:40:20.906: INFO: (3) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 23.0393ms)
Dec 12 22:40:20.915: INFO: (4) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 8.833498ms)
Dec 12 22:40:20.916: INFO: (4) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 9.375018ms)
Dec 12 22:40:20.922: INFO: (4) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 15.737448ms)
Dec 12 22:40:20.924: INFO: (4) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 16.823369ms)
Dec 12 22:40:20.923: INFO: (4) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 16.281024ms)
Dec 12 22:40:20.924: INFO: (4) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 17.048474ms)
Dec 12 22:40:20.924: INFO: (4) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 16.798253ms)
Dec 12 22:40:20.924: INFO: (4) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.867482ms)
Dec 12 22:40:20.925: INFO: (4) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 18.884132ms)
Dec 12 22:40:20.925: INFO: (4) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 18.027228ms)
Dec 12 22:40:20.925: INFO: (4) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 18.324583ms)
Dec 12 22:40:20.926: INFO: (4) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 19.174085ms)
Dec 12 22:40:20.926: INFO: (4) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 19.735346ms)
Dec 12 22:40:20.927: INFO: (4) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 19.76762ms)
Dec 12 22:40:20.927: INFO: (4) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 20.470729ms)
Dec 12 22:40:20.927: INFO: (4) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 20.281493ms)
Dec 12 22:40:20.934: INFO: (5) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 6.449889ms)
Dec 12 22:40:20.934: INFO: (5) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 6.99626ms)
Dec 12 22:40:20.934: INFO: (5) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 6.824572ms)
Dec 12 22:40:20.936: INFO: (5) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 8.172542ms)
Dec 12 22:40:20.939: INFO: (5) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 11.400063ms)
Dec 12 22:40:20.939: INFO: (5) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 11.57699ms)
Dec 12 22:40:20.940: INFO: (5) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 11.819384ms)
Dec 12 22:40:20.940: INFO: (5) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 12.600125ms)
Dec 12 22:40:20.941: INFO: (5) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 13.752011ms)
Dec 12 22:40:20.942: INFO: (5) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 14.592655ms)
Dec 12 22:40:20.942: INFO: (5) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 14.181508ms)
Dec 12 22:40:20.944: INFO: (5) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 16.522306ms)
Dec 12 22:40:20.946: INFO: (5) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 17.700471ms)
Dec 12 22:40:20.946: INFO: (5) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 18.062867ms)
Dec 12 22:40:20.947: INFO: (5) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 18.775608ms)
Dec 12 22:40:20.950: INFO: (5) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 21.460233ms)
Dec 12 22:40:20.961: INFO: (6) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 11.032579ms)
Dec 12 22:40:20.966: INFO: (6) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 15.711628ms)
Dec 12 22:40:20.967: INFO: (6) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.288196ms)
Dec 12 22:40:20.967: INFO: (6) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 16.397053ms)
Dec 12 22:40:20.968: INFO: (6) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 17.465942ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 18.170449ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 18.687987ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 18.557586ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 17.892756ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 18.054071ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 18.312837ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 17.966127ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 18.247982ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 18.333337ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 18.333159ms)
Dec 12 22:40:20.969: INFO: (6) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 19.481275ms)
Dec 12 22:40:20.983: INFO: (7) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 13.105522ms)
Dec 12 22:40:20.983: INFO: (7) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 12.993326ms)
Dec 12 22:40:20.983: INFO: (7) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 13.191143ms)
Dec 12 22:40:20.986: INFO: (7) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 16.788238ms)
Dec 12 22:40:20.986: INFO: (7) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 17.090916ms)
Dec 12 22:40:20.986: INFO: (7) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 16.601972ms)
Dec 12 22:40:20.986: INFO: (7) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.976798ms)
Dec 12 22:40:20.986: INFO: (7) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 16.755699ms)
Dec 12 22:40:20.987: INFO: (7) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 17.442403ms)
Dec 12 22:40:20.987: INFO: (7) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 17.055112ms)
Dec 12 22:40:20.987: INFO: (7) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 17.173662ms)
Dec 12 22:40:20.987: INFO: (7) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 17.134454ms)
Dec 12 22:40:20.988: INFO: (7) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 18.024556ms)
Dec 12 22:40:20.988: INFO: (7) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 18.138113ms)
Dec 12 22:40:20.988: INFO: (7) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 18.614568ms)
Dec 12 22:40:20.988: INFO: (7) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 18.845036ms)
Dec 12 22:40:20.998: INFO: (8) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 8.960941ms)
Dec 12 22:40:20.998: INFO: (8) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 8.760542ms)
Dec 12 22:40:21.000: INFO: (8) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 10.040178ms)
Dec 12 22:40:21.001: INFO: (8) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 12.05386ms)
Dec 12 22:40:21.002: INFO: (8) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 12.338415ms)
Dec 12 22:40:21.002: INFO: (8) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 12.644085ms)
Dec 12 22:40:21.003: INFO: (8) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 13.056976ms)
Dec 12 22:40:21.005: INFO: (8) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 15.735863ms)
Dec 12 22:40:21.006: INFO: (8) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 15.91335ms)
Dec 12 22:40:21.006: INFO: (8) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 15.757196ms)
Dec 12 22:40:21.006: INFO: (8) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 16.664805ms)
Dec 12 22:40:21.006: INFO: (8) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 16.655352ms)
Dec 12 22:40:21.007: INFO: (8) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 16.92051ms)
Dec 12 22:40:21.008: INFO: (8) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 17.956674ms)
Dec 12 22:40:21.008: INFO: (8) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 17.758273ms)
Dec 12 22:40:21.008: INFO: (8) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 18.134195ms)
Dec 12 22:40:21.021: INFO: (9) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 11.619618ms)
Dec 12 22:40:21.023: INFO: (9) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 14.878741ms)
Dec 12 22:40:21.024: INFO: (9) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 14.442411ms)
Dec 12 22:40:21.024: INFO: (9) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 15.196263ms)
Dec 12 22:40:21.025: INFO: (9) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 15.355413ms)
Dec 12 22:40:21.025: INFO: (9) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 15.226756ms)
Dec 12 22:40:21.025: INFO: (9) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 16.446395ms)
Dec 12 22:40:21.026: INFO: (9) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 16.367276ms)
Dec 12 22:40:21.026: INFO: (9) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 16.85494ms)
Dec 12 22:40:21.027: INFO: (9) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 17.171717ms)
Dec 12 22:40:21.027: INFO: (9) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 17.7274ms)
Dec 12 22:40:21.028: INFO: (9) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 19.231543ms)
Dec 12 22:40:21.028: INFO: (9) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 18.521231ms)
Dec 12 22:40:21.028: INFO: (9) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 18.569159ms)
Dec 12 22:40:21.028: INFO: (9) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 18.791494ms)
Dec 12 22:40:21.028: INFO: (9) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 18.534641ms)
Dec 12 22:40:21.036: INFO: (10) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 7.53308ms)
Dec 12 22:40:21.038: INFO: (10) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 9.458103ms)
Dec 12 22:40:21.039: INFO: (10) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 10.343309ms)
Dec 12 22:40:21.042: INFO: (10) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 13.641294ms)
Dec 12 22:40:21.042: INFO: (10) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 13.468154ms)
Dec 12 22:40:21.044: INFO: (10) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 15.358427ms)
Dec 12 22:40:21.045: INFO: (10) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 16.295236ms)
Dec 12 22:40:21.045: INFO: (10) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 16.220528ms)
Dec 12 22:40:21.046: INFO: (10) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.923314ms)
Dec 12 22:40:21.046: INFO: (10) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 17.216751ms)
Dec 12 22:40:21.049: INFO: (10) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 20.068663ms)
Dec 12 22:40:21.049: INFO: (10) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 19.872588ms)
Dec 12 22:40:21.051: INFO: (10) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 21.572491ms)
Dec 12 22:40:21.052: INFO: (10) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 23.837171ms)
Dec 12 22:40:21.053: INFO: (10) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 23.433973ms)
Dec 12 22:40:21.053: INFO: (10) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 23.78918ms)
Dec 12 22:40:21.069: INFO: (11) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 11.70508ms)
Dec 12 22:40:21.071: INFO: (11) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 13.720371ms)
Dec 12 22:40:21.072: INFO: (11) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 18.573905ms)
Dec 12 22:40:21.072: INFO: (11) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 14.144651ms)
Dec 12 22:40:21.072: INFO: (11) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 14.915031ms)
Dec 12 22:40:21.072: INFO: (11) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 15.199061ms)
Dec 12 22:40:21.073: INFO: (11) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 15.013198ms)
Dec 12 22:40:21.073: INFO: (11) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 15.27734ms)
Dec 12 22:40:21.077: INFO: (11) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 18.961991ms)
Dec 12 22:40:21.077: INFO: (11) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 19.126571ms)
Dec 12 22:40:21.077: INFO: (11) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 19.32675ms)
Dec 12 22:40:21.078: INFO: (11) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 19.934779ms)
Dec 12 22:40:21.078: INFO: (11) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 20.176686ms)
Dec 12 22:40:21.078: INFO: (11) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 20.199706ms)
Dec 12 22:40:21.078: INFO: (11) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 20.218758ms)
Dec 12 22:40:21.079: INFO: (11) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 21.000967ms)
Dec 12 22:40:21.095: INFO: (12) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 15.710026ms)
Dec 12 22:40:21.096: INFO: (12) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 16.764407ms)
Dec 12 22:40:21.096: INFO: (12) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.47767ms)
Dec 12 22:40:21.096: INFO: (12) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 16.785945ms)
Dec 12 22:40:21.097: INFO: (12) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 17.676563ms)
Dec 12 22:40:21.098: INFO: (12) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 18.169173ms)
Dec 12 22:40:21.098: INFO: (12) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 18.794051ms)
Dec 12 22:40:21.099: INFO: (12) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 19.657192ms)
Dec 12 22:40:21.099: INFO: (12) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 20.091189ms)
Dec 12 22:40:21.099: INFO: (12) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 20.071867ms)
Dec 12 22:40:21.099: INFO: (12) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 20.235337ms)
Dec 12 22:40:21.099: INFO: (12) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 20.31375ms)
Dec 12 22:40:21.100: INFO: (12) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 20.986908ms)
Dec 12 22:40:21.100: INFO: (12) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 20.824889ms)
Dec 12 22:40:21.102: INFO: (12) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 23.135986ms)
Dec 12 22:40:21.102: INFO: (12) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 22.909875ms)
Dec 12 22:40:21.117: INFO: (13) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 13.719199ms)
Dec 12 22:40:21.118: INFO: (13) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 14.607538ms)
Dec 12 22:40:21.123: INFO: (13) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 19.850886ms)
Dec 12 22:40:21.125: INFO: (13) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 21.121743ms)
Dec 12 22:40:21.125: INFO: (13) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 21.317359ms)
Dec 12 22:40:21.125: INFO: (13) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 21.221592ms)
Dec 12 22:40:21.125: INFO: (13) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 21.602754ms)
Dec 12 22:40:21.126: INFO: (13) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 22.044254ms)
Dec 12 22:40:21.127: INFO: (13) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 23.336671ms)
Dec 12 22:40:21.127: INFO: (13) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 23.481123ms)
Dec 12 22:40:21.127: INFO: (13) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 24.305882ms)
Dec 12 22:40:21.127: INFO: (13) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 23.508432ms)
Dec 12 22:40:21.127: INFO: (13) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 23.744763ms)
Dec 12 22:40:21.127: INFO: (13) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 23.578485ms)
Dec 12 22:40:21.128: INFO: (13) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 23.893246ms)
Dec 12 22:40:21.128: INFO: (13) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 24.366082ms)
Dec 12 22:40:21.136: INFO: (14) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 8.15306ms)
Dec 12 22:40:21.137: INFO: (14) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 9.384132ms)
Dec 12 22:40:21.138: INFO: (14) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 9.854937ms)
Dec 12 22:40:21.138: INFO: (14) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 9.867056ms)
Dec 12 22:40:21.140: INFO: (14) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 11.352452ms)
Dec 12 22:40:21.140: INFO: (14) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 11.318163ms)
Dec 12 22:40:21.140: INFO: (14) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 11.198374ms)
Dec 12 22:40:21.140: INFO: (14) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 11.549021ms)
Dec 12 22:40:21.140: INFO: (14) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 12.234228ms)
Dec 12 22:40:21.141: INFO: (14) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 13.219952ms)
Dec 12 22:40:21.143: INFO: (14) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 14.489059ms)
Dec 12 22:40:21.143: INFO: (14) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 14.402377ms)
Dec 12 22:40:21.147: INFO: (14) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 18.807926ms)
Dec 12 22:40:21.147: INFO: (14) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 18.778943ms)
Dec 12 22:40:21.147: INFO: (14) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 18.685707ms)
Dec 12 22:40:21.148: INFO: (14) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 19.41471ms)
Dec 12 22:40:21.160: INFO: (15) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 11.335421ms)
Dec 12 22:40:21.161: INFO: (15) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 12.851584ms)
Dec 12 22:40:21.161: INFO: (15) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 12.915482ms)
Dec 12 22:40:21.162: INFO: (15) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 13.99928ms)
Dec 12 22:40:21.162: INFO: (15) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 13.815152ms)
Dec 12 22:40:21.163: INFO: (15) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 13.757868ms)
Dec 12 22:40:21.164: INFO: (15) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 15.443534ms)
Dec 12 22:40:21.165: INFO: (15) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 15.803921ms)
Dec 12 22:40:21.165: INFO: (15) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 16.212695ms)
Dec 12 22:40:21.165: INFO: (15) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 16.205903ms)
Dec 12 22:40:21.167: INFO: (15) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 17.809562ms)
Dec 12 22:40:21.170: INFO: (15) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 21.039496ms)
Dec 12 22:40:21.170: INFO: (15) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 21.267404ms)
Dec 12 22:40:21.170: INFO: (15) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 20.969546ms)
Dec 12 22:40:21.170: INFO: (15) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 21.396215ms)
Dec 12 22:40:21.170: INFO: (15) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 21.012316ms)
Dec 12 22:40:21.177: INFO: (16) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 6.636846ms)
Dec 12 22:40:21.179: INFO: (16) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 8.138959ms)
Dec 12 22:40:21.182: INFO: (16) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 10.738295ms)
Dec 12 22:40:21.183: INFO: (16) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 11.830884ms)
Dec 12 22:40:21.184: INFO: (16) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 13.187841ms)
Dec 12 22:40:21.184: INFO: (16) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 12.648263ms)
Dec 12 22:40:21.187: INFO: (16) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 15.620206ms)
Dec 12 22:40:21.187: INFO: (16) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 15.688157ms)
Dec 12 22:40:21.188: INFO: (16) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 16.18408ms)
Dec 12 22:40:21.188: INFO: (16) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 16.215499ms)
Dec 12 22:40:21.189: INFO: (16) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 17.077257ms)
Dec 12 22:40:21.190: INFO: (16) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 18.52406ms)
Dec 12 22:40:21.190: INFO: (16) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 19.260051ms)
Dec 12 22:40:21.191: INFO: (16) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 19.715453ms)
Dec 12 22:40:21.191: INFO: (16) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 19.614951ms)
Dec 12 22:40:21.191: INFO: (16) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 19.618558ms)
Dec 12 22:40:21.201: INFO: (17) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 9.035641ms)
Dec 12 22:40:21.202: INFO: (17) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 9.292461ms)
Dec 12 22:40:21.203: INFO: (17) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 11.733457ms)
Dec 12 22:40:21.205: INFO: (17) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 12.26989ms)
Dec 12 22:40:21.205: INFO: (17) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 12.121302ms)
Dec 12 22:40:21.205: INFO: (17) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 12.190373ms)
Dec 12 22:40:21.206: INFO: (17) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 14.992329ms)
Dec 12 22:40:21.207: INFO: (17) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 14.345324ms)
Dec 12 22:40:21.208: INFO: (17) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 15.188123ms)
Dec 12 22:40:21.208: INFO: (17) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 15.358368ms)
Dec 12 22:40:21.209: INFO: (17) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 17.391248ms)
Dec 12 22:40:21.210: INFO: (17) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 17.284443ms)
Dec 12 22:40:21.210: INFO: (17) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 17.672385ms)
Dec 12 22:40:21.218: INFO: (17) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 25.522519ms)
Dec 12 22:40:21.218: INFO: (17) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 26.00474ms)
Dec 12 22:40:21.218: INFO: (17) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 25.07595ms)
Dec 12 22:40:21.229: INFO: (18) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 10.64199ms)
Dec 12 22:40:21.229: INFO: (18) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 10.39152ms)
Dec 12 22:40:21.229: INFO: (18) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 9.856346ms)
Dec 12 22:40:21.230: INFO: (18) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 10.56832ms)
Dec 12 22:40:21.233: INFO: (18) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 13.466095ms)
Dec 12 22:40:21.238: INFO: (18) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 18.583401ms)
Dec 12 22:40:21.239: INFO: (18) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 19.81575ms)
Dec 12 22:40:21.239: INFO: (18) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 19.722431ms)
Dec 12 22:40:21.240: INFO: (18) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 19.885719ms)
Dec 12 22:40:21.240: INFO: (18) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 20.718787ms)
Dec 12 22:40:21.240: INFO: (18) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 19.713423ms)
Dec 12 22:40:21.241: INFO: (18) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 21.884608ms)
Dec 12 22:40:21.242: INFO: (18) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 22.690608ms)
Dec 12 22:40:21.242: INFO: (18) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 21.597286ms)
Dec 12 22:40:21.242: INFO: (18) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 21.666334ms)
Dec 12 22:40:21.242: INFO: (18) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 22.398236ms)
Dec 12 22:40:21.250: INFO: (19) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 7.58896ms)
Dec 12 22:40:21.251: INFO: (19) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname1/proxy/: foo (200; 8.461419ms)
Dec 12 22:40:21.252: INFO: (19) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:460/proxy/: tls baz (200; 8.784828ms)
Dec 12 22:40:21.257: INFO: (19) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname1/proxy/: foo (200; 13.772292ms)
Dec 12 22:40:21.259: INFO: (19) /api/v1/namespaces/proxy-7046/services/proxy-service-rhvmg:portname2/proxy/: bar (200; 16.133896ms)
Dec 12 22:40:21.260: INFO: (19) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:162/proxy/: bar (200; 17.585953ms)
Dec 12 22:40:21.261: INFO: (19) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 17.952596ms)
Dec 12 22:40:21.261: INFO: (19) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">test<... (200; 17.788633ms)
Dec 12 22:40:21.261: INFO: (19) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:1080/proxy/rewriteme">... (200; 18.330999ms)
Dec 12 22:40:21.262: INFO: (19) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:462/proxy/: tls qux (200; 18.353979ms)
Dec 12 22:40:21.263: INFO: (19) /api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/https:proxy-service-rhvmg-rsf78:443/proxy/tlsrewritem... (200; 19.450038ms)
Dec 12 22:40:21.264: INFO: (19) /api/v1/namespaces/proxy-7046/pods/http:proxy-service-rhvmg-rsf78:160/proxy/: foo (200; 19.834653ms)
Dec 12 22:40:21.264: INFO: (19) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname1/proxy/: tls baz (200; 21.043858ms)
Dec 12 22:40:21.264: INFO: (19) /api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/: <a href="/api/v1/namespaces/proxy-7046/pods/proxy-service-rhvmg-rsf78/proxy/rewriteme">test</a> (200; 21.51565ms)
Dec 12 22:40:21.265: INFO: (19) /api/v1/namespaces/proxy-7046/services/http:proxy-service-rhvmg:portname2/proxy/: bar (200; 20.825354ms)
Dec 12 22:40:21.267: INFO: (19) /api/v1/namespaces/proxy-7046/services/https:proxy-service-rhvmg:tlsportname2/proxy/: tls qux (200; 23.471425ms)
STEP: deleting ReplicationController proxy-service-rhvmg in namespace proxy-7046, will wait for the garbage collector to delete the pods
Dec 12 22:40:21.333: INFO: Deleting ReplicationController proxy-service-rhvmg took: 11.710164ms
Dec 12 22:40:21.633: INFO: Terminating ReplicationController proxy-service-rhvmg pods took: 300.285512ms
[AfterEach] version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:40:26.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7046" for this suite.

• [SLOW TEST:17.011 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":211,"skipped":3441,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:40:26.549: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7896
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 12 22:40:26.721: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
Dec 12 22:40:29.617: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:40:41.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7896" for this suite.

• [SLOW TEST:14.610 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":212,"skipped":3445,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:40:41.163: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 12 22:40:41.323: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 12 22:40:41.336: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 22:40:41.340: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-0 before test
Dec 12 22:40:41.361: INFO: kube-proxy-bkbtw from kube-system started at 2019-12-12 21:35:43 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.361: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:40:41.361: INFO: kube-apiserver-9sv5v from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.361: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 12 22:40:41.361: INFO: pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.361: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:40:41.361: INFO: kube-flannel-69ftd from kube-system started at 2019-12-12 21:35:43 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.361: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:40:41.361: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:40:41.362: INFO: pod-checkpointer-wbndz from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.362: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:40:41.362: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-1 before test
Dec 12 22:40:41.385: INFO: kube-proxy-mb48g from kube-system started at 2019-12-12 21:35:36 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.385: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:40:41.385: INFO: kube-controller-manager-5c547b548-hxgrq from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.385: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 12 22:40:41.385: INFO: pod-checkpointer-8ftvl-talos-0-3-0-beta-0-gcp-controlplane-1 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.385: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:40:41.385: INFO: pod-checkpointer-8ftvl from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.385: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:40:41.385: INFO: coredns-5bcc66f5bd-6pzdf from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.385: INFO: 	Container coredns ready: true, restart count 0
Dec 12 22:40:41.386: INFO: kube-scheduler-6db5bfb7bc-tx7bk from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.386: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 12 22:40:41.386: INFO: kube-flannel-cq4nw from kube-system started at 2019-12-12 21:35:36 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.386: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:40:41.386: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:40:41.386: INFO: kube-scheduler-6db5bfb7bc-dplt8 from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.386: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 12 22:40:41.386: INFO: kube-controller-manager-5c547b548-cqqln from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.386: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 12 22:40:41.386: INFO: kube-apiserver-j24rk from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.386: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:40:41.386: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-2 before test
Dec 12 22:40:41.408: INFO: kube-apiserver-kf44j from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.408: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:40:41.408: INFO: kube-proxy-9wbfd from kube-system started at 2019-12-12 21:35:29 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.408: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:40:41.408: INFO: kube-flannel-9hznp from kube-system started at 2019-12-12 21:35:29 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.408: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:40:41.408: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:40:41.408: INFO: pod-checkpointer-5mnr9 from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.408: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:40:41.408: INFO: pod-checkpointer-5mnr9-talos-0-3-0-beta-0-gcp-controlplane-2 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.408: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:40:41.408: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-4kj7s before test
Dec 12 22:40:41.429: INFO: kube-flannel-bz9ht from kube-system started at 2019-12-12 21:36:55 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.429: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:40:41.429: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:40:41.429: INFO: kube-proxy-xh54v from kube-system started at 2019-12-12 21:36:55 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.429: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:40:41.429: INFO: sonobuoy-e2e-job-01c7b380be144838 from sonobuoy started at 2019-12-12 21:39:37 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.430: INFO: 	Container e2e ready: true, restart count 0
Dec 12 22:40:41.430: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 12 22:40:41.430: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-nt7qc before test
Dec 12 22:40:41.437: INFO: kube-proxy-jtlmf from kube-system started at 2019-12-12 21:37:10 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.437: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:40:41.437: INFO: kube-flannel-p79j9 from kube-system started at 2019-12-12 21:37:10 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.437: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:40:41.437: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:40:41.437: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-ntvmn before test
Dec 12 22:40:41.458: INFO: kube-proxy-bvldz from kube-system started at 2019-12-12 21:37:28 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.458: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:40:41.458: INFO: kube-flannel-gk4cx from kube-system started at 2019-12-12 21:37:28 +0000 UTC (2 container statuses recorded)
Dec 12 22:40:41.458: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:40:41.458: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:40:41.458: INFO: sonobuoy from sonobuoy started at 2019-12-12 21:39:34 +0000 UTC (1 container statuses recorded)
Dec 12 22:40:41.458: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b7dedb02-3dfe-45c3-9aef-d31a8cb44be4 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b7dedb02-3dfe-45c3-9aef-d31a8cb44be4 off the node talos-0-3-0-beta-0-gcp-workers-nt7qc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b7dedb02-3dfe-45c3-9aef-d31a8cb44be4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:45:47.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7461" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:306.448 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":213,"skipped":3465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:45:47.612: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:45:51.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4387" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3507,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:45:51.897: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 22:45:55.145: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:45:55.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8254" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3507,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:45:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1212 22:46:25.946858      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 22:46:25.946: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:46:25.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-764" for this suite.

• [SLOW TEST:30.771 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":216,"skipped":3510,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:46:25.966: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-0f0874cb-3f15-470c-a49b-dc879bbee907
STEP: Creating a pod to test consume secrets
Dec 12 22:46:26.202: INFO: Waiting up to 5m0s for pod "pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596" in namespace "secrets-8678" to be "success or failure"
Dec 12 22:46:26.227: INFO: Pod "pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596": Phase="Pending", Reason="", readiness=false. Elapsed: 25.451669ms
Dec 12 22:46:28.232: INFO: Pod "pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030550221s
Dec 12 22:46:30.236: INFO: Pod "pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034546584s
STEP: Saw pod success
Dec 12 22:46:30.236: INFO: Pod "pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596" satisfied condition "success or failure"
Dec 12 22:46:30.241: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:46:30.276: INFO: Waiting for pod pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596 to disappear
Dec 12 22:46:30.280: INFO: Pod pod-secrets-2a61bbaf-5f3e-47b5-93d0-1246ef2b7596 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:46:30.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8678" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3525,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:46:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:46:30.464: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 12 22:46:32.506: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:46:33.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4602" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":218,"skipped":3538,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:46:33.529: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:46:34.389: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 22:46:36.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787594, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787594, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787594, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787594, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:46:39.425: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:46:39.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7702" for this suite.
STEP: Destroying namespace "webhook-7702-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.088 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":219,"skipped":3546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:46:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5204
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Dec 12 22:46:39.835: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:46:54.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5204" for this suite.

• [SLOW TEST:15.232 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":220,"skipped":3588,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:46:54.852: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:46:55.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2" in namespace "projected-5357" to be "success or failure"
Dec 12 22:46:55.055: INFO: Pod "downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.987277ms
Dec 12 22:46:57.059: INFO: Pod "downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008774201s
Dec 12 22:46:59.065: INFO: Pod "downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014795248s
STEP: Saw pod success
Dec 12 22:46:59.065: INFO: Pod "downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2" satisfied condition "success or failure"
Dec 12 22:46:59.069: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2 container client-container: <nil>
STEP: delete the pod
Dec 12 22:46:59.090: INFO: Waiting for pod downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2 to disappear
Dec 12 22:46:59.094: INFO: Pod downwardapi-volume-e4139054-a655-4c53-9da1-e326690e50d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:46:59.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5357" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":221,"skipped":3601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:46:59.109: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:46:59.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 22:47:01.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787620, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787620, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787620, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787619, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:47:04.946: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 12 22:47:08.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 attach --namespace=webhook-2677 to-be-attached-pod -i -c=container1'
Dec 12 22:47:09.111: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:47:09.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2677" for this suite.
STEP: Destroying namespace "webhook-2677-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":222,"skipped":3626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:47:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 12 22:47:13.927: INFO: Successfully updated pod "labelsupdatea3e63cbb-2588-4d8b-aaf6-9995b4570942"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:47:15.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3834" for this suite.

• [SLOW TEST:6.753 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":223,"skipped":3668,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:47:15.958: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:47:20.190: INFO: Waiting up to 5m0s for pod "client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d" in namespace "pods-7165" to be "success or failure"
Dec 12 22:47:20.198: INFO: Pod "client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.213907ms
Dec 12 22:47:22.202: INFO: Pod "client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0123825s
Dec 12 22:47:24.207: INFO: Pod "client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017298927s
STEP: Saw pod success
Dec 12 22:47:24.207: INFO: Pod "client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d" satisfied condition "success or failure"
Dec 12 22:47:24.211: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-ntvmn pod client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d container env3cont: <nil>
STEP: delete the pod
Dec 12 22:47:24.245: INFO: Waiting for pod client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d to disappear
Dec 12 22:47:24.250: INFO: Pod client-envvars-00b01c13-0cab-4a86-b139-ad07441da61d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:47:24.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7165" for this suite.

• [SLOW TEST:8.304 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":224,"skipped":3677,"failed":0}
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:47:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 12 22:47:24.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6295 /api/v1/namespaces/watch-6295/configmaps/e2e-watch-test-label-changed 0caa198a-79df-4d34-90bb-cd8386f46562 26053 0 2019-12-12 22:47:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 22:47:24.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6295 /api/v1/namespaces/watch-6295/configmaps/e2e-watch-test-label-changed 0caa198a-79df-4d34-90bb-cd8386f46562 26054 0 2019-12-12 22:47:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 12 22:47:24.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6295 /api/v1/namespaces/watch-6295/configmaps/e2e-watch-test-label-changed 0caa198a-79df-4d34-90bb-cd8386f46562 26055 0 2019-12-12 22:47:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 12 22:47:34.526: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6295 /api/v1/namespaces/watch-6295/configmaps/e2e-watch-test-label-changed 0caa198a-79df-4d34-90bb-cd8386f46562 26108 0 2019-12-12 22:47:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 22:47:34.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6295 /api/v1/namespaces/watch-6295/configmaps/e2e-watch-test-label-changed 0caa198a-79df-4d34-90bb-cd8386f46562 26109 0 2019-12-12 22:47:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 12 22:47:34.526: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6295 /api/v1/namespaces/watch-6295/configmaps/e2e-watch-test-label-changed 0caa198a-79df-4d34-90bb-cd8386f46562 26110 0 2019-12-12 22:47:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:47:34.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6295" for this suite.

• [SLOW TEST:10.273 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":225,"skipped":3677,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:47:34.539: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:47:34.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1289" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":226,"skipped":3691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:47:34.766: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 12 22:47:34.928: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Dec 12 22:47:35.433: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Dec 12 22:47:37.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:39.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:41.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:43.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:45.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:47.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:49.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787655, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:47:54.528: INFO: Waited 3.024167033s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:47:54.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7387" for this suite.

• [SLOW TEST:20.300 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":227,"skipped":3717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:47:55.067: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-964
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-964
I1212 22:47:55.410125      22 runners.go:189] Created replication controller with name: externalname-service, namespace: services-964, replica count: 2
Dec 12 22:47:58.461: INFO: Creating new exec pod
I1212 22:47:58.461404      22 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 22:48:03.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-964 execpodfmh2p -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 12 22:48:03.666: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 12 22:48:03.666: INFO: stdout: ""
Dec 12 22:48:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-964 execpodfmh2p -- /bin/sh -x -c nc -zv -t -w 2 10.96.77.96 80'
Dec 12 22:48:03.827: INFO: stderr: "+ nc -zv -t -w 2 10.96.77.96 80\nConnection to 10.96.77.96 80 port [tcp/http] succeeded!\n"
Dec 12 22:48:03.827: INFO: stdout: ""
Dec 12 22:48:03.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-964 execpodfmh2p -- /bin/sh -x -c nc -zv -t -w 2 10.128.15.218 30269'
Dec 12 22:48:03.998: INFO: stderr: "+ nc -zv -t -w 2 10.128.15.218 30269\nConnection to 10.128.15.218 30269 port [tcp/30269] succeeded!\n"
Dec 12 22:48:03.998: INFO: stdout: ""
Dec 12 22:48:03.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-964 execpodfmh2p -- /bin/sh -x -c nc -zv -t -w 2 10.128.15.231 30269'
Dec 12 22:48:04.167: INFO: stderr: "+ nc -zv -t -w 2 10.128.15.231 30269\nConnection to 10.128.15.231 30269 port [tcp/30269] succeeded!\n"
Dec 12 22:48:04.167: INFO: stdout: ""
Dec 12 22:48:04.167: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:48:04.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-964" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.188 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":228,"skipped":3747,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:48:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 12 22:48:04.512: INFO: Number of nodes with available pods: 0
Dec 12 22:48:04.512: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:48:05.522: INFO: Number of nodes with available pods: 0
Dec 12 22:48:05.522: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:48:06.523: INFO: Number of nodes with available pods: 0
Dec 12 22:48:06.523: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-0 is running more than one daemon pod
Dec 12 22:48:07.523: INFO: Number of nodes with available pods: 6
Dec 12 22:48:07.523: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 12 22:48:07.559: INFO: Number of nodes with available pods: 5
Dec 12 22:48:07.559: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:08.568: INFO: Number of nodes with available pods: 5
Dec 12 22:48:08.568: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:09.568: INFO: Number of nodes with available pods: 5
Dec 12 22:48:09.568: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:10.570: INFO: Number of nodes with available pods: 5
Dec 12 22:48:10.570: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:11.567: INFO: Number of nodes with available pods: 5
Dec 12 22:48:11.567: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:12.570: INFO: Number of nodes with available pods: 5
Dec 12 22:48:12.570: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:13.568: INFO: Number of nodes with available pods: 5
Dec 12 22:48:13.568: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-2 is running more than one daemon pod
Dec 12 22:48:14.570: INFO: Number of nodes with available pods: 6
Dec 12 22:48:14.570: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6968, will wait for the garbage collector to delete the pods
Dec 12 22:48:14.637: INFO: Deleting DaemonSet.extensions daemon-set took: 10.127873ms
Dec 12 22:48:15.037: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.248034ms
Dec 12 22:48:26.543: INFO: Number of nodes with available pods: 0
Dec 12 22:48:26.543: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 22:48:26.549: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6968/daemonsets","resourceVersion":"26586"},"items":null}

Dec 12 22:48:26.553: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6968/pods","resourceVersion":"26586"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:48:26.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6968" for this suite.

• [SLOW TEST:22.340 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":229,"skipped":3751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:48:26.599: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:48:31.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8517" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":230,"skipped":3774,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:48:31.221: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 12 22:48:31.397: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:48:46.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4663" for this suite.

• [SLOW TEST:15.291 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":231,"skipped":3777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:48:46.513: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:48:47.471: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 12 22:48:49.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787727, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787727, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787727, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787727, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:48:52.506: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:48:52.511: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:48:53.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6721" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.582 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":232,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:48:54.096: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-385
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-6f387ed2-e946-473e-93bc-19c016386613
STEP: Creating secret with name s-test-opt-upd-33bb39c7-cd87-4f7e-a954-eb64a95439f9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6f387ed2-e946-473e-93bc-19c016386613
STEP: Updating secret s-test-opt-upd-33bb39c7-cd87-4f7e-a954-eb64a95439f9
STEP: Creating secret with name s-test-opt-create-cfe75c50-eacd-413d-af0f-c62904d864fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:00.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-385" for this suite.

• [SLOW TEST:6.338 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:00.532: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:04.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4712" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":3892,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:04.779: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 12 22:49:04.957: INFO: Waiting up to 5m0s for pod "pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e" in namespace "emptydir-6169" to be "success or failure"
Dec 12 22:49:04.963: INFO: Pod "pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.381836ms
Dec 12 22:49:06.967: INFO: Pod "pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009928836s
Dec 12 22:49:08.971: INFO: Pod "pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014288151s
STEP: Saw pod success
Dec 12 22:49:08.972: INFO: Pod "pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e" satisfied condition "success or failure"
Dec 12 22:49:08.975: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-4kj7s pod pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e container test-container: <nil>
STEP: delete the pod
Dec 12 22:49:09.026: INFO: Waiting for pod pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e to disappear
Dec 12 22:49:09.030: INFO: Pod pod-8bd8e194-777b-468a-9eb8-b3fd94abe13e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6169" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":235,"skipped":3901,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:09.051: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:49:09.986: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 22:49:12.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787749, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787749, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787750, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787749, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:49:15.070: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:49:15.074: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3247-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:16.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8642" for this suite.
STEP: Destroying namespace "webhook-8642-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":236,"skipped":3911,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:16.566: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Dec 12 22:49:21.315: INFO: Successfully updated pod "annotationupdate4a9edf10-1418-4e02-ad7c-660d9c040c2f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:23.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4273" for this suite.

• [SLOW TEST:6.781 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":237,"skipped":3930,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:23.347: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-acda9667-ea62-4bc2-9e46-56cbc419057c
STEP: Creating a pod to test consume secrets
Dec 12 22:49:23.609: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e" in namespace "projected-5534" to be "success or failure"
Dec 12 22:49:23.613: INFO: Pod "pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545774ms
Dec 12 22:49:25.618: INFO: Pod "pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009508049s
STEP: Saw pod success
Dec 12 22:49:25.618: INFO: Pod "pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e" satisfied condition "success or failure"
Dec 12 22:49:25.622: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:49:25.648: INFO: Waiting for pod pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e to disappear
Dec 12 22:49:25.651: INFO: Pod pod-projected-secrets-e11f8ed6-0962-43b0-a260-bd0ebe33b95e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:25.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5534" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":238,"skipped":3942,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:25.671: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-d3660df3-634e-4d83-981d-915c8c4a5b59
STEP: Creating secret with name secret-projected-all-test-volume-aa25913b-144b-4b2a-a027-0c0fddb9ff2f
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 12 22:49:25.884: INFO: Waiting up to 5m0s for pod "projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9" in namespace "projected-678" to be "success or failure"
Dec 12 22:49:25.895: INFO: Pod "projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.412911ms
Dec 12 22:49:27.900: INFO: Pod "projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01570399s
Dec 12 22:49:29.905: INFO: Pod "projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020881051s
STEP: Saw pod success
Dec 12 22:49:29.905: INFO: Pod "projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9" satisfied condition "success or failure"
Dec 12 22:49:29.909: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 12 22:49:29.931: INFO: Waiting for pod projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9 to disappear
Dec 12 22:49:29.936: INFO: Pod projected-volume-f573fa15-89df-49fc-94f4-8a20f51081b9 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:29.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-678" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 12 22:49:30.147: INFO: Waiting up to 5m0s for pod "pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd" in namespace "emptydir-1803" to be "success or failure"
Dec 12 22:49:30.151: INFO: Pod "pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447363ms
Dec 12 22:49:32.156: INFO: Pod "pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009303894s
Dec 12 22:49:34.162: INFO: Pod "pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014823905s
STEP: Saw pod success
Dec 12 22:49:34.162: INFO: Pod "pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd" satisfied condition "success or failure"
Dec 12 22:49:34.166: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd container test-container: <nil>
STEP: delete the pod
Dec 12 22:49:34.189: INFO: Waiting for pod pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd to disappear
Dec 12 22:49:34.193: INFO: Pod pod-d5eca460-a523-4440-a9e1-9b9ba3674dfd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:34.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1803" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3983,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:34.210: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:49:34.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 22:49:36.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787774, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787774, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787774, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787774, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:49:39.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:40.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4815" for this suite.
STEP: Destroying namespace "webhook-4815-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.929 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":241,"skipped":3984,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:40.143: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-7893a032-ee1f-4d5e-a6d2-78c7b0d990fd
STEP: Creating a pod to test consume secrets
Dec 12 22:49:40.345: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4" in namespace "projected-9254" to be "success or failure"
Dec 12 22:49:40.355: INFO: Pod "pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.715542ms
Dec 12 22:49:42.360: INFO: Pod "pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014723033s
Dec 12 22:49:44.368: INFO: Pod "pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022816339s
STEP: Saw pod success
Dec 12 22:49:44.368: INFO: Pod "pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4" satisfied condition "success or failure"
Dec 12 22:49:44.372: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:49:44.397: INFO: Waiting for pod pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4 to disappear
Dec 12 22:49:44.402: INFO: Pod pod-projected-secrets-e5fbbce0-b4aa-4311-a7d8-adb148d4fdf4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:44.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9254" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":4002,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:44.417: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Dec 12 22:49:44.598: INFO: Waiting up to 5m0s for pod "var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3" in namespace "var-expansion-802" to be "success or failure"
Dec 12 22:49:44.602: INFO: Pod "var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713142ms
Dec 12 22:49:46.606: INFO: Pod "var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008238656s
Dec 12 22:49:48.611: INFO: Pod "var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012750128s
STEP: Saw pod success
Dec 12 22:49:48.611: INFO: Pod "var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3" satisfied condition "success or failure"
Dec 12 22:49:48.614: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3 container dapi-container: <nil>
STEP: delete the pod
Dec 12 22:49:48.639: INFO: Waiting for pod var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3 to disappear
Dec 12 22:49:48.648: INFO: Pod var-expansion-f92991c5-e320-4da0-b077-b1bfb721faf3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:48.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-802" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":4015,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:48.667: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:49:48.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70" in namespace "projected-5521" to be "success or failure"
Dec 12 22:49:48.852: INFO: Pod "downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606315ms
Dec 12 22:49:50.856: INFO: Pod "downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00766441s
Dec 12 22:49:52.861: INFO: Pod "downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012797423s
STEP: Saw pod success
Dec 12 22:49:52.861: INFO: Pod "downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70" satisfied condition "success or failure"
Dec 12 22:49:52.866: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70 container client-container: <nil>
STEP: delete the pod
Dec 12 22:49:52.908: INFO: Waiting for pod downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70 to disappear
Dec 12 22:49:52.914: INFO: Pod downwardapi-volume-3275bb8a-9bfb-4367-858d-31c7dcc8ba70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:52.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5521" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":244,"skipped":4021,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:52.928: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-61d1afbc-866d-4f9f-ab67-1c9fcecded23
STEP: Creating a pod to test consume secrets
Dec 12 22:49:53.108: INFO: Waiting up to 5m0s for pod "pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506" in namespace "secrets-4923" to be "success or failure"
Dec 12 22:49:53.111: INFO: Pod "pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361053ms
Dec 12 22:49:55.115: INFO: Pod "pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007568144s
Dec 12 22:49:57.121: INFO: Pod "pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013248171s
STEP: Saw pod success
Dec 12 22:49:57.121: INFO: Pod "pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506" satisfied condition "success or failure"
Dec 12 22:49:57.124: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:49:57.146: INFO: Waiting for pod pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506 to disappear
Dec 12 22:49:57.151: INFO: Pod pod-secrets-dd9a5167-fb2c-42a3-817e-8f172ca24506 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:49:57.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4923" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":4025,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:49:57.163: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1212 22:50:37.414723      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 22:50:37.414: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:50:37.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4993" for this suite.

• [SLOW TEST:40.267 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":246,"skipped":4040,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:50:37.430: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9636
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[BeforeEach] Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:329
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Dec 12 22:50:37.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-9636'
Dec 12 22:50:38.485: INFO: stderr: ""
Dec 12 22:50:38.485: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 22:50:38.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9636'
Dec 12 22:50:38.573: INFO: stderr: ""
Dec 12 22:50:38.573: INFO: stdout: "update-demo-nautilus-5hrvv update-demo-nautilus-dz5tx "
Dec 12 22:50:38.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:38.661: INFO: stderr: ""
Dec 12 22:50:38.661: INFO: stdout: ""
Dec 12 22:50:38.661: INFO: update-demo-nautilus-5hrvv is created but not running
Dec 12 22:50:43.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9636'
Dec 12 22:50:43.760: INFO: stderr: ""
Dec 12 22:50:43.760: INFO: stdout: "update-demo-nautilus-5hrvv update-demo-nautilus-dz5tx "
Dec 12 22:50:43.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:43.848: INFO: stderr: ""
Dec 12 22:50:43.848: INFO: stdout: "true"
Dec 12 22:50:43.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:43.940: INFO: stderr: ""
Dec 12 22:50:43.940: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:50:43.940: INFO: validating pod update-demo-nautilus-5hrvv
Dec 12 22:50:43.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:50:43.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:50:43.952: INFO: update-demo-nautilus-5hrvv is verified up and running
Dec 12 22:50:43.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-dz5tx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:44.038: INFO: stderr: ""
Dec 12 22:50:44.038: INFO: stdout: "true"
Dec 12 22:50:44.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-dz5tx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:44.128: INFO: stderr: ""
Dec 12 22:50:44.128: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:50:44.129: INFO: validating pod update-demo-nautilus-dz5tx
Dec 12 22:50:44.137: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:50:44.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:50:44.137: INFO: update-demo-nautilus-dz5tx is verified up and running
STEP: scaling down the replication controller
Dec 12 22:50:44.140: INFO: scanned /root for discovery docs: <nil>
Dec 12 22:50:44.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9636'
Dec 12 22:50:45.258: INFO: stderr: ""
Dec 12 22:50:45.258: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 22:50:45.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9636'
Dec 12 22:50:45.352: INFO: stderr: ""
Dec 12 22:50:45.352: INFO: stdout: "update-demo-nautilus-5hrvv update-demo-nautilus-dz5tx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 12 22:50:50.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9636'
Dec 12 22:50:50.441: INFO: stderr: ""
Dec 12 22:50:50.441: INFO: stdout: "update-demo-nautilus-5hrvv "
Dec 12 22:50:50.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:50.540: INFO: stderr: ""
Dec 12 22:50:50.540: INFO: stdout: "true"
Dec 12 22:50:50.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:50.630: INFO: stderr: ""
Dec 12 22:50:50.630: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:50:50.630: INFO: validating pod update-demo-nautilus-5hrvv
Dec 12 22:50:50.637: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:50:50.637: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:50:50.637: INFO: update-demo-nautilus-5hrvv is verified up and running
STEP: scaling up the replication controller
Dec 12 22:50:50.641: INFO: scanned /root for discovery docs: <nil>
Dec 12 22:50:50.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9636'
Dec 12 22:50:51.772: INFO: stderr: ""
Dec 12 22:50:51.772: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 22:50:51.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9636'
Dec 12 22:50:51.863: INFO: stderr: ""
Dec 12 22:50:51.863: INFO: stdout: "update-demo-nautilus-5hrvv update-demo-nautilus-7l8kw "
Dec 12 22:50:51.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:51.951: INFO: stderr: ""
Dec 12 22:50:51.951: INFO: stdout: "true"
Dec 12 22:50:51.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:52.046: INFO: stderr: ""
Dec 12 22:50:52.046: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:50:52.046: INFO: validating pod update-demo-nautilus-5hrvv
Dec 12 22:50:52.054: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:50:52.054: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:50:52.054: INFO: update-demo-nautilus-5hrvv is verified up and running
Dec 12 22:50:52.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-7l8kw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:52.149: INFO: stderr: ""
Dec 12 22:50:52.149: INFO: stdout: ""
Dec 12 22:50:52.149: INFO: update-demo-nautilus-7l8kw is created but not running
Dec 12 22:50:57.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9636'
Dec 12 22:50:57.246: INFO: stderr: ""
Dec 12 22:50:57.246: INFO: stdout: "update-demo-nautilus-5hrvv update-demo-nautilus-7l8kw "
Dec 12 22:50:57.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:57.340: INFO: stderr: ""
Dec 12 22:50:57.340: INFO: stdout: "true"
Dec 12 22:50:57.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-5hrvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:57.431: INFO: stderr: ""
Dec 12 22:50:57.431: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:50:57.431: INFO: validating pod update-demo-nautilus-5hrvv
Dec 12 22:50:57.441: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:50:57.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:50:57.441: INFO: update-demo-nautilus-5hrvv is verified up and running
Dec 12 22:50:57.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-7l8kw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:57.533: INFO: stderr: ""
Dec 12 22:50:57.533: INFO: stdout: "true"
Dec 12 22:50:57.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods update-demo-nautilus-7l8kw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9636'
Dec 12 22:50:57.618: INFO: stderr: ""
Dec 12 22:50:57.619: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 22:50:57.619: INFO: validating pod update-demo-nautilus-7l8kw
Dec 12 22:50:57.629: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 22:50:57.629: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 22:50:57.629: INFO: update-demo-nautilus-7l8kw is verified up and running
STEP: using delete to clean up resources
Dec 12 22:50:57.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 delete --grace-period=0 --force -f - --namespace=kubectl-9636'
Dec 12 22:50:57.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 22:50:57.729: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 12 22:50:57.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9636'
Dec 12 22:50:57.827: INFO: stderr: "No resources found in kubectl-9636 namespace.\n"
Dec 12 22:50:57.828: INFO: stdout: ""
Dec 12 22:50:57.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -l name=update-demo --namespace=kubectl-9636 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 22:50:57.916: INFO: stderr: ""
Dec 12 22:50:57.916: INFO: stdout: "update-demo-nautilus-5hrvv\nupdate-demo-nautilus-7l8kw\n"
Dec 12 22:50:58.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9636'
Dec 12 22:50:58.511: INFO: stderr: "No resources found in kubectl-9636 namespace.\n"
Dec 12 22:50:58.511: INFO: stdout: ""
Dec 12 22:50:58.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 get pods -l name=update-demo --namespace=kubectl-9636 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 22:50:58.605: INFO: stderr: ""
Dec 12 22:50:58.605: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:50:58.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9636" for this suite.

• [SLOW TEST:21.189 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:327
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":247,"skipped":4055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:50:58.623: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Dec 12 22:50:58.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 cluster-info'
Dec 12 22:50:59.008: INFO: stderr: ""
Dec 12 22:50:59.008: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:50:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1267" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":248,"skipped":4094,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:50:59.023: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Dec 12 22:50:59.220: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 12 22:50:59.238: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 22:50:59.244: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-0 before test
Dec 12 22:50:59.266: INFO: kube-proxy-bkbtw from kube-system started at 2019-12-12 21:35:43 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.267: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:50:59.267: INFO: kube-apiserver-9sv5v from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.267: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 12 22:50:59.267: INFO: pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.267: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:50:59.267: INFO: kube-flannel-69ftd from kube-system started at 2019-12-12 21:35:43 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.267: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:50:59.267: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:50:59.267: INFO: pod-checkpointer-wbndz from kube-system started at 2019-12-12 21:36:03 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.267: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:50:59.267: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-1 before test
Dec 12 22:50:59.292: INFO: pod-checkpointer-8ftvl from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.292: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:50:59.292: INFO: coredns-5bcc66f5bd-6pzdf from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.292: INFO: 	Container coredns ready: true, restart count 0
Dec 12 22:50:59.292: INFO: kube-scheduler-6db5bfb7bc-tx7bk from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.292: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 12 22:50:59.292: INFO: kube-flannel-cq4nw from kube-system started at 2019-12-12 21:35:36 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.292: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:50:59.292: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:50:59.292: INFO: kube-scheduler-6db5bfb7bc-dplt8 from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.293: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 12 22:50:59.293: INFO: kube-controller-manager-5c547b548-cqqln from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.293: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 12 22:50:59.293: INFO: kube-apiserver-j24rk from kube-system started at 2019-12-12 21:35:56 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.293: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:50:59.293: INFO: kube-proxy-mb48g from kube-system started at 2019-12-12 21:35:36 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.293: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:50:59.293: INFO: kube-controller-manager-5c547b548-hxgrq from kube-system started at 2019-12-12 21:35:57 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.293: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 12 22:50:59.293: INFO: pod-checkpointer-8ftvl-talos-0-3-0-beta-0-gcp-controlplane-1 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.293: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:50:59.293: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-controlplane-2 before test
Dec 12 22:50:59.316: INFO: kube-proxy-9wbfd from kube-system started at 2019-12-12 21:35:29 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.316: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:50:59.316: INFO: kube-flannel-9hznp from kube-system started at 2019-12-12 21:35:29 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.316: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:50:59.316: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:50:59.316: INFO: kube-apiserver-kf44j from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.316: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 12 22:50:59.316: INFO: pod-checkpointer-5mnr9 from kube-system started at 2019-12-12 21:35:58 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.316: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:50:59.316: INFO: pod-checkpointer-5mnr9-talos-0-3-0-beta-0-gcp-controlplane-2 from kube-system started at 2019-12-12 21:36:12 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.316: INFO: 	Container pod-checkpointer ready: true, restart count 0
Dec 12 22:50:59.316: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-4kj7s before test
Dec 12 22:50:59.335: INFO: sonobuoy-e2e-job-01c7b380be144838 from sonobuoy started at 2019-12-12 21:39:37 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.335: INFO: 	Container e2e ready: true, restart count 0
Dec 12 22:50:59.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 12 22:50:59.336: INFO: kube-flannel-bz9ht from kube-system started at 2019-12-12 21:36:55 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.338: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:50:59.338: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:50:59.339: INFO: kube-proxy-xh54v from kube-system started at 2019-12-12 21:36:55 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.339: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:50:59.339: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-nt7qc before test
Dec 12 22:50:59.347: INFO: kube-proxy-jtlmf from kube-system started at 2019-12-12 21:37:10 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.347: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:50:59.347: INFO: update-demo-nautilus-5hrvv from kubectl-9636 started at 2019-12-12 22:50:38 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.347: INFO: 	Container update-demo ready: false, restart count 0
Dec 12 22:50:59.347: INFO: kube-flannel-p79j9 from kube-system started at 2019-12-12 21:37:10 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.347: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:50:59.347: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:50:59.347: INFO: 
Logging pods the kubelet thinks is on node talos-0-3-0-beta-0-gcp-workers-ntvmn before test
Dec 12 22:50:59.369: INFO: update-demo-nautilus-7l8kw from kubectl-9636 started at 2019-12-12 22:50:50 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.369: INFO: 	Container update-demo ready: false, restart count 0
Dec 12 22:50:59.369: INFO: kube-proxy-bvldz from kube-system started at 2019-12-12 21:37:28 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.369: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 12 22:50:59.369: INFO: kube-flannel-gk4cx from kube-system started at 2019-12-12 21:37:28 +0000 UTC (2 container statuses recorded)
Dec 12 22:50:59.369: INFO: 	Container install-cni ready: true, restart count 0
Dec 12 22:50:59.369: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 12 22:50:59.369: INFO: sonobuoy from sonobuoy started at 2019-12-12 21:39:34 +0000 UTC (1 container statuses recorded)
Dec 12 22:50:59.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node talos-0-3-0-beta-0-gcp-controlplane-0
STEP: verifying the node has the label node talos-0-3-0-beta-0-gcp-controlplane-1
STEP: verifying the node has the label node talos-0-3-0-beta-0-gcp-controlplane-2
STEP: verifying the node has the label node talos-0-3-0-beta-0-gcp-workers-4kj7s
STEP: verifying the node has the label node talos-0-3-0-beta-0-gcp-workers-nt7qc
STEP: verifying the node has the label node talos-0-3-0-beta-0-gcp-workers-ntvmn
Dec 12 22:50:59.522: INFO: Pod coredns-5bcc66f5bd-6pzdf requesting resource cpu=100m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-apiserver-9sv5v requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-0
Dec 12 22:50:59.522: INFO: Pod kube-apiserver-j24rk requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-apiserver-kf44j requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-2
Dec 12 22:50:59.522: INFO: Pod kube-controller-manager-5c547b548-cqqln requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-controller-manager-5c547b548-hxgrq requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-flannel-69ftd requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-0
Dec 12 22:50:59.522: INFO: Pod kube-flannel-9hznp requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-2
Dec 12 22:50:59.522: INFO: Pod kube-flannel-bz9ht requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-4kj7s
Dec 12 22:50:59.522: INFO: Pod kube-flannel-cq4nw requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-flannel-gk4cx requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-ntvmn
Dec 12 22:50:59.522: INFO: Pod kube-flannel-p79j9 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-nt7qc
Dec 12 22:50:59.522: INFO: Pod kube-proxy-9wbfd requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-2
Dec 12 22:50:59.522: INFO: Pod kube-proxy-bkbtw requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-0
Dec 12 22:50:59.522: INFO: Pod kube-proxy-bvldz requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-ntvmn
Dec 12 22:50:59.522: INFO: Pod kube-proxy-jtlmf requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-nt7qc
Dec 12 22:50:59.522: INFO: Pod kube-proxy-mb48g requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-proxy-xh54v requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-4kj7s
Dec 12 22:50:59.522: INFO: Pod kube-scheduler-6db5bfb7bc-dplt8 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod kube-scheduler-6db5bfb7bc-tx7bk requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod pod-checkpointer-5mnr9 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-2
Dec 12 22:50:59.522: INFO: Pod pod-checkpointer-5mnr9-talos-0-3-0-beta-0-gcp-controlplane-2 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-2
Dec 12 22:50:59.522: INFO: Pod pod-checkpointer-8ftvl requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod pod-checkpointer-8ftvl-talos-0-3-0-beta-0-gcp-controlplane-1 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-1
Dec 12 22:50:59.522: INFO: Pod pod-checkpointer-wbndz requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-0
Dec 12 22:50:59.522: INFO: Pod pod-checkpointer-wbndz-talos-0-3-0-beta-0-gcp-controlplane-0 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-controlplane-0
Dec 12 22:50:59.522: INFO: Pod update-demo-nautilus-5hrvv requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-nt7qc
Dec 12 22:50:59.522: INFO: Pod update-demo-nautilus-7l8kw requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-ntvmn
Dec 12 22:50:59.522: INFO: Pod sonobuoy requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-ntvmn
Dec 12 22:50:59.522: INFO: Pod sonobuoy-e2e-job-01c7b380be144838 requesting resource cpu=0m on Node talos-0-3-0-beta-0-gcp-workers-4kj7s
STEP: Starting Pods to consume most of the cluster CPU.
Dec 12 22:50:59.523: INFO: Creating a pod which consumes cpu=1400m on Node talos-0-3-0-beta-0-gcp-controlplane-2
Dec 12 22:50:59.535: INFO: Creating a pod which consumes cpu=1400m on Node talos-0-3-0-beta-0-gcp-workers-4kj7s
Dec 12 22:50:59.564: INFO: Creating a pod which consumes cpu=1400m on Node talos-0-3-0-beta-0-gcp-workers-nt7qc
Dec 12 22:50:59.585: INFO: Creating a pod which consumes cpu=1400m on Node talos-0-3-0-beta-0-gcp-workers-ntvmn
Dec 12 22:50:59.646: INFO: Creating a pod which consumes cpu=1400m on Node talos-0-3-0-beta-0-gcp-controlplane-0
Dec 12 22:50:59.662: INFO: Creating a pod which consumes cpu=1330m on Node talos-0-3-0-beta-0-gcp-controlplane-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a.15dfc16af35ff0d7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1888/filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a to talos-0-3-0-beta-0-gcp-controlplane-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a.15dfc16b7e45996f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a.15dfc16b836a622d], Reason = [Created], Message = [Created container filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a.15dfc16b891a7e0d], Reason = [Started], Message = [Started container filler-pod-08d97bd0-e244-4ada-bcfa-b2da5eeac88a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2.15dfc16af24cbf51], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1888/filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2 to talos-0-3-0-beta-0-gcp-controlplane-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2.15dfc16b5deaeb4c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2.15dfc16b68bb9229], Reason = [Created], Message = [Created container filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2.15dfc16b6eb75ba0], Reason = [Started], Message = [Started container filler-pod-2099318a-eb41-4f27-aee0-77c8d99cd6d2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af.15dfc16aef8d648a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1888/filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af to talos-0-3-0-beta-0-gcp-workers-ntvmn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af.15dfc16b9c44c2d1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af.15dfc16ba21395f8], Reason = [Created], Message = [Created container filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af.15dfc16ba7f1c728], Reason = [Started], Message = [Started container filler-pod-3242c275-7f06-46bd-8150-b2ca72f185af]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6.15dfc16aea5296f2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1888/filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6 to talos-0-3-0-beta-0-gcp-controlplane-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6.15dfc16b7cdd4c44], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6.15dfc16b81c9aeb8], Reason = [Created], Message = [Created container filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6.15dfc16b87a705ce], Reason = [Started], Message = [Started container filler-pod-6d552645-f993-410f-a5ab-bf676c9907b6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93.15dfc16aeb66fe26], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1888/filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93 to talos-0-3-0-beta-0-gcp-workers-4kj7s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93.15dfc16b62f3377f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93.15dfc16b65a9fd60], Reason = [Created], Message = [Created container filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93.15dfc16b6a2dda24], Reason = [Started], Message = [Started container filler-pod-7672ddc8-a38b-4fe9-ac5b-01e63cd0da93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453.15dfc16aec6bf190], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1888/filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453 to talos-0-3-0-beta-0-gcp-workers-nt7qc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453.15dfc16b5a427a3c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453.15dfc16b5e0c9e5b], Reason = [Created], Message = [Created container filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453.15dfc16b62607b14], Reason = [Started], Message = [Started container filler-pod-7ddea658-aa8f-4191-af4a-71fad74bb453]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dfc16be42942a9], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: removing the label node off the node talos-0-3-0-beta-0-gcp-workers-nt7qc
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-0-3-0-beta-0-gcp-workers-ntvmn
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-0-3-0-beta-0-gcp-controlplane-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-0-3-0-beta-0-gcp-controlplane-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-0-3-0-beta-0-gcp-controlplane-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-0-3-0-beta-0-gcp-workers-4kj7s
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:04.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1888" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:5.902 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":249,"skipped":4112,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1212 22:51:15.272193      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 22:51:15.272: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:15.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2899" for this suite.

• [SLOW TEST:10.356 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":250,"skipped":4124,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:15.284: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4166
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:51:15.450: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:16.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4166" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":251,"skipped":4141,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:16.832: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-ba5857df-23f8-4399-a7d3-1ebb24b54d4a
STEP: Creating a pod to test consume secrets
Dec 12 22:51:17.069: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735" in namespace "projected-7842" to be "success or failure"
Dec 12 22:51:17.074: INFO: Pod "pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735": Phase="Pending", Reason="", readiness=false. Elapsed: 4.967091ms
Dec 12 22:51:19.080: INFO: Pod "pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010044596s
Dec 12 22:51:21.086: INFO: Pod "pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016727616s
STEP: Saw pod success
Dec 12 22:51:21.086: INFO: Pod "pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735" satisfied condition "success or failure"
Dec 12 22:51:21.095: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 22:51:21.150: INFO: Waiting for pod pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735 to disappear
Dec 12 22:51:21.154: INFO: Pod pod-projected-secrets-521a2ab6-d34f-43b2-b019-5796b6a16735 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:21.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7842" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":4157,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:51:21.435: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 12 22:51:21.445: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 12 22:51:26.450: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 12 22:51:26.450: INFO: Creating deployment "test-rolling-update-deployment"
Dec 12 22:51:26.457: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 12 22:51:26.466: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 12 22:51:28.479: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 12 22:51:28.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787886, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787886, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787886, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787886, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 22:51:30.488: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Dec 12 22:51:30.501: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6999 /apis/apps/v1/namespaces/deployment-6999/deployments/test-rolling-update-deployment dd4f5ad0-d62f-4a94-b3d5-e3a7958dd344 28610 1 2019-12-12 22:51:26 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035a0d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-12 22:51:26 +0000 UTC,LastTransitionTime:2019-12-12 22:51:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2019-12-12 22:51:29 +0000 UTC,LastTransitionTime:2019-12-12 22:51:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 12 22:51:30.505: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-6999 /apis/apps/v1/namespaces/deployment-6999/replicasets/test-rolling-update-deployment-67cf4f6444 5cb5ae44-640c-4640-abe5-dafe9c079bb1 28599 1 2019-12-12 22:51:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment dd4f5ad0-d62f-4a94-b3d5-e3a7958dd344 0xc0035a11f7 0xc0035a11f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035a1268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:51:30.505: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 12 22:51:30.505: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6999 /apis/apps/v1/namespaces/deployment-6999/replicasets/test-rolling-update-controller 6d638191-af3b-4615-8450-eb9ad594ad2a 28608 2 2019-12-12 22:51:21 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment dd4f5ad0-d62f-4a94-b3d5-e3a7958dd344 0xc0035a1127 0xc0035a1128}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035a1188 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 22:51:30.509: INFO: Pod "test-rolling-update-deployment-67cf4f6444-sb6pg" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-sb6pg test-rolling-update-deployment-67cf4f6444- deployment-6999 /api/v1/namespaces/deployment-6999/pods/test-rolling-update-deployment-67cf4f6444-sb6pg 0faa3b65-945a-4e09-a8ff-f735af9a9b31 28598 0 2019-12-12 22:51:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 5cb5ae44-640c-4640-abe5-dafe9c079bb1 0xc0035a16e7 0xc0035a16e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-czgn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-czgn8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-czgn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-0-3-0-beta-0-gcp-workers-ntvmn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:51:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 22:51:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.15.231,PodIP:10.244.5.99,StartTime:2019-12-12 22:51:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 22:51:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://b48dffb4ca7bfb57ad8017b31e48fa67e7a7285989aa0244182780b79017ceb5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:30.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6999" for this suite.

• [SLOW TEST:9.344 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":253,"skipped":4166,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:30.525: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6323
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 12 22:51:30.713: INFO: Waiting up to 5m0s for pod "pod-f147a30e-509f-490e-85d3-312adaaf4d84" in namespace "emptydir-6323" to be "success or failure"
Dec 12 22:51:30.755: INFO: Pod "pod-f147a30e-509f-490e-85d3-312adaaf4d84": Phase="Pending", Reason="", readiness=false. Elapsed: 41.49079ms
Dec 12 22:51:32.760: INFO: Pod "pod-f147a30e-509f-490e-85d3-312adaaf4d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046436765s
Dec 12 22:51:34.766: INFO: Pod "pod-f147a30e-509f-490e-85d3-312adaaf4d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052482206s
STEP: Saw pod success
Dec 12 22:51:34.766: INFO: Pod "pod-f147a30e-509f-490e-85d3-312adaaf4d84" satisfied condition "success or failure"
Dec 12 22:51:34.770: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-f147a30e-509f-490e-85d3-312adaaf4d84 container test-container: <nil>
STEP: delete the pod
Dec 12 22:51:34.810: INFO: Waiting for pod pod-f147a30e-509f-490e-85d3-312adaaf4d84 to disappear
Dec 12 22:51:34.814: INFO: Pod pod-f147a30e-509f-490e-85d3-312adaaf4d84 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:34.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6323" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":4174,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:34.840: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:51:35.929: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 22:51:37.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787895, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787895, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787896, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787895, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:51:40.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 12 22:51:40.998: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2537" for this suite.
STEP: Destroying namespace "webhook-2537-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.266 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":255,"skipped":4188,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:41.108: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:51:41.306: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8942
I1212 22:51:41.322325      22 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8942, replica count: 1
I1212 22:51:42.372942      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:51:43.373302      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:51:44.373564      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 22:51:44.488: INFO: Created: latency-svc-bk5cg
Dec 12 22:51:44.498: INFO: Got endpoints: latency-svc-bk5cg [25.011051ms]
Dec 12 22:51:44.525: INFO: Created: latency-svc-v7nnx
Dec 12 22:51:44.532: INFO: Got endpoints: latency-svc-v7nnx [33.400232ms]
Dec 12 22:51:44.537: INFO: Created: latency-svc-85g6s
Dec 12 22:51:44.590: INFO: Got endpoints: latency-svc-85g6s [90.549607ms]
Dec 12 22:51:44.596: INFO: Created: latency-svc-dn87c
Dec 12 22:51:44.604: INFO: Created: latency-svc-5p4vb
Dec 12 22:51:44.617: INFO: Created: latency-svc-cb5c7
Dec 12 22:51:44.638: INFO: Got endpoints: latency-svc-dn87c [138.303934ms]
Dec 12 22:51:44.640: INFO: Got endpoints: latency-svc-cb5c7 [140.141419ms]
Dec 12 22:51:44.641: INFO: Got endpoints: latency-svc-5p4vb [140.584124ms]
Dec 12 22:51:44.651: INFO: Created: latency-svc-q9rrz
Dec 12 22:51:44.654: INFO: Got endpoints: latency-svc-q9rrz [154.474286ms]
Dec 12 22:51:44.674: INFO: Created: latency-svc-rwtfq
Dec 12 22:51:44.681: INFO: Got endpoints: latency-svc-rwtfq [180.754389ms]
Dec 12 22:51:44.691: INFO: Created: latency-svc-4jm5z
Dec 12 22:51:44.726: INFO: Got endpoints: latency-svc-4jm5z [226.213674ms]
Dec 12 22:51:44.777: INFO: Created: latency-svc-857b6
Dec 12 22:51:44.788: INFO: Created: latency-svc-t4rmv
Dec 12 22:51:44.789: INFO: Got endpoints: latency-svc-857b6 [288.889745ms]
Dec 12 22:51:44.799: INFO: Got endpoints: latency-svc-t4rmv [299.374935ms]
Dec 12 22:51:44.802: INFO: Created: latency-svc-gh8qx
Dec 12 22:51:44.814: INFO: Got endpoints: latency-svc-gh8qx [313.823563ms]
Dec 12 22:51:44.820: INFO: Created: latency-svc-nl26p
Dec 12 22:51:44.845: INFO: Got endpoints: latency-svc-nl26p [345.31951ms]
Dec 12 22:51:44.855: INFO: Created: latency-svc-6k6tn
Dec 12 22:51:44.860: INFO: Got endpoints: latency-svc-6k6tn [359.956411ms]
Dec 12 22:51:44.905: INFO: Created: latency-svc-hjzvl
Dec 12 22:51:44.919: INFO: Got endpoints: latency-svc-hjzvl [418.754796ms]
Dec 12 22:51:44.922: INFO: Created: latency-svc-m8p6b
Dec 12 22:51:44.943: INFO: Created: latency-svc-lhtgl
Dec 12 22:51:44.944: INFO: Got endpoints: latency-svc-m8p6b [443.528899ms]
Dec 12 22:51:44.983: INFO: Got endpoints: latency-svc-lhtgl [450.569977ms]
Dec 12 22:51:44.983: INFO: Created: latency-svc-t5kff
Dec 12 22:51:44.991: INFO: Got endpoints: latency-svc-t5kff [401.900243ms]
Dec 12 22:51:44.995: INFO: Created: latency-svc-gzrxl
Dec 12 22:51:45.006: INFO: Got endpoints: latency-svc-gzrxl [367.024217ms]
Dec 12 22:51:45.007: INFO: Created: latency-svc-zncq2
Dec 12 22:51:45.018: INFO: Got endpoints: latency-svc-zncq2 [377.680126ms]
Dec 12 22:51:45.022: INFO: Created: latency-svc-r5xzb
Dec 12 22:51:45.036: INFO: Created: latency-svc-ljzms
Dec 12 22:51:45.038: INFO: Got endpoints: latency-svc-r5xzb [395.918907ms]
Dec 12 22:51:45.045: INFO: Created: latency-svc-qvt5l
Dec 12 22:51:45.049: INFO: Got endpoints: latency-svc-ljzms [395.197344ms]
Dec 12 22:51:45.062: INFO: Got endpoints: latency-svc-qvt5l [380.160568ms]
Dec 12 22:51:45.068: INFO: Created: latency-svc-969sk
Dec 12 22:51:45.108: INFO: Created: latency-svc-cmq78
Dec 12 22:51:45.122: INFO: Created: latency-svc-9jw2n
Dec 12 22:51:45.127: INFO: Created: latency-svc-42sdp
Dec 12 22:51:45.143: INFO: Got endpoints: latency-svc-cmq78 [353.210172ms]
Dec 12 22:51:45.147: INFO: Got endpoints: latency-svc-969sk [419.674996ms]
Dec 12 22:51:45.154: INFO: Got endpoints: latency-svc-42sdp [338.017565ms]
Dec 12 22:51:45.157: INFO: Got endpoints: latency-svc-9jw2n [356.980693ms]
Dec 12 22:51:45.158: INFO: Created: latency-svc-xlnqj
Dec 12 22:51:45.187: INFO: Created: latency-svc-69c9h
Dec 12 22:51:45.189: INFO: Created: latency-svc-h2f64
Dec 12 22:51:45.190: INFO: Got endpoints: latency-svc-xlnqj [343.175093ms]
Dec 12 22:51:45.201: INFO: Created: latency-svc-ntnts
Dec 12 22:51:45.225: INFO: Got endpoints: latency-svc-ntnts [280.571867ms]
Dec 12 22:51:45.227: INFO: Got endpoints: latency-svc-h2f64 [306.261164ms]
Dec 12 22:51:45.227: INFO: Got endpoints: latency-svc-69c9h [366.11098ms]
Dec 12 22:51:45.236: INFO: Created: latency-svc-smpbj
Dec 12 22:51:45.261: INFO: Created: latency-svc-gj4pw
Dec 12 22:51:45.272: INFO: Created: latency-svc-cfnmt
Dec 12 22:51:45.293: INFO: Got endpoints: latency-svc-gj4pw [299.534816ms]
Dec 12 22:51:45.294: INFO: Got endpoints: latency-svc-smpbj [309.829422ms]
Dec 12 22:51:45.299: INFO: Got endpoints: latency-svc-cfnmt [291.724019ms]
Dec 12 22:51:45.302: INFO: Created: latency-svc-skblh
Dec 12 22:51:45.313: INFO: Got endpoints: latency-svc-skblh [294.71129ms]
Dec 12 22:51:45.322: INFO: Created: latency-svc-bff4k
Dec 12 22:51:45.343: INFO: Got endpoints: latency-svc-bff4k [303.357336ms]
Dec 12 22:51:45.344: INFO: Created: latency-svc-j9jdw
Dec 12 22:51:45.348: INFO: Created: latency-svc-m4s86
Dec 12 22:51:45.350: INFO: Got endpoints: latency-svc-j9jdw [299.491256ms]
Dec 12 22:51:45.356: INFO: Got endpoints: latency-svc-m4s86 [293.509078ms]
Dec 12 22:51:45.364: INFO: Created: latency-svc-brnqj
Dec 12 22:51:45.382: INFO: Created: latency-svc-r9tpx
Dec 12 22:51:45.387: INFO: Created: latency-svc-mlxg2
Dec 12 22:51:45.398: INFO: Got endpoints: latency-svc-brnqj [253.256366ms]
Dec 12 22:51:45.400: INFO: Got endpoints: latency-svc-r9tpx [242.457646ms]
Dec 12 22:51:45.403: INFO: Got endpoints: latency-svc-mlxg2 [247.708272ms]
Dec 12 22:51:45.412: INFO: Created: latency-svc-hjcrg
Dec 12 22:51:45.435: INFO: Got endpoints: latency-svc-hjcrg [287.393042ms]
Dec 12 22:51:45.436: INFO: Created: latency-svc-zbpk8
Dec 12 22:51:45.440: INFO: Created: latency-svc-j7dpn
Dec 12 22:51:45.453: INFO: Got endpoints: latency-svc-zbpk8 [262.080511ms]
Dec 12 22:51:45.456: INFO: Got endpoints: latency-svc-j7dpn [229.656421ms]
Dec 12 22:51:45.468: INFO: Created: latency-svc-qgf57
Dec 12 22:51:45.493: INFO: Got endpoints: latency-svc-qgf57 [264.413738ms]
Dec 12 22:51:45.497: INFO: Created: latency-svc-jrlxw
Dec 12 22:51:45.510: INFO: Got endpoints: latency-svc-jrlxw [282.685513ms]
Dec 12 22:51:45.519: INFO: Created: latency-svc-w9xdc
Dec 12 22:51:45.577: INFO: Got endpoints: latency-svc-w9xdc [281.540117ms]
Dec 12 22:51:45.577: INFO: Created: latency-svc-gvpff
Dec 12 22:51:45.578: INFO: Got endpoints: latency-svc-gvpff [278.242156ms]
Dec 12 22:51:45.595: INFO: Created: latency-svc-6l84b
Dec 12 22:51:45.610: INFO: Created: latency-svc-kdp6g
Dec 12 22:51:45.626: INFO: Created: latency-svc-9ndgv
Dec 12 22:51:45.638: INFO: Got endpoints: latency-svc-kdp6g [324.949611ms]
Dec 12 22:51:45.640: INFO: Created: latency-svc-8k85m
Dec 12 22:51:45.641: INFO: Got endpoints: latency-svc-6l84b [346.765057ms]
Dec 12 22:51:45.642: INFO: Got endpoints: latency-svc-9ndgv [297.64555ms]
Dec 12 22:51:45.664: INFO: Created: latency-svc-pvr4v
Dec 12 22:51:45.701: INFO: Got endpoints: latency-svc-pvr4v [343.670515ms]
Dec 12 22:51:45.701: INFO: Got endpoints: latency-svc-8k85m [349.978924ms]
Dec 12 22:51:45.708: INFO: Created: latency-svc-dg7ch
Dec 12 22:51:45.713: INFO: Created: latency-svc-qrlvq
Dec 12 22:51:45.720: INFO: Got endpoints: latency-svc-dg7ch [320.068552ms]
Dec 12 22:51:45.729: INFO: Created: latency-svc-4phzz
Dec 12 22:51:45.739: INFO: Created: latency-svc-5mzbm
Dec 12 22:51:45.755: INFO: Got endpoints: latency-svc-qrlvq [352.202633ms]
Dec 12 22:51:45.761: INFO: Created: latency-svc-c24fp
Dec 12 22:51:45.780: INFO: Created: latency-svc-kwtpz
Dec 12 22:51:45.785: INFO: Created: latency-svc-7hcqh
Dec 12 22:51:45.811: INFO: Created: latency-svc-g2mn4
Dec 12 22:51:45.813: INFO: Got endpoints: latency-svc-4phzz [413.926771ms]
Dec 12 22:51:45.824: INFO: Created: latency-svc-dngqs
Dec 12 22:51:45.840: INFO: Created: latency-svc-6sf26
Dec 12 22:51:45.849: INFO: Got endpoints: latency-svc-c24fp [392.780878ms]
Dec 12 22:51:45.859: INFO: Created: latency-svc-8llvx
Dec 12 22:51:45.872: INFO: Created: latency-svc-vt6bn
Dec 12 22:51:45.886: INFO: Created: latency-svc-lr9lb
Dec 12 22:51:45.902: INFO: Created: latency-svc-bwkd2
Dec 12 22:51:45.907: INFO: Got endpoints: latency-svc-5mzbm [472.053547ms]
Dec 12 22:51:45.956: INFO: Created: latency-svc-lp96d
Dec 12 22:51:45.958: INFO: Got endpoints: latency-svc-kwtpz [504.57893ms]
Dec 12 22:51:45.981: INFO: Created: latency-svc-z8lkw
Dec 12 22:51:46.004: INFO: Got endpoints: latency-svc-7hcqh [510.933868ms]
Dec 12 22:51:46.013: INFO: Created: latency-svc-flfjx
Dec 12 22:51:46.022: INFO: Created: latency-svc-zxt2w
Dec 12 22:51:46.031: INFO: Created: latency-svc-l9k78
Dec 12 22:51:46.051: INFO: Created: latency-svc-wgksf
Dec 12 22:51:46.054: INFO: Got endpoints: latency-svc-g2mn4 [543.098003ms]
Dec 12 22:51:46.113: INFO: Created: latency-svc-qfpq2
Dec 12 22:51:46.143: INFO: Got endpoints: latency-svc-dngqs [566.258065ms]
Dec 12 22:51:46.146: INFO: Created: latency-svc-22k2l
Dec 12 22:51:46.171: INFO: Created: latency-svc-hjlm5
Dec 12 22:51:46.174: INFO: Got endpoints: latency-svc-6sf26 [596.807221ms]
Dec 12 22:51:46.187: INFO: Created: latency-svc-phcds
Dec 12 22:51:46.204: INFO: Created: latency-svc-ngddj
Dec 12 22:51:46.205: INFO: Got endpoints: latency-svc-8llvx [564.275545ms]
Dec 12 22:51:46.232: INFO: Created: latency-svc-2z97d
Dec 12 22:51:46.248: INFO: Got endpoints: latency-svc-vt6bn [606.523238ms]
Dec 12 22:51:46.269: INFO: Created: latency-svc-nmcxr
Dec 12 22:51:46.306: INFO: Got endpoints: latency-svc-lr9lb [666.195134ms]
Dec 12 22:51:46.328: INFO: Created: latency-svc-kvjm2
Dec 12 22:51:46.361: INFO: Got endpoints: latency-svc-bwkd2 [659.411989ms]
Dec 12 22:51:46.388: INFO: Created: latency-svc-ktnm9
Dec 12 22:51:46.398: INFO: Got endpoints: latency-svc-lp96d [696.742374ms]
Dec 12 22:51:46.419: INFO: Created: latency-svc-dd45j
Dec 12 22:51:46.451: INFO: Got endpoints: latency-svc-z8lkw [730.522054ms]
Dec 12 22:51:46.492: INFO: Created: latency-svc-xmgc8
Dec 12 22:51:46.499: INFO: Got endpoints: latency-svc-flfjx [743.651359ms]
Dec 12 22:51:46.524: INFO: Created: latency-svc-l68lb
Dec 12 22:51:46.555: INFO: Got endpoints: latency-svc-zxt2w [741.910144ms]
Dec 12 22:51:46.602: INFO: Created: latency-svc-fp64g
Dec 12 22:51:46.620: INFO: Got endpoints: latency-svc-l9k78 [768.276318ms]
Dec 12 22:51:46.650: INFO: Got endpoints: latency-svc-wgksf [741.432399ms]
Dec 12 22:51:46.684: INFO: Created: latency-svc-zbzqw
Dec 12 22:51:46.704: INFO: Got endpoints: latency-svc-qfpq2 [744.395666ms]
Dec 12 22:51:46.707: INFO: Created: latency-svc-bbskr
Dec 12 22:51:46.741: INFO: Created: latency-svc-fh4qf
Dec 12 22:51:46.754: INFO: Got endpoints: latency-svc-22k2l [749.576165ms]
Dec 12 22:51:46.800: INFO: Created: latency-svc-7fpwt
Dec 12 22:51:46.809: INFO: Got endpoints: latency-svc-hjlm5 [754.733897ms]
Dec 12 22:51:46.857: INFO: Got endpoints: latency-svc-phcds [712.119612ms]
Dec 12 22:51:46.865: INFO: Created: latency-svc-q6bsm
Dec 12 22:51:46.910: INFO: Got endpoints: latency-svc-ngddj [734.535143ms]
Dec 12 22:51:46.915: INFO: Created: latency-svc-59lnn
Dec 12 22:51:46.932: INFO: Created: latency-svc-pfr4r
Dec 12 22:51:46.955: INFO: Got endpoints: latency-svc-2z97d [749.114344ms]
Dec 12 22:51:46.980: INFO: Created: latency-svc-v5k6s
Dec 12 22:51:47.003: INFO: Got endpoints: latency-svc-nmcxr [754.391284ms]
Dec 12 22:51:47.086: INFO: Got endpoints: latency-svc-kvjm2 [779.451468ms]
Dec 12 22:51:47.092: INFO: Created: latency-svc-qcjzg
Dec 12 22:51:47.114: INFO: Created: latency-svc-gtngw
Dec 12 22:51:47.117: INFO: Got endpoints: latency-svc-ktnm9 [754.926856ms]
Dec 12 22:51:47.134: INFO: Created: latency-svc-lxt8q
Dec 12 22:51:47.160: INFO: Got endpoints: latency-svc-dd45j [760.604007ms]
Dec 12 22:51:47.188: INFO: Created: latency-svc-2wr4c
Dec 12 22:51:47.200: INFO: Got endpoints: latency-svc-xmgc8 [749.017403ms]
Dec 12 22:51:47.221: INFO: Created: latency-svc-klv42
Dec 12 22:51:47.249: INFO: Got endpoints: latency-svc-l68lb [748.656641ms]
Dec 12 22:51:47.309: INFO: Got endpoints: latency-svc-fp64g [752.489782ms]
Dec 12 22:51:47.312: INFO: Created: latency-svc-qv6m5
Dec 12 22:51:47.330: INFO: Created: latency-svc-zc4lj
Dec 12 22:51:47.346: INFO: Got endpoints: latency-svc-zbzqw [725.269356ms]
Dec 12 22:51:47.382: INFO: Created: latency-svc-r8bzl
Dec 12 22:51:47.412: INFO: Got endpoints: latency-svc-bbskr [761.022794ms]
Dec 12 22:51:47.448: INFO: Created: latency-svc-8ngcx
Dec 12 22:51:47.458: INFO: Got endpoints: latency-svc-fh4qf [753.05537ms]
Dec 12 22:51:47.510: INFO: Created: latency-svc-wh27j
Dec 12 22:51:47.512: INFO: Got endpoints: latency-svc-7fpwt [756.564778ms]
Dec 12 22:51:47.544: INFO: Created: latency-svc-wr42p
Dec 12 22:51:47.556: INFO: Got endpoints: latency-svc-q6bsm [746.507912ms]
Dec 12 22:51:47.581: INFO: Created: latency-svc-2dwrg
Dec 12 22:51:47.598: INFO: Got endpoints: latency-svc-59lnn [739.756062ms]
Dec 12 22:51:47.641: INFO: Created: latency-svc-v2wtr
Dec 12 22:51:47.651: INFO: Got endpoints: latency-svc-pfr4r [740.616422ms]
Dec 12 22:51:47.679: INFO: Created: latency-svc-w9m5d
Dec 12 22:51:47.703: INFO: Got endpoints: latency-svc-v5k6s [746.659424ms]
Dec 12 22:51:47.725: INFO: Created: latency-svc-t9cvg
Dec 12 22:51:47.787: INFO: Got endpoints: latency-svc-qcjzg [783.16635ms]
Dec 12 22:51:47.805: INFO: Got endpoints: latency-svc-gtngw [718.586995ms]
Dec 12 22:51:47.817: INFO: Created: latency-svc-4qpsq
Dec 12 22:51:47.838: INFO: Created: latency-svc-pfgrk
Dec 12 22:51:47.845: INFO: Got endpoints: latency-svc-lxt8q [727.94175ms]
Dec 12 22:51:47.869: INFO: Created: latency-svc-nwggj
Dec 12 22:51:47.896: INFO: Got endpoints: latency-svc-2wr4c [735.261006ms]
Dec 12 22:51:47.920: INFO: Created: latency-svc-5rpx4
Dec 12 22:51:47.950: INFO: Got endpoints: latency-svc-klv42 [749.992175ms]
Dec 12 22:51:47.989: INFO: Created: latency-svc-t2wmg
Dec 12 22:51:47.997: INFO: Got endpoints: latency-svc-qv6m5 [746.324253ms]
Dec 12 22:51:48.020: INFO: Created: latency-svc-sggpb
Dec 12 22:51:48.047: INFO: Got endpoints: latency-svc-zc4lj [737.838569ms]
Dec 12 22:51:48.069: INFO: Created: latency-svc-tng79
Dec 12 22:51:48.099: INFO: Got endpoints: latency-svc-r8bzl [752.823576ms]
Dec 12 22:51:48.122: INFO: Created: latency-svc-xls9h
Dec 12 22:51:48.146: INFO: Got endpoints: latency-svc-8ngcx [733.028278ms]
Dec 12 22:51:48.164: INFO: Created: latency-svc-d8jlx
Dec 12 22:51:48.197: INFO: Got endpoints: latency-svc-wh27j [737.983826ms]
Dec 12 22:51:48.220: INFO: Created: latency-svc-rfjv6
Dec 12 22:51:48.252: INFO: Got endpoints: latency-svc-wr42p [739.104538ms]
Dec 12 22:51:48.289: INFO: Created: latency-svc-sqkhf
Dec 12 22:51:48.297: INFO: Got endpoints: latency-svc-2dwrg [740.702582ms]
Dec 12 22:51:48.318: INFO: Created: latency-svc-csqr9
Dec 12 22:51:48.348: INFO: Got endpoints: latency-svc-v2wtr [748.465415ms]
Dec 12 22:51:48.396: INFO: Created: latency-svc-krs62
Dec 12 22:51:48.397: INFO: Got endpoints: latency-svc-w9m5d [746.390006ms]
Dec 12 22:51:48.420: INFO: Created: latency-svc-scp4c
Dec 12 22:51:48.445: INFO: Got endpoints: latency-svc-t9cvg [742.750378ms]
Dec 12 22:51:48.465: INFO: Created: latency-svc-7gn7g
Dec 12 22:51:48.496: INFO: Got endpoints: latency-svc-4qpsq [708.398657ms]
Dec 12 22:51:48.515: INFO: Created: latency-svc-hdhwr
Dec 12 22:51:48.548: INFO: Got endpoints: latency-svc-pfgrk [742.832822ms]
Dec 12 22:51:48.569: INFO: Created: latency-svc-dmrwr
Dec 12 22:51:48.600: INFO: Got endpoints: latency-svc-nwggj [754.308629ms]
Dec 12 22:51:48.620: INFO: Created: latency-svc-jsp76
Dec 12 22:51:48.647: INFO: Got endpoints: latency-svc-5rpx4 [749.995744ms]
Dec 12 22:51:48.666: INFO: Created: latency-svc-rr562
Dec 12 22:51:48.698: INFO: Got endpoints: latency-svc-t2wmg [748.18365ms]
Dec 12 22:51:48.760: INFO: Got endpoints: latency-svc-sggpb [761.547179ms]
Dec 12 22:51:48.767: INFO: Created: latency-svc-89gpc
Dec 12 22:51:48.785: INFO: Created: latency-svc-jvw5c
Dec 12 22:51:48.796: INFO: Got endpoints: latency-svc-tng79 [747.915086ms]
Dec 12 22:51:48.816: INFO: Created: latency-svc-b754c
Dec 12 22:51:48.852: INFO: Got endpoints: latency-svc-xls9h [751.432999ms]
Dec 12 22:51:48.875: INFO: Created: latency-svc-fshjr
Dec 12 22:51:48.896: INFO: Got endpoints: latency-svc-d8jlx [747.833287ms]
Dec 12 22:51:48.913: INFO: Created: latency-svc-cl4v8
Dec 12 22:51:48.946: INFO: Got endpoints: latency-svc-rfjv6 [748.460967ms]
Dec 12 22:51:48.973: INFO: Created: latency-svc-94qph
Dec 12 22:51:48.996: INFO: Got endpoints: latency-svc-sqkhf [741.680322ms]
Dec 12 22:51:49.027: INFO: Created: latency-svc-z4m5p
Dec 12 22:51:49.050: INFO: Got endpoints: latency-svc-csqr9 [751.61129ms]
Dec 12 22:51:49.069: INFO: Created: latency-svc-s4jj6
Dec 12 22:51:49.097: INFO: Got endpoints: latency-svc-krs62 [748.216619ms]
Dec 12 22:51:49.140: INFO: Created: latency-svc-pc8zn
Dec 12 22:51:49.148: INFO: Got endpoints: latency-svc-scp4c [749.411379ms]
Dec 12 22:51:49.170: INFO: Created: latency-svc-thl2c
Dec 12 22:51:49.196: INFO: Got endpoints: latency-svc-7gn7g [750.859716ms]
Dec 12 22:51:49.216: INFO: Created: latency-svc-bmqc5
Dec 12 22:51:49.246: INFO: Got endpoints: latency-svc-hdhwr [750.43409ms]
Dec 12 22:51:49.273: INFO: Created: latency-svc-pqk4h
Dec 12 22:51:49.296: INFO: Got endpoints: latency-svc-dmrwr [746.532444ms]
Dec 12 22:51:49.321: INFO: Created: latency-svc-68sk2
Dec 12 22:51:49.358: INFO: Got endpoints: latency-svc-jsp76 [757.437878ms]
Dec 12 22:51:49.375: INFO: Created: latency-svc-xrdcj
Dec 12 22:51:49.397: INFO: Got endpoints: latency-svc-rr562 [749.052746ms]
Dec 12 22:51:49.437: INFO: Created: latency-svc-gxb7w
Dec 12 22:51:49.447: INFO: Got endpoints: latency-svc-89gpc [747.04941ms]
Dec 12 22:51:49.469: INFO: Created: latency-svc-2xcwd
Dec 12 22:51:49.497: INFO: Got endpoints: latency-svc-jvw5c [736.871457ms]
Dec 12 22:51:49.521: INFO: Created: latency-svc-tk89r
Dec 12 22:51:49.552: INFO: Got endpoints: latency-svc-b754c [753.66622ms]
Dec 12 22:51:49.571: INFO: Created: latency-svc-sql28
Dec 12 22:51:49.596: INFO: Got endpoints: latency-svc-fshjr [743.123082ms]
Dec 12 22:51:49.613: INFO: Created: latency-svc-mwmn2
Dec 12 22:51:49.662: INFO: Got endpoints: latency-svc-cl4v8 [765.119715ms]
Dec 12 22:51:49.705: INFO: Got endpoints: latency-svc-94qph [758.091406ms]
Dec 12 22:51:49.711: INFO: Created: latency-svc-hpmcx
Dec 12 22:51:49.738: INFO: Created: latency-svc-dfvtf
Dec 12 22:51:49.764: INFO: Got endpoints: latency-svc-z4m5p [767.427186ms]
Dec 12 22:51:49.815: INFO: Got endpoints: latency-svc-s4jj6 [764.725249ms]
Dec 12 22:51:49.817: INFO: Created: latency-svc-xdlkn
Dec 12 22:51:49.834: INFO: Created: latency-svc-sx6tq
Dec 12 22:51:49.861: INFO: Got endpoints: latency-svc-pc8zn [762.122443ms]
Dec 12 22:51:49.893: INFO: Created: latency-svc-hc8k8
Dec 12 22:51:49.900: INFO: Got endpoints: latency-svc-thl2c [750.446166ms]
Dec 12 22:51:49.918: INFO: Created: latency-svc-qk7jb
Dec 12 22:51:49.953: INFO: Got endpoints: latency-svc-bmqc5 [757.006012ms]
Dec 12 22:51:50.018: INFO: Created: latency-svc-fqnlh
Dec 12 22:51:50.021: INFO: Got endpoints: latency-svc-pqk4h [774.6588ms]
Dec 12 22:51:50.041: INFO: Created: latency-svc-gppm8
Dec 12 22:51:50.051: INFO: Got endpoints: latency-svc-68sk2 [753.395174ms]
Dec 12 22:51:50.072: INFO: Created: latency-svc-ln2hq
Dec 12 22:51:50.097: INFO: Got endpoints: latency-svc-xrdcj [739.398172ms]
Dec 12 22:51:50.118: INFO: Created: latency-svc-bcckb
Dec 12 22:51:50.161: INFO: Got endpoints: latency-svc-gxb7w [763.285753ms]
Dec 12 22:51:50.191: INFO: Created: latency-svc-llkth
Dec 12 22:51:50.215: INFO: Got endpoints: latency-svc-2xcwd [766.268861ms]
Dec 12 22:51:50.248: INFO: Got endpoints: latency-svc-tk89r [750.37131ms]
Dec 12 22:51:50.251: INFO: Created: latency-svc-jsnc8
Dec 12 22:51:50.269: INFO: Created: latency-svc-tfrvn
Dec 12 22:51:50.300: INFO: Got endpoints: latency-svc-sql28 [746.53829ms]
Dec 12 22:51:50.319: INFO: Created: latency-svc-8cqdn
Dec 12 22:51:50.348: INFO: Got endpoints: latency-svc-mwmn2 [752.075957ms]
Dec 12 22:51:50.407: INFO: Got endpoints: latency-svc-hpmcx [743.954132ms]
Dec 12 22:51:50.409: INFO: Created: latency-svc-4mhg6
Dec 12 22:51:50.426: INFO: Created: latency-svc-lft5f
Dec 12 22:51:50.446: INFO: Got endpoints: latency-svc-dfvtf [740.924924ms]
Dec 12 22:51:50.485: INFO: Created: latency-svc-pjqxl
Dec 12 22:51:50.495: INFO: Got endpoints: latency-svc-xdlkn [730.448667ms]
Dec 12 22:51:50.511: INFO: Created: latency-svc-kh9xl
Dec 12 22:51:50.548: INFO: Got endpoints: latency-svc-sx6tq [732.178609ms]
Dec 12 22:51:50.567: INFO: Created: latency-svc-lvhc2
Dec 12 22:51:50.598: INFO: Got endpoints: latency-svc-hc8k8 [735.683596ms]
Dec 12 22:51:50.617: INFO: Created: latency-svc-fdggm
Dec 12 22:51:50.648: INFO: Got endpoints: latency-svc-qk7jb [746.848659ms]
Dec 12 22:51:50.666: INFO: Created: latency-svc-s6z9m
Dec 12 22:51:50.703: INFO: Got endpoints: latency-svc-fqnlh [748.077094ms]
Dec 12 22:51:50.750: INFO: Got endpoints: latency-svc-gppm8 [727.382106ms]
Dec 12 22:51:50.753: INFO: Created: latency-svc-7p7cr
Dec 12 22:51:50.786: INFO: Created: latency-svc-4fcrz
Dec 12 22:51:50.796: INFO: Got endpoints: latency-svc-ln2hq [743.026617ms]
Dec 12 22:51:50.823: INFO: Created: latency-svc-j9scc
Dec 12 22:51:50.851: INFO: Got endpoints: latency-svc-bcckb [751.865992ms]
Dec 12 22:51:50.867: INFO: Created: latency-svc-g86cq
Dec 12 22:51:50.912: INFO: Got endpoints: latency-svc-llkth [750.847903ms]
Dec 12 22:51:50.932: INFO: Created: latency-svc-vnmkm
Dec 12 22:51:50.949: INFO: Got endpoints: latency-svc-jsnc8 [732.668855ms]
Dec 12 22:51:50.967: INFO: Created: latency-svc-2d988
Dec 12 22:51:50.999: INFO: Got endpoints: latency-svc-tfrvn [750.124626ms]
Dec 12 22:51:51.039: INFO: Created: latency-svc-x4m9m
Dec 12 22:51:51.046: INFO: Got endpoints: latency-svc-8cqdn [745.968319ms]
Dec 12 22:51:51.068: INFO: Created: latency-svc-gbrd5
Dec 12 22:51:51.147: INFO: Got endpoints: latency-svc-4mhg6 [797.391894ms]
Dec 12 22:51:51.149: INFO: Got endpoints: latency-svc-lft5f [741.691101ms]
Dec 12 22:51:51.187: INFO: Created: latency-svc-rbpcc
Dec 12 22:51:51.197: INFO: Created: latency-svc-7zcd4
Dec 12 22:51:51.199: INFO: Got endpoints: latency-svc-pjqxl [751.264359ms]
Dec 12 22:51:51.237: INFO: Created: latency-svc-dzjd5
Dec 12 22:51:51.248: INFO: Got endpoints: latency-svc-kh9xl [752.839731ms]
Dec 12 22:51:51.268: INFO: Created: latency-svc-f4g98
Dec 12 22:51:51.299: INFO: Got endpoints: latency-svc-lvhc2 [750.2434ms]
Dec 12 22:51:51.335: INFO: Created: latency-svc-qncvp
Dec 12 22:51:51.349: INFO: Got endpoints: latency-svc-fdggm [750.411056ms]
Dec 12 22:51:51.366: INFO: Created: latency-svc-5x6dm
Dec 12 22:51:51.398: INFO: Got endpoints: latency-svc-s6z9m [748.634408ms]
Dec 12 22:51:51.436: INFO: Created: latency-svc-92xd6
Dec 12 22:51:51.449: INFO: Got endpoints: latency-svc-7p7cr [745.931658ms]
Dec 12 22:51:51.482: INFO: Created: latency-svc-cdrk4
Dec 12 22:51:51.517: INFO: Got endpoints: latency-svc-4fcrz [766.605205ms]
Dec 12 22:51:51.539: INFO: Created: latency-svc-fbsr8
Dec 12 22:51:51.546: INFO: Got endpoints: latency-svc-j9scc [749.974304ms]
Dec 12 22:51:51.568: INFO: Created: latency-svc-gc8ww
Dec 12 22:51:51.600: INFO: Got endpoints: latency-svc-g86cq [749.458893ms]
Dec 12 22:51:51.623: INFO: Created: latency-svc-95gcs
Dec 12 22:51:51.646: INFO: Got endpoints: latency-svc-vnmkm [734.051291ms]
Dec 12 22:51:51.667: INFO: Created: latency-svc-rsv69
Dec 12 22:51:51.696: INFO: Got endpoints: latency-svc-2d988 [746.188181ms]
Dec 12 22:51:51.722: INFO: Created: latency-svc-qr625
Dec 12 22:51:51.781: INFO: Got endpoints: latency-svc-x4m9m [780.429699ms]
Dec 12 22:51:51.798: INFO: Got endpoints: latency-svc-gbrd5 [751.055756ms]
Dec 12 22:51:51.808: INFO: Created: latency-svc-2m58s
Dec 12 22:51:51.843: INFO: Created: latency-svc-n72d5
Dec 12 22:51:51.873: INFO: Got endpoints: latency-svc-rbpcc [724.984957ms]
Dec 12 22:51:51.893: INFO: Created: latency-svc-5zvsj
Dec 12 22:51:51.897: INFO: Got endpoints: latency-svc-7zcd4 [746.944082ms]
Dec 12 22:51:51.919: INFO: Created: latency-svc-tknmh
Dec 12 22:51:51.949: INFO: Got endpoints: latency-svc-dzjd5 [750.169177ms]
Dec 12 22:51:51.975: INFO: Created: latency-svc-47l7n
Dec 12 22:51:51.997: INFO: Got endpoints: latency-svc-f4g98 [747.634007ms]
Dec 12 22:51:52.020: INFO: Created: latency-svc-89ddb
Dec 12 22:51:52.056: INFO: Got endpoints: latency-svc-qncvp [756.979605ms]
Dec 12 22:51:52.077: INFO: Created: latency-svc-jz477
Dec 12 22:51:52.111: INFO: Got endpoints: latency-svc-5x6dm [761.1167ms]
Dec 12 22:51:52.132: INFO: Created: latency-svc-dlmzj
Dec 12 22:51:52.149: INFO: Got endpoints: latency-svc-92xd6 [750.544092ms]
Dec 12 22:51:52.172: INFO: Created: latency-svc-dcfcw
Dec 12 22:51:52.198: INFO: Got endpoints: latency-svc-cdrk4 [747.766935ms]
Dec 12 22:51:52.219: INFO: Created: latency-svc-tj4j9
Dec 12 22:51:52.249: INFO: Got endpoints: latency-svc-fbsr8 [730.835338ms]
Dec 12 22:51:52.282: INFO: Created: latency-svc-6khm6
Dec 12 22:51:52.296: INFO: Got endpoints: latency-svc-gc8ww [749.199797ms]
Dec 12 22:51:52.314: INFO: Created: latency-svc-4nhjr
Dec 12 22:51:52.348: INFO: Got endpoints: latency-svc-95gcs [746.168024ms]
Dec 12 22:51:52.398: INFO: Got endpoints: latency-svc-rsv69 [750.925399ms]
Dec 12 22:51:52.447: INFO: Got endpoints: latency-svc-qr625 [749.42392ms]
Dec 12 22:51:52.507: INFO: Got endpoints: latency-svc-2m58s [724.625372ms]
Dec 12 22:51:52.547: INFO: Got endpoints: latency-svc-n72d5 [747.166948ms]
Dec 12 22:51:52.599: INFO: Got endpoints: latency-svc-5zvsj [723.249077ms]
Dec 12 22:51:52.648: INFO: Got endpoints: latency-svc-tknmh [749.225786ms]
Dec 12 22:51:52.697: INFO: Got endpoints: latency-svc-47l7n [747.994661ms]
Dec 12 22:51:52.749: INFO: Got endpoints: latency-svc-89ddb [751.633494ms]
Dec 12 22:51:52.796: INFO: Got endpoints: latency-svc-jz477 [738.808487ms]
Dec 12 22:51:52.849: INFO: Got endpoints: latency-svc-dlmzj [736.206575ms]
Dec 12 22:51:52.897: INFO: Got endpoints: latency-svc-dcfcw [747.9019ms]
Dec 12 22:51:52.947: INFO: Got endpoints: latency-svc-tj4j9 [748.798381ms]
Dec 12 22:51:52.997: INFO: Got endpoints: latency-svc-6khm6 [748.417775ms]
Dec 12 22:51:53.049: INFO: Got endpoints: latency-svc-4nhjr [753.060652ms]
Dec 12 22:51:53.049: INFO: Latencies: [33.400232ms 90.549607ms 138.303934ms 140.141419ms 140.584124ms 154.474286ms 180.754389ms 226.213674ms 229.656421ms 242.457646ms 247.708272ms 253.256366ms 262.080511ms 264.413738ms 278.242156ms 280.571867ms 281.540117ms 282.685513ms 287.393042ms 288.889745ms 291.724019ms 293.509078ms 294.71129ms 297.64555ms 299.374935ms 299.491256ms 299.534816ms 303.357336ms 306.261164ms 309.829422ms 313.823563ms 320.068552ms 324.949611ms 338.017565ms 343.175093ms 343.670515ms 345.31951ms 346.765057ms 349.978924ms 352.202633ms 353.210172ms 356.980693ms 359.956411ms 366.11098ms 367.024217ms 377.680126ms 380.160568ms 392.780878ms 395.197344ms 395.918907ms 401.900243ms 413.926771ms 418.754796ms 419.674996ms 443.528899ms 450.569977ms 472.053547ms 504.57893ms 510.933868ms 543.098003ms 564.275545ms 566.258065ms 596.807221ms 606.523238ms 659.411989ms 666.195134ms 696.742374ms 708.398657ms 712.119612ms 718.586995ms 723.249077ms 724.625372ms 724.984957ms 725.269356ms 727.382106ms 727.94175ms 730.448667ms 730.522054ms 730.835338ms 732.178609ms 732.668855ms 733.028278ms 734.051291ms 734.535143ms 735.261006ms 735.683596ms 736.206575ms 736.871457ms 737.838569ms 737.983826ms 738.808487ms 739.104538ms 739.398172ms 739.756062ms 740.616422ms 740.702582ms 740.924924ms 741.432399ms 741.680322ms 741.691101ms 741.910144ms 742.750378ms 742.832822ms 743.026617ms 743.123082ms 743.651359ms 743.954132ms 744.395666ms 745.931658ms 745.968319ms 746.168024ms 746.188181ms 746.324253ms 746.390006ms 746.507912ms 746.532444ms 746.53829ms 746.659424ms 746.848659ms 746.944082ms 747.04941ms 747.166948ms 747.634007ms 747.766935ms 747.833287ms 747.9019ms 747.915086ms 747.994661ms 748.077094ms 748.18365ms 748.216619ms 748.417775ms 748.460967ms 748.465415ms 748.634408ms 748.656641ms 748.798381ms 749.017403ms 749.052746ms 749.114344ms 749.199797ms 749.225786ms 749.411379ms 749.42392ms 749.458893ms 749.576165ms 749.974304ms 749.992175ms 749.995744ms 750.124626ms 750.169177ms 750.2434ms 750.37131ms 750.411056ms 750.43409ms 750.446166ms 750.544092ms 750.847903ms 750.859716ms 750.925399ms 751.055756ms 751.264359ms 751.432999ms 751.61129ms 751.633494ms 751.865992ms 752.075957ms 752.489782ms 752.823576ms 752.839731ms 753.05537ms 753.060652ms 753.395174ms 753.66622ms 754.308629ms 754.391284ms 754.733897ms 754.926856ms 756.564778ms 756.979605ms 757.006012ms 757.437878ms 758.091406ms 760.604007ms 761.022794ms 761.1167ms 761.547179ms 762.122443ms 763.285753ms 764.725249ms 765.119715ms 766.268861ms 766.605205ms 767.427186ms 768.276318ms 774.6588ms 779.451468ms 780.429699ms 783.16635ms 797.391894ms]
Dec 12 22:51:53.049: INFO: 50 %ile: 741.910144ms
Dec 12 22:51:53.049: INFO: 90 %ile: 757.006012ms
Dec 12 22:51:53.049: INFO: 99 %ile: 783.16635ms
Dec 12 22:51:53.050: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:53.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8942" for this suite.

• [SLOW TEST:11.959 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":256,"skipped":4197,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:53.076: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 12 22:51:53.271: INFO: Waiting up to 5m0s for pod "pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa" in namespace "emptydir-6850" to be "success or failure"
Dec 12 22:51:53.275: INFO: Pod "pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.661624ms
Dec 12 22:51:55.280: INFO: Pod "pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008806433s
Dec 12 22:51:57.284: INFO: Pod "pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013138255s
STEP: Saw pod success
Dec 12 22:51:57.284: INFO: Pod "pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa" satisfied condition "success or failure"
Dec 12 22:51:57.288: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa container test-container: <nil>
STEP: delete the pod
Dec 12 22:51:57.317: INFO: Waiting for pod pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa to disappear
Dec 12 22:51:57.320: INFO: Pod pod-c4bbd55e-8842-4346-bf76-0af1116d0ffa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:51:57.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6850" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4216,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:51:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:52:13.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3059" for this suite.

• [SLOW TEST:16.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":258,"skipped":4225,"failed":0}
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:52:13.605: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Dec 12 22:52:13.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=kubectl-3008 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 12 22:52:17.015: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 12 22:52:17.015: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:52:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3008" for this suite.

• [SLOW TEST:5.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1924
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":259,"skipped":4225,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:52:19.057: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-29a37067-a6f2-49fe-9042-e893fac8ee7b
STEP: Creating a pod to test consume configMaps
Dec 12 22:52:19.274: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db" in namespace "projected-9085" to be "success or failure"
Dec 12 22:52:19.277: INFO: Pod "pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.427223ms
Dec 12 22:52:21.281: INFO: Pod "pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007709829s
Dec 12 22:52:23.286: INFO: Pod "pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01266523s
STEP: Saw pod success
Dec 12 22:52:23.286: INFO: Pod "pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db" satisfied condition "success or failure"
Dec 12 22:52:23.290: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:52:23.311: INFO: Waiting for pod pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db to disappear
Dec 12 22:52:23.315: INFO: Pod pod-projected-configmaps-af12e320-62cb-48b2-97c5-3555b58185db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:52:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9085" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4229,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:52:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Dec 12 22:52:23.536: INFO: namespace kubectl-5624
Dec 12 22:52:23.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 create -f - --namespace=kubectl-5624'
Dec 12 22:52:23.886: INFO: stderr: ""
Dec 12 22:52:23.886: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 12 22:52:24.890: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:52:24.890: INFO: Found 0 / 1
Dec 12 22:52:25.892: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:52:25.892: INFO: Found 0 / 1
Dec 12 22:52:26.891: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:52:26.891: INFO: Found 0 / 1
Dec 12 22:52:27.891: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:52:27.891: INFO: Found 1 / 1
Dec 12 22:52:27.891: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 12 22:52:27.894: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 12 22:52:27.895: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 12 22:52:27.895: INFO: wait on agnhost-master startup in kubectl-5624 
Dec 12 22:52:27.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 logs agnhost-master-9cpqc agnhost-master --namespace=kubectl-5624'
Dec 12 22:52:28.001: INFO: stderr: ""
Dec 12 22:52:28.001: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 12 22:52:28.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5624'
Dec 12 22:52:28.114: INFO: stderr: ""
Dec 12 22:52:28.114: INFO: stdout: "service/rm2 exposed\n"
Dec 12 22:52:28.119: INFO: Service rm2 in namespace kubectl-5624 found.
STEP: exposing service
Dec 12 22:52:30.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5624'
Dec 12 22:52:30.300: INFO: stderr: ""
Dec 12 22:52:30.300: INFO: stdout: "service/rm3 exposed\n"
Dec 12 22:52:30.308: INFO: Service rm3 in namespace kubectl-5624 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:52:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5624" for this suite.

• [SLOW TEST:8.999 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":261,"skipped":4248,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:52:32.326: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Dec 12 22:52:32.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607" in namespace "downward-api-7880" to be "success or failure"
Dec 12 22:52:32.503: INFO: Pod "downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04733ms
Dec 12 22:52:34.508: INFO: Pod "downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008979984s
Dec 12 22:52:36.513: INFO: Pod "downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013773355s
STEP: Saw pod success
Dec 12 22:52:36.513: INFO: Pod "downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607" satisfied condition "success or failure"
Dec 12 22:52:36.517: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607 container client-container: <nil>
STEP: delete the pod
Dec 12 22:52:36.543: INFO: Waiting for pod downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607 to disappear
Dec 12 22:52:36.546: INFO: Pod downwardapi-volume-e6225449-6fa9-48db-bb59-bff9cde96607 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:52:36.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7880" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4255,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:52:36.559: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 12 22:52:41.271: INFO: Successfully updated pod "adopt-release-8fmpc"
STEP: Checking that the Job readopts the Pod
Dec 12 22:52:41.272: INFO: Waiting up to 15m0s for pod "adopt-release-8fmpc" in namespace "job-4280" to be "adopted"
Dec 12 22:52:41.275: INFO: Pod "adopt-release-8fmpc": Phase="Running", Reason="", readiness=true. Elapsed: 3.305692ms
Dec 12 22:52:43.280: INFO: Pod "adopt-release-8fmpc": Phase="Running", Reason="", readiness=true. Elapsed: 2.00783643s
Dec 12 22:52:43.280: INFO: Pod "adopt-release-8fmpc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 12 22:52:43.793: INFO: Successfully updated pod "adopt-release-8fmpc"
STEP: Checking that the Job releases the Pod
Dec 12 22:52:43.793: INFO: Waiting up to 15m0s for pod "adopt-release-8fmpc" in namespace "job-4280" to be "released"
Dec 12 22:52:43.796: INFO: Pod "adopt-release-8fmpc": Phase="Running", Reason="", readiness=true. Elapsed: 3.295251ms
Dec 12 22:52:45.801: INFO: Pod "adopt-release-8fmpc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007761545s
Dec 12 22:52:45.801: INFO: Pod "adopt-release-8fmpc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:52:45.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4280" for this suite.

• [SLOW TEST:9.253 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":263,"skipped":4258,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:52:45.814: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-45xw
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 22:52:46.023: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-45xw" in namespace "subpath-747" to be "success or failure"
Dec 12 22:52:46.028: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810855ms
Dec 12 22:52:48.033: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009838929s
Dec 12 22:52:50.037: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 4.014155784s
Dec 12 22:52:52.042: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 6.018770525s
Dec 12 22:52:54.046: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 8.02302582s
Dec 12 22:52:56.051: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 10.027301016s
Dec 12 22:52:58.055: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 12.031521332s
Dec 12 22:53:00.059: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 14.035828563s
Dec 12 22:53:02.063: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 16.040169088s
Dec 12 22:53:04.071: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 18.047874776s
Dec 12 22:53:06.077: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 20.053591527s
Dec 12 22:53:08.082: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Running", Reason="", readiness=true. Elapsed: 22.058479265s
Dec 12 22:53:10.086: INFO: Pod "pod-subpath-test-secret-45xw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.062461817s
STEP: Saw pod success
Dec 12 22:53:10.086: INFO: Pod "pod-subpath-test-secret-45xw" satisfied condition "success or failure"
Dec 12 22:53:10.090: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-4kj7s pod pod-subpath-test-secret-45xw container test-container-subpath-secret-45xw: <nil>
STEP: delete the pod
Dec 12 22:53:10.123: INFO: Waiting for pod pod-subpath-test-secret-45xw to disappear
Dec 12 22:53:10.127: INFO: Pod pod-subpath-test-secret-45xw no longer exists
STEP: Deleting pod pod-subpath-test-secret-45xw
Dec 12 22:53:10.127: INFO: Deleting pod "pod-subpath-test-secret-45xw" in namespace "subpath-747"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:53:10.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-747" for this suite.

• [SLOW TEST:24.357 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":264,"skipped":4265,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:53:10.173: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:53:15.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6990" for this suite.

• [SLOW TEST:5.273 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":265,"skipped":4267,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:53:15.449: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 22:53:16.314: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 12 22:53:18.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787996, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787996, loc:(*time.Location)(0x7d421e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787996, loc:(*time.Location)(0x7d421e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711787996, loc:(*time.Location)(0x7d421e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 22:53:21.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:53:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9342-crds.webhook.example.com via the AdmissionRegistration API
Dec 12 22:53:21.969: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:53:22.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1207" for this suite.
STEP: Destroying namespace "webhook-1207-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.511 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":266,"skipped":4288,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:53:22.964: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 12 22:53:31.295: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 22:53:31.299: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 22:53:33.299: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 22:53:33.305: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 22:53:35.300: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 22:53:35.305: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 22:53:37.299: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 22:53:37.305: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:53:37.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3104" for this suite.

• [SLOW TEST:14.360 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4296,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:53:37.325: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-3e9ee6fe-e908-40bd-af34-4398356672f2
STEP: Creating a pod to test consume configMaps
Dec 12 22:53:37.534: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1" in namespace "configmap-5075" to be "success or failure"
Dec 12 22:53:37.538: INFO: Pod "pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358529ms
Dec 12 22:53:39.543: INFO: Pod "pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008185333s
Dec 12 22:53:41.551: INFO: Pod "pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016610509s
STEP: Saw pod success
Dec 12 22:53:41.551: INFO: Pod "pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1" satisfied condition "success or failure"
Dec 12 22:53:41.555: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 22:53:41.575: INFO: Waiting for pod pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1 to disappear
Dec 12 22:53:41.580: INFO: Pod pod-configmaps-e8edbd01-d683-4bb7-b7af-1491db0223b1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:53:41.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5075" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":268,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:53:41.596: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 22:53:44.799: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:53:44.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8879" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":269,"skipped":4328,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:53:44.843: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1046
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1046
STEP: creating replication controller externalsvc in namespace services-1046
I1212 22:53:45.059459      22 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1046, replica count: 2
I1212 22:53:48.109920      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 22:53:51.110273      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 12 22:53:51.139: INFO: Creating new exec pod
Dec 12 22:53:55.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 exec --namespace=services-1046 execpodjcgdh -- /bin/sh -x -c nslookup nodeport-service'
Dec 12 22:53:55.353: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 12 22:53:55.353: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1046.svc.cluster.local\tcanonical name = externalsvc.services-1046.svc.cluster.local.\nName:\texternalsvc.services-1046.svc.cluster.local\nAddress: 10.96.157.149\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1046, will wait for the garbage collector to delete the pods
Dec 12 22:53:55.419: INFO: Deleting ReplicationController externalsvc took: 11.480638ms
Dec 12 22:53:55.719: INFO: Terminating ReplicationController externalsvc pods took: 300.32297ms
Dec 12 22:54:03.845: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:54:03.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1046" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:19.039 seconds]
[sig-network] Services
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":270,"skipped":4335,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:54:03.887: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-1090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Dec 12 22:54:04.089: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 12 22:55:04.109: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:55:04.117: INFO: Starting informer...
STEP: Starting pod...
Dec 12 22:55:04.335: INFO: Pod is running on talos-0-3-0-beta-0-gcp-workers-nt7qc. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 12 22:55:04.355: INFO: Pod wasn't evicted. Proceeding
Dec 12 22:55:04.355: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 12 22:56:19.373: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:56:19.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1090" for this suite.

• [SLOW TEST:135.497 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":271,"skipped":4354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:56:19.386: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 12 22:56:19.559: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1266 /api/v1/namespaces/watch-1266/configmaps/e2e-watch-test-watch-closed a387884c-f610-4e5d-8c1b-a81614f4dc7e 31426 0 2019-12-12 22:56:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 22:56:19.560: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1266 /api/v1/namespaces/watch-1266/configmaps/e2e-watch-test-watch-closed a387884c-f610-4e5d-8c1b-a81614f4dc7e 31427 0 2019-12-12 22:56:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 12 22:56:19.583: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1266 /api/v1/namespaces/watch-1266/configmaps/e2e-watch-test-watch-closed a387884c-f610-4e5d-8c1b-a81614f4dc7e 31428 0 2019-12-12 22:56:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 22:56:19.583: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1266 /api/v1/namespaces/watch-1266/configmaps/e2e-watch-test-watch-closed a387884c-f610-4e5d-8c1b-a81614f4dc7e 31429 0 2019-12-12 22:56:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:56:19.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1266" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":272,"skipped":4392,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:56:19.597: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:56:27.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1789" for this suite.

• [SLOW TEST:8.205 seconds]
[sig-apps] Job
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":273,"skipped":4401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:56:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8016
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:56:27.965: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 12 22:56:30.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-8016 create -f -'
Dec 12 22:56:31.315: INFO: stderr: ""
Dec 12 22:56:31.315: INFO: stdout: "e2e-test-crd-publish-openapi-4225-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 12 22:56:31.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-8016 delete e2e-test-crd-publish-openapi-4225-crds test-cr'
Dec 12 22:56:31.411: INFO: stderr: ""
Dec 12 22:56:31.411: INFO: stdout: "e2e-test-crd-publish-openapi-4225-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 12 22:56:31.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-8016 apply -f -'
Dec 12 22:56:31.762: INFO: stderr: ""
Dec 12 22:56:31.762: INFO: stdout: "e2e-test-crd-publish-openapi-4225-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 12 22:56:31.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 --namespace=crd-publish-openapi-8016 delete e2e-test-crd-publish-openapi-4225-crds test-cr'
Dec 12 22:56:31.864: INFO: stderr: ""
Dec 12 22:56:31.864: INFO: stdout: "e2e-test-crd-publish-openapi-4225-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 12 22:56:31.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 explain e2e-test-crd-publish-openapi-4225-crds'
Dec 12 22:56:32.045: INFO: stderr: ""
Dec 12 22:56:32.045: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4225-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:56:34.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8016" for this suite.

• [SLOW TEST:7.146 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":274,"skipped":4445,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:56:34.950: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Dec 12 22:56:35.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 api-versions'
Dec 12 22:56:35.196: INFO: stderr: ""
Dec 12 22:56:35.196: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:56:35.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-599" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":275,"skipped":4451,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:56:35.208: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5833
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:56:35.419: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 12 22:56:35.432: INFO: Number of nodes with available pods: 0
Dec 12 22:56:35.432: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 12 22:56:35.454: INFO: Number of nodes with available pods: 0
Dec 12 22:56:35.454: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:36.459: INFO: Number of nodes with available pods: 0
Dec 12 22:56:36.459: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:37.459: INFO: Number of nodes with available pods: 0
Dec 12 22:56:37.459: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:38.459: INFO: Number of nodes with available pods: 0
Dec 12 22:56:38.459: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:39.460: INFO: Number of nodes with available pods: 1
Dec 12 22:56:39.460: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 12 22:56:39.481: INFO: Number of nodes with available pods: 1
Dec 12 22:56:39.481: INFO: Number of running nodes: 0, number of available pods: 1
Dec 12 22:56:40.486: INFO: Number of nodes with available pods: 0
Dec 12 22:56:40.486: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 12 22:56:40.505: INFO: Number of nodes with available pods: 0
Dec 12 22:56:40.505: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:41.509: INFO: Number of nodes with available pods: 0
Dec 12 22:56:41.509: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:42.509: INFO: Number of nodes with available pods: 0
Dec 12 22:56:42.509: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:43.510: INFO: Number of nodes with available pods: 0
Dec 12 22:56:43.510: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:44.509: INFO: Number of nodes with available pods: 0
Dec 12 22:56:44.509: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:45.509: INFO: Number of nodes with available pods: 0
Dec 12 22:56:45.509: INFO: Node talos-0-3-0-beta-0-gcp-controlplane-1 is running more than one daemon pod
Dec 12 22:56:46.509: INFO: Number of nodes with available pods: 1
Dec 12 22:56:46.509: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5833, will wait for the garbage collector to delete the pods
Dec 12 22:56:46.577: INFO: Deleting DaemonSet.extensions daemon-set took: 8.930428ms
Dec 12 22:56:46.877: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.216971ms
Dec 12 22:56:56.182: INFO: Number of nodes with available pods: 0
Dec 12 22:56:56.182: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 22:56:56.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5833/daemonsets","resourceVersion":"31756"},"items":null}

Dec 12 22:56:56.188: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5833/pods","resourceVersion":"31756"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:56:56.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5833" for this suite.

• [SLOW TEST:21.078 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":276,"skipped":4466,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:56:56.289: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 12 22:57:01.498: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:57:02.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4342" for this suite.

• [SLOW TEST:6.246 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":277,"skipped":4469,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:57:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Dec 12 22:57:02.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-860659511 version'
Dec 12 22:57:02.869: INFO: stderr: ""
Dec 12 22:57:02.869: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:20:10Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:12:17Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:57:02.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3459" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":278,"skipped":4485,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:57:02.901: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9206
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3099de2c-7ac3-4b3d-84f0-d86f9661d537
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3099de2c-7ac3-4b3d-84f0-d86f9661d537
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:57:09.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9206" for this suite.

• [SLOW TEST:6.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 22:57:09.176: INFO: >>> kubeConfig: /tmp/kubeconfig-860659511
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8941
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Dec 12 22:57:09.349: INFO: Waiting up to 5m0s for pod "client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5" in namespace "containers-8941" to be "success or failure"
Dec 12 22:57:09.353: INFO: Pod "client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184622ms
Dec 12 22:57:11.359: INFO: Pod "client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009770389s
Dec 12 22:57:13.365: INFO: Pod "client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016164854s
STEP: Saw pod success
Dec 12 22:57:13.365: INFO: Pod "client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5" satisfied condition "success or failure"
Dec 12 22:57:13.370: INFO: Trying to get logs from node talos-0-3-0-beta-0-gcp-workers-nt7qc pod client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5 container test-container: <nil>
STEP: delete the pod
Dec 12 22:57:13.409: INFO: Waiting for pod client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5 to disappear
Dec 12 22:57:13.413: INFO: Pod client-containers-d23acdc9-26be-4590-82e1-6647bd3755d5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.0-rc.2.10+70132b0f130acc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 22:57:13.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8941" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4534,"failed":0}
Dec 12 22:57:13.428: INFO: Running AfterSuite actions on all nodes
Dec 12 22:57:13.428: INFO: Running AfterSuite actions on node 1
Dec 12 22:57:13.428: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4534,"failed":0}

Ran 280 of 4814 Specs in 4651.554 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4534 Skipped
PASS

Ginkgo ran 1 suite in 1h17m33.10969369s
Test Suite Passed
