I0609 12:47:54.525476      24 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-503842985
I0609 12:47:54.525553      24 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0609 12:47:54.525710      24 e2e.go:109] Starting e2e run "40ee9106-dde4-44a5-b5bb-60b416b406d7" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1591706872 - Will randomize all specs
Will run 280 of 4843 specs

Jun  9 12:47:54.548: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 12:47:54.556: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun  9 12:47:54.602: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  9 12:47:54.708: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  9 12:47:54.708: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Jun  9 12:47:54.709: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  9 12:47:54.733: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun  9 12:47:54.733: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cert-exporter' (0 seconds elapsed)
Jun  9 12:47:54.733: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'docker-mem-limit-startup-script' (0 seconds elapsed)
Jun  9 12:47:54.733: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun  9 12:47:54.733: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'net-exporter' (0 seconds elapsed)
Jun  9 12:47:54.733: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jun  9 12:47:54.733: INFO: e2e test version: v1.17.6
Jun  9 12:47:54.738: INFO: kube-apiserver version: v1.17.6
Jun  9 12:47:54.738: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 12:47:54.835: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:47:54.840: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
Jun  9 12:47:55.152: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun  9 12:47:55.229: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 12:47:56.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c" in namespace "downward-api-7158" to be "success or failure"
Jun  9 12:47:56.383: INFO: Pod "downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c": Phase="Pending", Reason="", readiness=false. Elapsed: 150.357149ms
Jun  9 12:47:58.400: INFO: Pod "downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16720668s
Jun  9 12:48:00.408: INFO: Pod "downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174853251s
Jun  9 12:48:02.416: INFO: Pod "downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.183012562s
STEP: Saw pod success
Jun  9 12:48:02.416: INFO: Pod "downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c" satisfied condition "success or failure"
Jun  9 12:48:02.421: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c container client-container: <nil>
STEP: delete the pod
Jun  9 12:48:02.462: INFO: Waiting for pod downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c to disappear
Jun  9 12:48:02.468: INFO: Pod downwardapi-volume-4d2fb37c-9c8a-4995-94fe-fe2279bec58c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:48:02.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7158" for this suite.

• [SLOW TEST:7.648 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":1,"skipped":32,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:48:02.499: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 12:48:04.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 12:48:06.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 12:48:08.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 12:48:10.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 12:48:12.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303684, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 12:48:15.170: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:48:15.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2012" for this suite.
STEP: Destroying namespace "webhook-2012-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":2,"skipped":58,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:48:16.008: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jun  9 12:48:16.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=kubectl-8038 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun  9 12:48:23.794: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun  9 12:48:23.795: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:48:25.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8038" for this suite.

• [SLOW TEST:9.813 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1837
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":3,"skipped":60,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:48:25.821: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 12:48:26.035: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun  9 12:48:26.050: INFO: Number of nodes with available pods: 0
Jun  9 12:48:26.050: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun  9 12:48:26.112: INFO: Number of nodes with available pods: 0
Jun  9 12:48:26.113: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:27.120: INFO: Number of nodes with available pods: 0
Jun  9 12:48:27.120: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:28.120: INFO: Number of nodes with available pods: 0
Jun  9 12:48:28.120: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:29.119: INFO: Number of nodes with available pods: 0
Jun  9 12:48:29.119: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:30.121: INFO: Number of nodes with available pods: 0
Jun  9 12:48:30.121: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:31.118: INFO: Number of nodes with available pods: 0
Jun  9 12:48:31.118: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:32.120: INFO: Number of nodes with available pods: 0
Jun  9 12:48:32.120: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:33.119: INFO: Number of nodes with available pods: 0
Jun  9 12:48:33.119: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:34.119: INFO: Number of nodes with available pods: 0
Jun  9 12:48:34.119: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:35.118: INFO: Number of nodes with available pods: 0
Jun  9 12:48:35.118: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:36.122: INFO: Number of nodes with available pods: 0
Jun  9 12:48:36.122: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:37.119: INFO: Number of nodes with available pods: 0
Jun  9 12:48:37.119: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:38.119: INFO: Number of nodes with available pods: 0
Jun  9 12:48:38.119: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:39.121: INFO: Number of nodes with available pods: 0
Jun  9 12:48:39.122: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:40.120: INFO: Number of nodes with available pods: 1
Jun  9 12:48:40.120: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun  9 12:48:40.157: INFO: Number of nodes with available pods: 1
Jun  9 12:48:40.157: INFO: Number of running nodes: 0, number of available pods: 1
Jun  9 12:48:41.164: INFO: Number of nodes with available pods: 0
Jun  9 12:48:41.164: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun  9 12:48:41.187: INFO: Number of nodes with available pods: 0
Jun  9 12:48:41.187: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:42.195: INFO: Number of nodes with available pods: 0
Jun  9 12:48:42.195: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:43.198: INFO: Number of nodes with available pods: 0
Jun  9 12:48:43.198: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:44.196: INFO: Number of nodes with available pods: 0
Jun  9 12:48:44.196: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:45.194: INFO: Number of nodes with available pods: 0
Jun  9 12:48:45.195: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:46.197: INFO: Number of nodes with available pods: 0
Jun  9 12:48:46.197: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:47.198: INFO: Number of nodes with available pods: 0
Jun  9 12:48:47.199: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:48.193: INFO: Number of nodes with available pods: 0
Jun  9 12:48:48.193: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 12:48:49.204: INFO: Number of nodes with available pods: 1
Jun  9 12:48:49.204: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9692, will wait for the garbage collector to delete the pods
Jun  9 12:48:49.277: INFO: Deleting DaemonSet.extensions daemon-set took: 9.106847ms
Jun  9 12:48:50.177: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.515365ms
Jun  9 12:49:06.583: INFO: Number of nodes with available pods: 0
Jun  9 12:49:06.583: INFO: Number of running nodes: 0, number of available pods: 0
Jun  9 12:49:06.589: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9692/daemonsets","resourceVersion":"12684"},"items":null}

Jun  9 12:49:06.593: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9692/pods","resourceVersion":"12684"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:49:06.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9692" for this suite.

• [SLOW TEST:40.844 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":4,"skipped":67,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:49:06.669: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 12:49:06.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679" in namespace "projected-2949" to be "success or failure"
Jun  9 12:49:06.875: INFO: Pod "downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679": Phase="Pending", Reason="", readiness=false. Elapsed: 5.300524ms
Jun  9 12:49:08.882: INFO: Pod "downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012746912s
Jun  9 12:49:10.892: INFO: Pod "downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022715564s
STEP: Saw pod success
Jun  9 12:49:10.892: INFO: Pod "downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679" satisfied condition "success or failure"
Jun  9 12:49:10.898: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679 container client-container: <nil>
STEP: delete the pod
Jun  9 12:49:10.985: INFO: Waiting for pod downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679 to disappear
Jun  9 12:49:10.999: INFO: Pod downwardapi-volume-8f54cb75-d204-49a4-b2d4-89e8d7dc7679 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:49:10.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2949" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":5,"skipped":86,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:49:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-c1134ddd-1e4b-4cbe-a641-8c824857c818 in namespace container-probe-513
Jun  9 12:49:15.283: INFO: Started pod busybox-c1134ddd-1e4b-4cbe-a641-8c824857c818 in namespace container-probe-513
STEP: checking the pod's current state and verifying that restartCount is present
Jun  9 12:49:15.286: INFO: Initial restart count of pod busybox-c1134ddd-1e4b-4cbe-a641-8c824857c818 is 0
Jun  9 12:50:03.503: INFO: Restart count of pod container-probe-513/busybox-c1134ddd-1e4b-4cbe-a641-8c824857c818 is now 1 (48.21611979s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:03.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-513" for this suite.

• [SLOW TEST:52.592 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":6,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:03.625: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 12:50:03.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2" in namespace "projected-698" to be "success or failure"
Jun  9 12:50:03.853: INFO: Pod "downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.959498ms
Jun  9 12:50:05.858: INFO: Pod "downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014786406s
Jun  9 12:50:07.867: INFO: Pod "downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02325414s
STEP: Saw pod success
Jun  9 12:50:07.867: INFO: Pod "downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2" satisfied condition "success or failure"
Jun  9 12:50:07.872: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2 container client-container: <nil>
STEP: delete the pod
Jun  9 12:50:07.913: INFO: Waiting for pod downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2 to disappear
Jun  9 12:50:07.919: INFO: Pod downwardapi-volume-4e08dd27-502a-4783-99f0-053de7a7f5f2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:07.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-698" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":145,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:07.935: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 12:50:09.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 12:50:11.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303809, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303809, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303809, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727303809, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 12:50:14.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:14.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8986" for this suite.
STEP: Destroying namespace "webhook-8986-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.562 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":8,"skipped":162,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6047
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-89f47706-7762-4c23-9d6b-35f79db8941c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:18.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6047" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":166,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:18.889: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun  9 12:50:19.115: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030 /api/v1/namespaces/watch-7030/configmaps/e2e-watch-test-label-changed d88e0366-6a07-4f00-b47d-9cc046c2fe62 13124 0 2020-06-09 12:50:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  9 12:50:19.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030 /api/v1/namespaces/watch-7030/configmaps/e2e-watch-test-label-changed d88e0366-6a07-4f00-b47d-9cc046c2fe62 13125 0 2020-06-09 12:50:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun  9 12:50:19.115: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030 /api/v1/namespaces/watch-7030/configmaps/e2e-watch-test-label-changed d88e0366-6a07-4f00-b47d-9cc046c2fe62 13126 0 2020-06-09 12:50:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun  9 12:50:29.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030 /api/v1/namespaces/watch-7030/configmaps/e2e-watch-test-label-changed d88e0366-6a07-4f00-b47d-9cc046c2fe62 13197 0 2020-06-09 12:50:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  9 12:50:29.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030 /api/v1/namespaces/watch-7030/configmaps/e2e-watch-test-label-changed d88e0366-6a07-4f00-b47d-9cc046c2fe62 13198 0 2020-06-09 12:50:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun  9 12:50:29.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030 /api/v1/namespaces/watch-7030/configmaps/e2e-watch-test-label-changed d88e0366-6a07-4f00-b47d-9cc046c2fe62 13199 0 2020-06-09 12:50:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:29.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7030" for this suite.

• [SLOW TEST:10.307 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":10,"skipped":180,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:29.196: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun  9 12:50:41.453: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:41.457: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:43.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:43.464: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:45.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:45.465: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:47.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:47.469: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:49.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:49.465: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:51.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:51.465: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:53.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:53.467: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:55.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:55.465: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 12:50:57.458: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 12:50:57.464: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:57.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2049" for this suite.

• [SLOW TEST:28.307 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":185,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 12:50:57.742: INFO: (0) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 20.924341ms)
Jun  9 12:50:57.748: INFO: (1) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.957099ms)
Jun  9 12:50:57.754: INFO: (2) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.911933ms)
Jun  9 12:50:57.759: INFO: (3) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.091579ms)
Jun  9 12:50:57.769: INFO: (4) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.885986ms)
Jun  9 12:50:57.778: INFO: (5) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.649537ms)
Jun  9 12:50:57.783: INFO: (6) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.743863ms)
Jun  9 12:50:57.788: INFO: (7) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.168222ms)
Jun  9 12:50:57.795: INFO: (8) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.82928ms)
Jun  9 12:50:57.800: INFO: (9) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.166976ms)
Jun  9 12:50:57.806: INFO: (10) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.038163ms)
Jun  9 12:50:57.821: INFO: (11) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 15.136538ms)
Jun  9 12:50:57.831: INFO: (12) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.343244ms)
Jun  9 12:50:57.838: INFO: (13) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.976081ms)
Jun  9 12:50:57.847: INFO: (14) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.567378ms)
Jun  9 12:50:57.852: INFO: (15) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.092696ms)
Jun  9 12:50:57.859: INFO: (16) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.407419ms)
Jun  9 12:50:57.866: INFO: (17) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.108129ms)
Jun  9 12:50:57.873: INFO: (18) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.256076ms)
Jun  9 12:50:57.880: INFO: (19) /api/v1/nodes/worker-2jqhr-6f5dbbb884-vqc7c/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.397474ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:57.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8866" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":12,"skipped":191,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:57.899: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jun  9 12:50:58.673: INFO: created pod pod-service-account-defaultsa
Jun  9 12:50:58.673: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  9 12:50:58.689: INFO: created pod pod-service-account-mountsa
Jun  9 12:50:58.689: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  9 12:50:58.712: INFO: created pod pod-service-account-nomountsa
Jun  9 12:50:58.712: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  9 12:50:58.746: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  9 12:50:58.746: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  9 12:50:58.766: INFO: created pod pod-service-account-mountsa-mountspec
Jun  9 12:50:58.766: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  9 12:50:58.786: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  9 12:50:58.786: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  9 12:50:58.820: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  9 12:50:58.820: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  9 12:50:58.847: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  9 12:50:58.847: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  9 12:50:58.873: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  9 12:50:58.873: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:50:58.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-871" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":13,"skipped":203,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:50:58.923: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jun  9 12:50:59.153: INFO: Waiting up to 5m0s for pod "var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912" in namespace "var-expansion-446" to be "success or failure"
Jun  9 12:50:59.169: INFO: Pod "var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912": Phase="Pending", Reason="", readiness=false. Elapsed: 15.440423ms
Jun  9 12:51:01.180: INFO: Pod "var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026400552s
Jun  9 12:51:03.211: INFO: Pod "var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057417s
Jun  9 12:51:05.231: INFO: Pod "var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077586341s
STEP: Saw pod success
Jun  9 12:51:05.231: INFO: Pod "var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912" satisfied condition "success or failure"
Jun  9 12:51:05.236: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912 container dapi-container: <nil>
STEP: delete the pod
Jun  9 12:51:05.294: INFO: Waiting for pod var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912 to disappear
Jun  9 12:51:05.299: INFO: Pod var-expansion-17e57327-a46f-4e11-b207-1ad9176f0912 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:51:05.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-446" for this suite.

• [SLOW TEST:6.398 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":219,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:51:05.320: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jun  9 12:51:05.505: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jun  9 12:51:05.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-9482'
Jun  9 12:51:05.976: INFO: stderr: ""
Jun  9 12:51:05.976: INFO: stdout: "service/agnhost-slave created\n"
Jun  9 12:51:05.976: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jun  9 12:51:05.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-9482'
Jun  9 12:51:06.300: INFO: stderr: ""
Jun  9 12:51:06.301: INFO: stdout: "service/agnhost-master created\n"
Jun  9 12:51:06.304: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  9 12:51:06.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-9482'
Jun  9 12:51:06.656: INFO: stderr: ""
Jun  9 12:51:06.656: INFO: stdout: "service/frontend created\n"
Jun  9 12:51:06.658: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun  9 12:51:06.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-9482'
Jun  9 12:51:07.097: INFO: stderr: ""
Jun  9 12:51:07.098: INFO: stdout: "deployment.apps/frontend created\n"
Jun  9 12:51:07.098: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  9 12:51:07.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-9482'
Jun  9 12:51:07.386: INFO: stderr: ""
Jun  9 12:51:07.386: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jun  9 12:51:07.386: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  9 12:51:07.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-9482'
Jun  9 12:51:07.690: INFO: stderr: ""
Jun  9 12:51:07.690: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jun  9 12:51:07.690: INFO: Waiting for all frontend pods to be Running.
Jun  9 12:51:17.743: INFO: Waiting for frontend to serve content.
Jun  9 12:51:17.767: INFO: Trying to add a new entry to the guestbook.
Jun  9 12:51:17.794: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun  9 12:51:17.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-9482'
Jun  9 12:51:17.955: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:51:17.955: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun  9 12:51:17.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-9482'
Jun  9 12:51:18.108: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:51:18.110: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jun  9 12:51:18.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-9482'
Jun  9 12:51:18.299: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:51:18.299: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  9 12:51:18.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-9482'
Jun  9 12:51:18.438: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:51:18.439: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  9 12:51:18.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-9482'
Jun  9 12:51:18.588: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:51:18.588: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jun  9 12:51:18.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-9482'
Jun  9 12:51:18.731: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:51:18.732: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:51:18.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9482" for this suite.

• [SLOW TEST:13.446 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":15,"skipped":233,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:51:18.767: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:51:19.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3507" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":16,"skipped":246,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:51:19.090: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 12:51:19.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2829'
Jun  9 12:51:19.646: INFO: stderr: ""
Jun  9 12:51:19.646: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun  9 12:51:34.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pod e2e-test-httpd-pod --namespace=kubectl-2829 -o json'
Jun  9 12:51:34.831: INFO: stderr: ""
Jun  9 12:51:34.831: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"cert-exporter-psp\"\n        },\n        \"creationTimestamp\": \"2020-06-09T12:51:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2829\",\n        \"resourceVersion\": \"13939\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2829/pods/e2e-test-httpd-pod\",\n        \"uid\": \"eeb1d148-608f-427a-9f10-fc608e78b67a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jq4fm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-dfhc8-64bc8fc496-xx7cx\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jq4fm\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jq4fm\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-09T12:51:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-09T12:51:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-09T12:51:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-09T12:51:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2caf00a9f07310672f61999e96e7455ad7bb772f4296153099e2a2c61817facd\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-06-09T12:51:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.23.24.46\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.24.160.75\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.24.160.75\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-06-09T12:51:19Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun  9 12:51:34.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 replace -f - --namespace=kubectl-2829'
Jun  9 12:51:35.127: INFO: stderr: ""
Jun  9 12:51:35.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Jun  9 12:51:35.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete pods e2e-test-httpd-pod --namespace=kubectl-2829'
Jun  9 12:51:43.710: INFO: stderr: ""
Jun  9 12:51:43.710: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:51:43.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2829" for this suite.

• [SLOW TEST:24.633 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":17,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:51:43.724: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun  9 12:51:54.063: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:51:54.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0609 12:51:54.063281      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2999" for this suite.

• [SLOW TEST:10.355 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":18,"skipped":280,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:51:54.084: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  9 12:51:54.351: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:51:54.360: INFO: Number of nodes with available pods: 0
Jun  9 12:51:54.360: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:51:55.372: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:51:55.393: INFO: Number of nodes with available pods: 0
Jun  9 12:51:55.393: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:51:56.371: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:51:56.382: INFO: Number of nodes with available pods: 0
Jun  9 12:51:56.382: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:51:57.373: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:51:57.380: INFO: Number of nodes with available pods: 2
Jun  9 12:51:57.380: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:51:58.370: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:51:58.376: INFO: Number of nodes with available pods: 2
Jun  9 12:51:58.376: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:51:59.382: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:51:59.390: INFO: Number of nodes with available pods: 2
Jun  9 12:51:59.390: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:00.368: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:00.379: INFO: Number of nodes with available pods: 2
Jun  9 12:52:00.379: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:01.370: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:01.374: INFO: Number of nodes with available pods: 2
Jun  9 12:52:01.374: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:02.372: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:02.379: INFO: Number of nodes with available pods: 2
Jun  9 12:52:02.379: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:03.405: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:03.410: INFO: Number of nodes with available pods: 2
Jun  9 12:52:03.410: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:04.369: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:04.375: INFO: Number of nodes with available pods: 2
Jun  9 12:52:04.375: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:05.368: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:05.373: INFO: Number of nodes with available pods: 2
Jun  9 12:52:05.373: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 12:52:06.370: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:06.376: INFO: Number of nodes with available pods: 3
Jun  9 12:52:06.376: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun  9 12:52:06.403: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 12:52:06.411: INFO: Number of nodes with available pods: 3
Jun  9 12:52:06.411: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4094, will wait for the garbage collector to delete the pods
Jun  9 12:52:07.501: INFO: Deleting DaemonSet.extensions daemon-set took: 14.177952ms
Jun  9 12:52:08.402: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.95384ms
Jun  9 12:52:26.508: INFO: Number of nodes with available pods: 0
Jun  9 12:52:26.508: INFO: Number of running nodes: 0, number of available pods: 0
Jun  9 12:52:26.512: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4094/daemonsets","resourceVersion":"14470"},"items":null}

Jun  9 12:52:26.516: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4094/pods","resourceVersion":"14470"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:26.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4094" for this suite.

• [SLOW TEST:32.504 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":19,"skipped":288,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  9 12:52:29.383: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e66bc14a-9b2b-4720-840d-74a5f2555811"
Jun  9 12:52:29.383: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e66bc14a-9b2b-4720-840d-74a5f2555811" in namespace "pods-3494" to be "terminated due to deadline exceeded"
Jun  9 12:52:29.387: INFO: Pod "pod-update-activedeadlineseconds-e66bc14a-9b2b-4720-840d-74a5f2555811": Phase="Running", Reason="", readiness=true. Elapsed: 3.775766ms
Jun  9 12:52:31.396: INFO: Pod "pod-update-activedeadlineseconds-e66bc14a-9b2b-4720-840d-74a5f2555811": Phase="Running", Reason="", readiness=true. Elapsed: 2.013074805s
Jun  9 12:52:33.404: INFO: Pod "pod-update-activedeadlineseconds-e66bc14a-9b2b-4720-840d-74a5f2555811": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.020848638s
Jun  9 12:52:33.404: INFO: Pod "pod-update-activedeadlineseconds-e66bc14a-9b2b-4720-840d-74a5f2555811" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:33.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3494" for this suite.

• [SLOW TEST:6.836 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":20,"skipped":295,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:33.426: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 12:52:33.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3" in namespace "downward-api-3582" to be "success or failure"
Jun  9 12:52:33.659: INFO: Pod "downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.918598ms
Jun  9 12:52:35.666: INFO: Pod "downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020116675s
Jun  9 12:52:37.674: INFO: Pod "downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028244269s
Jun  9 12:52:39.684: INFO: Pod "downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037488303s
STEP: Saw pod success
Jun  9 12:52:39.684: INFO: Pod "downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3" satisfied condition "success or failure"
Jun  9 12:52:39.688: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3 container client-container: <nil>
STEP: delete the pod
Jun  9 12:52:39.744: INFO: Waiting for pod downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3 to disappear
Jun  9 12:52:39.755: INFO: Pod downwardapi-volume-b8a020c6-8e1d-4554-8f0d-497ce0a5d7a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:39.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3582" for this suite.

• [SLOW TEST:6.353 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":21,"skipped":312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jun  9 12:52:39.995: INFO: Waiting up to 5m0s for pod "client-containers-cb03892e-d96a-441d-804f-db0825d21095" in namespace "containers-7740" to be "success or failure"
Jun  9 12:52:40.001: INFO: Pod "client-containers-cb03892e-d96a-441d-804f-db0825d21095": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069037ms
Jun  9 12:52:42.009: INFO: Pod "client-containers-cb03892e-d96a-441d-804f-db0825d21095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013892576s
Jun  9 12:52:44.017: INFO: Pod "client-containers-cb03892e-d96a-441d-804f-db0825d21095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02188537s
STEP: Saw pod success
Jun  9 12:52:44.018: INFO: Pod "client-containers-cb03892e-d96a-441d-804f-db0825d21095" satisfied condition "success or failure"
Jun  9 12:52:44.023: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod client-containers-cb03892e-d96a-441d-804f-db0825d21095 container test-container: <nil>
STEP: delete the pod
Jun  9 12:52:44.077: INFO: Waiting for pod client-containers-cb03892e-d96a-441d-804f-db0825d21095 to disappear
Jun  9 12:52:44.097: INFO: Pod client-containers-cb03892e-d96a-441d-804f-db0825d21095 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7740" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:55.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2866" for this suite.

• [SLOW TEST:11.311 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":23,"skipped":357,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jun  9 12:52:55.649: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-438" to be "success or failure"
Jun  9 12:52:55.666: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 16.435187ms
Jun  9 12:52:57.676: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026000827s
Jun  9 12:52:59.685: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03555498s
STEP: Saw pod success
Jun  9 12:52:59.685: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun  9 12:52:59.689: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun  9 12:52:59.716: INFO: Waiting for pod pod-host-path-test to disappear
Jun  9 12:52:59.721: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:59.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-438" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":366,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:59.737: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:52:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6253" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":25,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:52:59.940: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6401
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jun  9 12:53:00.184: INFO: Waiting up to 5m0s for pod "pod-c2eee172-5684-41ad-8685-24fe0547fb93" in namespace "emptydir-6401" to be "success or failure"
Jun  9 12:53:00.190: INFO: Pod "pod-c2eee172-5684-41ad-8685-24fe0547fb93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666159ms
Jun  9 12:53:02.200: INFO: Pod "pod-c2eee172-5684-41ad-8685-24fe0547fb93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015286486s
Jun  9 12:53:04.208: INFO: Pod "pod-c2eee172-5684-41ad-8685-24fe0547fb93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023472347s
STEP: Saw pod success
Jun  9 12:53:04.208: INFO: Pod "pod-c2eee172-5684-41ad-8685-24fe0547fb93" satisfied condition "success or failure"
Jun  9 12:53:04.214: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-c2eee172-5684-41ad-8685-24fe0547fb93 container test-container: <nil>
STEP: delete the pod
Jun  9 12:53:04.246: INFO: Waiting for pod pod-c2eee172-5684-41ad-8685-24fe0547fb93 to disappear
Jun  9 12:53:04.257: INFO: Pod pod-c2eee172-5684-41ad-8685-24fe0547fb93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:04.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6401" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:04.278: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jun  9 12:53:08.488: INFO: Pod pod-hostip-fdc3edc2-7e5e-42f0-ad39-09ca5ffb52fd has hostIP: 172.23.24.246
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:08.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4472" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":414,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:08.513: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9232
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4055
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:25.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4674" for this suite.
STEP: Destroying namespace "nsdeletetest-9232" for this suite.
Jun  9 12:53:25.207: INFO: Namespace nsdeletetest-9232 was already deleted
STEP: Destroying namespace "nsdeletetest-4055" for this suite.

• [SLOW TEST:16.708 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":28,"skipped":423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:25.222: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun  9 12:53:25.400: INFO: Waiting up to 5m0s for pod "downward-api-80a1f930-d18d-47d5-b979-1388a7514bee" in namespace "downward-api-6675" to be "success or failure"
Jun  9 12:53:25.404: INFO: Pod "downward-api-80a1f930-d18d-47d5-b979-1388a7514bee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752057ms
Jun  9 12:53:27.410: INFO: Pod "downward-api-80a1f930-d18d-47d5-b979-1388a7514bee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010064912s
Jun  9 12:53:29.421: INFO: Pod "downward-api-80a1f930-d18d-47d5-b979-1388a7514bee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020527467s
STEP: Saw pod success
Jun  9 12:53:29.421: INFO: Pod "downward-api-80a1f930-d18d-47d5-b979-1388a7514bee" satisfied condition "success or failure"
Jun  9 12:53:29.426: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downward-api-80a1f930-d18d-47d5-b979-1388a7514bee container dapi-container: <nil>
STEP: delete the pod
Jun  9 12:53:29.479: INFO: Waiting for pod downward-api-80a1f930-d18d-47d5-b979-1388a7514bee to disappear
Jun  9 12:53:29.484: INFO: Pod downward-api-80a1f930-d18d-47d5-b979-1388a7514bee no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:29.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6675" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":488,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:29.499: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0609 12:53:35.792038      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun  9 12:53:35.792: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:35.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5763" for this suite.

• [SLOW TEST:6.355 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":30,"skipped":488,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:35.870: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:47.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9829" for this suite.

• [SLOW TEST:11.444 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":31,"skipped":503,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:47.315: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:53:52.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1640" for this suite.

• [SLOW TEST:5.296 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":32,"skipped":510,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:53:52.612: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-6686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jun  9 12:53:52.840: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 12:54:52.884: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 12:54:52.899: INFO: Starting informer...
STEP: Starting pods...
Jun  9 12:54:53.144: INFO: Pod1 is running on worker-k8xcg-8bbfd5b68-w4htb. Tainting Node
Jun  9 12:54:57.396: INFO: Pod2 is running on worker-k8xcg-8bbfd5b68-w4htb. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun  9 12:55:05.260: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun  9 12:55:26.468: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:55:26.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6686" for this suite.

• [SLOW TEST:93.918 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":33,"skipped":519,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:55:26.530: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 12:55:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1760
I0609 12:55:26.791599      24 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1760, replica count: 1
I0609 12:55:27.842784      24 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 12:55:28.843421      24 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 12:55:29.844036      24 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 12:55:29.966: INFO: Created: latency-svc-vs2wr
Jun  9 12:55:30.014: INFO: Got endpoints: latency-svc-vs2wr [69.716969ms]
Jun  9 12:55:30.062: INFO: Created: latency-svc-nsrdg
Jun  9 12:55:30.066: INFO: Created: latency-svc-dx5qt
Jun  9 12:55:30.110: INFO: Created: latency-svc-44rhr
Jun  9 12:55:30.136: INFO: Got endpoints: latency-svc-dx5qt [119.539301ms]
Jun  9 12:55:30.136: INFO: Got endpoints: latency-svc-nsrdg [121.44783ms]
Jun  9 12:55:30.156: INFO: Created: latency-svc-l2jrd
Jun  9 12:55:30.163: INFO: Got endpoints: latency-svc-44rhr [146.384273ms]
Jun  9 12:55:30.196: INFO: Got endpoints: latency-svc-l2jrd [180.437288ms]
Jun  9 12:55:30.210: INFO: Created: latency-svc-7kcqt
Jun  9 12:55:30.237: INFO: Created: latency-svc-bmztn
Jun  9 12:55:30.252: INFO: Got endpoints: latency-svc-bmztn [236.476008ms]
Jun  9 12:55:30.254: INFO: Got endpoints: latency-svc-7kcqt [237.854423ms]
Jun  9 12:55:30.292: INFO: Created: latency-svc-l26f2
Jun  9 12:55:30.297: INFO: Created: latency-svc-lfhfk
Jun  9 12:55:30.322: INFO: Created: latency-svc-68d8r
Jun  9 12:55:30.329: INFO: Created: latency-svc-pznvk
Jun  9 12:55:30.356: INFO: Created: latency-svc-lmzvp
Jun  9 12:55:30.367: INFO: Got endpoints: latency-svc-lfhfk [350.630049ms]
Jun  9 12:55:30.367: INFO: Got endpoints: latency-svc-l26f2 [351.134768ms]
Jun  9 12:55:30.375: INFO: Created: latency-svc-94fpc
Jun  9 12:55:30.385: INFO: Created: latency-svc-xhn5k
Jun  9 12:55:30.418: INFO: Got endpoints: latency-svc-68d8r [402.578747ms]
Jun  9 12:55:30.461: INFO: Got endpoints: latency-svc-lmzvp [444.677144ms]
Jun  9 12:55:30.477: INFO: Created: latency-svc-755zt
Jun  9 12:55:30.483: INFO: Got endpoints: latency-svc-94fpc [466.532571ms]
Jun  9 12:55:30.484: INFO: Got endpoints: latency-svc-xhn5k [467.340864ms]
Jun  9 12:55:30.484: INFO: Got endpoints: latency-svc-pznvk [467.535785ms]
Jun  9 12:55:30.497: INFO: Created: latency-svc-cbf56
Jun  9 12:55:30.515: INFO: Got endpoints: latency-svc-755zt [498.425338ms]
Jun  9 12:55:30.541: INFO: Got endpoints: latency-svc-cbf56 [527.175582ms]
Jun  9 12:55:30.547: INFO: Created: latency-svc-g2sln
Jun  9 12:55:30.568: INFO: Got endpoints: latency-svc-g2sln [432.424049ms]
Jun  9 12:55:30.572: INFO: Created: latency-svc-g4p4m
Jun  9 12:55:30.584: INFO: Created: latency-svc-nzt7h
Jun  9 12:55:30.592: INFO: Got endpoints: latency-svc-g4p4m [455.876905ms]
Jun  9 12:55:30.604: INFO: Created: latency-svc-vvs72
Jun  9 12:55:30.607: INFO: Got endpoints: latency-svc-nzt7h [444.443629ms]
Jun  9 12:55:30.614: INFO: Created: latency-svc-6r74g
Jun  9 12:55:30.636: INFO: Created: latency-svc-p48wb
Jun  9 12:55:30.654: INFO: Got endpoints: latency-svc-vvs72 [456.980747ms]
Jun  9 12:55:30.663: INFO: Created: latency-svc-p8d69
Jun  9 12:55:30.715: INFO: Got endpoints: latency-svc-6r74g [462.650216ms]
Jun  9 12:55:30.717: INFO: Got endpoints: latency-svc-p8d69 [350.287631ms]
Jun  9 12:55:30.718: INFO: Got endpoints: latency-svc-p48wb [463.844119ms]
Jun  9 12:55:30.736: INFO: Created: latency-svc-fsv9c
Jun  9 12:55:30.756: INFO: Created: latency-svc-zzvkj
Jun  9 12:55:30.770: INFO: Got endpoints: latency-svc-fsv9c [401.362534ms]
Jun  9 12:55:30.782: INFO: Created: latency-svc-v5qzh
Jun  9 12:55:30.804: INFO: Got endpoints: latency-svc-zzvkj [385.760891ms]
Jun  9 12:55:30.830: INFO: Got endpoints: latency-svc-v5qzh [369.077086ms]
Jun  9 12:55:30.834: INFO: Created: latency-svc-g26sr
Jun  9 12:55:30.834: INFO: Created: latency-svc-msgdd
Jun  9 12:55:30.842: INFO: Got endpoints: latency-svc-msgdd [71.912017ms]
Jun  9 12:55:30.855: INFO: Got endpoints: latency-svc-g26sr [372.341619ms]
Jun  9 12:55:30.868: INFO: Created: latency-svc-xmvvv
Jun  9 12:55:30.869: INFO: Got endpoints: latency-svc-xmvvv [384.648108ms]
Jun  9 12:55:30.871: INFO: Created: latency-svc-5qsss
Jun  9 12:55:30.880: INFO: Created: latency-svc-62jmr
Jun  9 12:55:30.890: INFO: Created: latency-svc-qhkh6
Jun  9 12:55:30.897: INFO: Got endpoints: latency-svc-5qsss [412.730089ms]
Jun  9 12:55:30.899: INFO: Got endpoints: latency-svc-62jmr [384.117553ms]
Jun  9 12:55:30.921: INFO: Got endpoints: latency-svc-qhkh6 [379.680844ms]
Jun  9 12:55:30.963: INFO: Created: latency-svc-jtsqk
Jun  9 12:55:30.980: INFO: Created: latency-svc-l5wjd
Jun  9 12:55:30.987: INFO: Created: latency-svc-tzs9k
Jun  9 12:55:30.993: INFO: Got endpoints: latency-svc-jtsqk [424.918542ms]
Jun  9 12:55:31.002: INFO: Got endpoints: latency-svc-tzs9k [394.494469ms]
Jun  9 12:55:31.004: INFO: Got endpoints: latency-svc-l5wjd [411.977065ms]
Jun  9 12:55:31.018: INFO: Created: latency-svc-52kmk
Jun  9 12:55:31.033: INFO: Created: latency-svc-84b5l
Jun  9 12:55:31.071: INFO: Got endpoints: latency-svc-52kmk [416.909024ms]
Jun  9 12:55:31.080: INFO: Created: latency-svc-8pdrv
Jun  9 12:55:31.081: INFO: Got endpoints: latency-svc-84b5l [366.150827ms]
Jun  9 12:55:31.090: INFO: Created: latency-svc-cp9hz
Jun  9 12:55:31.091: INFO: Created: latency-svc-9v8fc
Jun  9 12:55:31.107: INFO: Got endpoints: latency-svc-8pdrv [388.931109ms]
Jun  9 12:55:31.156: INFO: Created: latency-svc-2rv8m
Jun  9 12:55:31.163: INFO: Got endpoints: latency-svc-9v8fc [359.175309ms]
Jun  9 12:55:31.164: INFO: Got endpoints: latency-svc-cp9hz [445.431507ms]
Jun  9 12:55:31.184: INFO: Created: latency-svc-bpqfp
Jun  9 12:55:31.204: INFO: Created: latency-svc-ktmnz
Jun  9 12:55:31.217: INFO: Got endpoints: latency-svc-bpqfp [373.911167ms]
Jun  9 12:55:31.220: INFO: Got endpoints: latency-svc-2rv8m [389.572749ms]
Jun  9 12:55:31.238: INFO: Created: latency-svc-z9d6h
Jun  9 12:55:31.255: INFO: Got endpoints: latency-svc-ktmnz [399.236243ms]
Jun  9 12:55:31.255: INFO: Got endpoints: latency-svc-z9d6h [385.760355ms]
Jun  9 12:55:31.282: INFO: Created: latency-svc-whbf8
Jun  9 12:55:31.285: INFO: Created: latency-svc-qw6rl
Jun  9 12:55:31.313: INFO: Got endpoints: latency-svc-whbf8 [416.657654ms]
Jun  9 12:55:31.315: INFO: Got endpoints: latency-svc-qw6rl [414.623764ms]
Jun  9 12:55:31.346: INFO: Created: latency-svc-c8b72
Jun  9 12:55:31.349: INFO: Created: latency-svc-2dxk9
Jun  9 12:55:31.352: INFO: Created: latency-svc-r8lbr
Jun  9 12:55:31.384: INFO: Got endpoints: latency-svc-2dxk9 [390.205696ms]
Jun  9 12:55:31.392: INFO: Created: latency-svc-zlhht
Jun  9 12:55:31.401: INFO: Created: latency-svc-c5nft
Jun  9 12:55:31.409: INFO: Got endpoints: latency-svc-c8b72 [487.536116ms]
Jun  9 12:55:31.409: INFO: Got endpoints: latency-svc-r8lbr [406.734266ms]
Jun  9 12:55:31.428: INFO: Got endpoints: latency-svc-zlhht [423.701353ms]
Jun  9 12:55:31.440: INFO: Created: latency-svc-dlsh8
Jun  9 12:55:31.455: INFO: Got endpoints: latency-svc-c5nft [384.430531ms]
Jun  9 12:55:31.488: INFO: Created: latency-svc-cglx8
Jun  9 12:55:31.502: INFO: Got endpoints: latency-svc-dlsh8 [417.911938ms]
Jun  9 12:55:31.525: INFO: Got endpoints: latency-svc-cglx8 [417.652523ms]
Jun  9 12:55:31.552: INFO: Created: latency-svc-96n7l
Jun  9 12:55:31.578: INFO: Created: latency-svc-56jnr
Jun  9 12:55:31.610: INFO: Got endpoints: latency-svc-96n7l [446.84306ms]
Jun  9 12:55:31.618: INFO: Got endpoints: latency-svc-56jnr [454.671637ms]
Jun  9 12:55:31.658: INFO: Created: latency-svc-fw9rb
Jun  9 12:55:31.680: INFO: Created: latency-svc-55wc9
Jun  9 12:55:31.698: INFO: Created: latency-svc-twb24
Jun  9 12:55:31.699: INFO: Got endpoints: latency-svc-fw9rb [482.376704ms]
Jun  9 12:55:31.733: INFO: Got endpoints: latency-svc-twb24 [478.072358ms]
Jun  9 12:55:31.734: INFO: Got endpoints: latency-svc-55wc9 [514.389803ms]
Jun  9 12:55:31.775: INFO: Created: latency-svc-zsszv
Jun  9 12:55:31.792: INFO: Created: latency-svc-hfgst
Jun  9 12:55:31.812: INFO: Got endpoints: latency-svc-hfgst [498.251882ms]
Jun  9 12:55:31.812: INFO: Got endpoints: latency-svc-zsszv [555.008356ms]
Jun  9 12:55:31.855: INFO: Created: latency-svc-sh7d5
Jun  9 12:55:31.920: INFO: Got endpoints: latency-svc-sh7d5 [605.246949ms]
Jun  9 12:55:31.946: INFO: Created: latency-svc-s2pp7
Jun  9 12:55:31.961: INFO: Created: latency-svc-nc95j
Jun  9 12:55:31.968: INFO: Got endpoints: latency-svc-s2pp7 [583.560857ms]
Jun  9 12:55:31.973: INFO: Created: latency-svc-bmcv9
Jun  9 12:55:31.991: INFO: Created: latency-svc-l7gvc
Jun  9 12:55:31.991: INFO: Got endpoints: latency-svc-nc95j [581.426122ms]
Jun  9 12:55:32.016: INFO: Got endpoints: latency-svc-bmcv9 [607.456421ms]
Jun  9 12:55:32.018: INFO: Created: latency-svc-k7qpt
Jun  9 12:55:32.040: INFO: Got endpoints: latency-svc-l7gvc [610.944502ms]
Jun  9 12:55:32.046: INFO: Got endpoints: latency-svc-k7qpt [588.698104ms]
Jun  9 12:55:32.049: INFO: Created: latency-svc-mg2ds
Jun  9 12:55:32.062: INFO: Got endpoints: latency-svc-mg2ds [560.23054ms]
Jun  9 12:55:32.077: INFO: Created: latency-svc-djtv7
Jun  9 12:55:32.088: INFO: Created: latency-svc-m42lc
Jun  9 12:55:32.090: INFO: Got endpoints: latency-svc-djtv7 [564.784465ms]
Jun  9 12:55:32.123: INFO: Got endpoints: latency-svc-m42lc [512.85888ms]
Jun  9 12:55:32.138: INFO: Created: latency-svc-x5pjx
Jun  9 12:55:32.152: INFO: Created: latency-svc-hwcqv
Jun  9 12:55:32.159: INFO: Created: latency-svc-rpk7b
Jun  9 12:55:32.167: INFO: Got endpoints: latency-svc-x5pjx [548.619217ms]
Jun  9 12:55:32.188: INFO: Got endpoints: latency-svc-rpk7b [452.379554ms]
Jun  9 12:55:32.192: INFO: Created: latency-svc-fm7wq
Jun  9 12:55:32.203: INFO: Got endpoints: latency-svc-hwcqv [502.961474ms]
Jun  9 12:55:32.209: INFO: Got endpoints: latency-svc-fm7wq [472.699907ms]
Jun  9 12:55:32.223: INFO: Created: latency-svc-mbvks
Jun  9 12:55:32.250: INFO: Created: latency-svc-5nhq6
Jun  9 12:55:32.270: INFO: Created: latency-svc-jg2zn
Jun  9 12:55:32.292: INFO: Got endpoints: latency-svc-mbvks [480.315988ms]
Jun  9 12:55:32.322: INFO: Created: latency-svc-wmxn9
Jun  9 12:55:32.357: INFO: Got endpoints: latency-svc-5nhq6 [543.253954ms]
Jun  9 12:55:32.374: INFO: Got endpoints: latency-svc-jg2zn [453.819235ms]
Jun  9 12:55:32.404: INFO: Created: latency-svc-spzch
Jun  9 12:55:32.442: INFO: Got endpoints: latency-svc-wmxn9 [474.213695ms]
Jun  9 12:55:32.445: INFO: Created: latency-svc-d26qk
Jun  9 12:55:32.496: INFO: Created: latency-svc-9vssd
Jun  9 12:55:32.496: INFO: Got endpoints: latency-svc-spzch [504.699763ms]
Jun  9 12:55:32.498: INFO: Created: latency-svc-c5wmf
Jun  9 12:55:32.498: INFO: Created: latency-svc-rrnt9
Jun  9 12:55:32.520: INFO: Created: latency-svc-fs2hs
Jun  9 12:55:32.523: INFO: Got endpoints: latency-svc-d26qk [506.267952ms]
Jun  9 12:55:32.570: INFO: Got endpoints: latency-svc-rrnt9 [530.142389ms]
Jun  9 12:55:32.570: INFO: Created: latency-svc-bvlnz
Jun  9 12:55:32.586: INFO: Created: latency-svc-fbvtz
Jun  9 12:55:32.617: INFO: Created: latency-svc-92vdg
Jun  9 12:55:32.622: INFO: Got endpoints: latency-svc-9vssd [574.041347ms]
Jun  9 12:55:32.649: INFO: Created: latency-svc-mns5w
Jun  9 12:55:32.655: INFO: Got endpoints: latency-svc-c5wmf [592.967933ms]
Jun  9 12:55:32.665: INFO: Created: latency-svc-cmw98
Jun  9 12:55:32.690: INFO: Created: latency-svc-x7svv
Jun  9 12:55:32.709: INFO: Got endpoints: latency-svc-fs2hs [619.053572ms]
Jun  9 12:55:32.712: INFO: Created: latency-svc-hvfvd
Jun  9 12:55:32.718: INFO: Created: latency-svc-gqk5p
Jun  9 12:55:32.741: INFO: Created: latency-svc-v98nq
Jun  9 12:55:32.750: INFO: Got endpoints: latency-svc-bvlnz [627.185567ms]
Jun  9 12:55:32.759: INFO: Created: latency-svc-bz5xx
Jun  9 12:55:32.836: INFO: Got endpoints: latency-svc-fbvtz [668.141871ms]
Jun  9 12:55:32.839: INFO: Created: latency-svc-bvjvf
Jun  9 12:55:32.839: INFO: Created: latency-svc-4b6r5
Jun  9 12:55:32.871: INFO: Created: latency-svc-9gxqs
Jun  9 12:55:32.873: INFO: Got endpoints: latency-svc-92vdg [684.789834ms]
Jun  9 12:55:32.885: INFO: Created: latency-svc-vzhmb
Jun  9 12:55:32.895: INFO: Created: latency-svc-hxcld
Jun  9 12:55:32.896: INFO: Created: latency-svc-czk2j
Jun  9 12:55:32.909: INFO: Got endpoints: latency-svc-mns5w [700.313126ms]
Jun  9 12:55:32.963: INFO: Created: latency-svc-vtb2h
Jun  9 12:55:32.969: INFO: Got endpoints: latency-svc-cmw98 [765.620701ms]
Jun  9 12:55:32.997: INFO: Created: latency-svc-898rq
Jun  9 12:55:33.000: INFO: Created: latency-svc-n8dqs
Jun  9 12:55:33.010: INFO: Got endpoints: latency-svc-x7svv [717.470507ms]
Jun  9 12:55:33.031: INFO: Created: latency-svc-sv9g4
Jun  9 12:55:33.043: INFO: Created: latency-svc-82kdp
Jun  9 12:55:33.050: INFO: Got endpoints: latency-svc-hvfvd [692.773409ms]
Jun  9 12:55:33.098: INFO: Created: latency-svc-w9kx7
Jun  9 12:55:33.113: INFO: Got endpoints: latency-svc-gqk5p [738.439076ms]
Jun  9 12:55:33.147: INFO: Created: latency-svc-nmlxt
Jun  9 12:55:33.156: INFO: Got endpoints: latency-svc-v98nq [713.657035ms]
Jun  9 12:55:33.198: INFO: Created: latency-svc-b69z8
Jun  9 12:55:33.206: INFO: Got endpoints: latency-svc-bz5xx [709.659069ms]
Jun  9 12:55:33.229: INFO: Created: latency-svc-9ng7b
Jun  9 12:55:33.274: INFO: Got endpoints: latency-svc-4b6r5 [751.425332ms]
Jun  9 12:55:33.304: INFO: Got endpoints: latency-svc-bvjvf [733.55589ms]
Jun  9 12:55:33.329: INFO: Created: latency-svc-czd4g
Jun  9 12:55:33.379: INFO: Created: latency-svc-hsf4l
Jun  9 12:55:33.401: INFO: Got endpoints: latency-svc-hxcld [778.85105ms]
Jun  9 12:55:33.443: INFO: Got endpoints: latency-svc-9gxqs [787.745145ms]
Jun  9 12:55:33.477: INFO: Created: latency-svc-w7ch9
Jun  9 12:55:33.505: INFO: Got endpoints: latency-svc-vzhmb [795.467074ms]
Jun  9 12:55:33.509: INFO: Got endpoints: latency-svc-czk2j [758.646226ms]
Jun  9 12:55:33.557: INFO: Created: latency-svc-tbp2h
Jun  9 12:55:33.560: INFO: Created: latency-svc-w2b7w
Jun  9 12:55:33.579: INFO: Got endpoints: latency-svc-vtb2h [743.173612ms]
Jun  9 12:55:33.603: INFO: Created: latency-svc-pc9vh
Jun  9 12:55:33.625: INFO: Got endpoints: latency-svc-898rq [752.289384ms]
Jun  9 12:55:33.660: INFO: Got endpoints: latency-svc-n8dqs [750.73321ms]
Jun  9 12:55:33.716: INFO: Created: latency-svc-zfrmd
Jun  9 12:55:33.737: INFO: Got endpoints: latency-svc-sv9g4 [767.532034ms]
Jun  9 12:55:33.767: INFO: Got endpoints: latency-svc-82kdp [757.168922ms]
Jun  9 12:55:33.777: INFO: Created: latency-svc-xvl7g
Jun  9 12:55:33.788: INFO: Created: latency-svc-bbx5t
Jun  9 12:55:33.823: INFO: Created: latency-svc-64zv5
Jun  9 12:55:33.824: INFO: Got endpoints: latency-svc-w9kx7 [773.663478ms]
Jun  9 12:55:33.837: INFO: Created: latency-svc-6q7bp
Jun  9 12:55:33.897: INFO: Got endpoints: latency-svc-nmlxt [784.424846ms]
Jun  9 12:55:33.929: INFO: Got endpoints: latency-svc-b69z8 [773.409736ms]
Jun  9 12:55:33.930: INFO: Created: latency-svc-tx9z9
Jun  9 12:55:33.953: INFO: Created: latency-svc-469zc
Jun  9 12:55:33.970: INFO: Got endpoints: latency-svc-9ng7b [764.30803ms]
Jun  9 12:55:34.022: INFO: Created: latency-svc-9495n
Jun  9 12:55:34.033: INFO: Got endpoints: latency-svc-czd4g [758.758871ms]
Jun  9 12:55:34.066: INFO: Created: latency-svc-kf59s
Jun  9 12:55:34.069: INFO: Got endpoints: latency-svc-hsf4l [765.018832ms]
Jun  9 12:55:34.080: INFO: Created: latency-svc-d4dr4
Jun  9 12:55:34.101: INFO: Got endpoints: latency-svc-w7ch9 [700.242781ms]
Jun  9 12:55:34.120: INFO: Created: latency-svc-5fllz
Jun  9 12:55:34.144: INFO: Created: latency-svc-jddzz
Jun  9 12:55:34.156: INFO: Got endpoints: latency-svc-w2b7w [712.862866ms]
Jun  9 12:55:34.212: INFO: Got endpoints: latency-svc-tbp2h [707.082562ms]
Jun  9 12:55:34.230: INFO: Created: latency-svc-zwmgc
Jun  9 12:55:34.251: INFO: Created: latency-svc-4czhf
Jun  9 12:55:34.261: INFO: Got endpoints: latency-svc-pc9vh [751.737381ms]
Jun  9 12:55:34.292: INFO: Created: latency-svc-5nzkc
Jun  9 12:55:34.303: INFO: Got endpoints: latency-svc-zfrmd [723.600789ms]
Jun  9 12:55:34.326: INFO: Created: latency-svc-k87g6
Jun  9 12:55:34.349: INFO: Got endpoints: latency-svc-xvl7g [723.25443ms]
Jun  9 12:55:34.370: INFO: Created: latency-svc-vml8k
Jun  9 12:55:34.400: INFO: Got endpoints: latency-svc-bbx5t [740.317681ms]
Jun  9 12:55:34.420: INFO: Created: latency-svc-m5zz8
Jun  9 12:55:34.445: INFO: Got endpoints: latency-svc-6q7bp [707.79375ms]
Jun  9 12:55:34.460: INFO: Created: latency-svc-zk8nv
Jun  9 12:55:34.496: INFO: Got endpoints: latency-svc-64zv5 [729.204558ms]
Jun  9 12:55:34.514: INFO: Created: latency-svc-x8d7q
Jun  9 12:55:34.551: INFO: Got endpoints: latency-svc-tx9z9 [726.879254ms]
Jun  9 12:55:34.566: INFO: Created: latency-svc-lmhcl
Jun  9 12:55:34.605: INFO: Got endpoints: latency-svc-469zc [707.566431ms]
Jun  9 12:55:34.629: INFO: Created: latency-svc-scfcx
Jun  9 12:55:34.645: INFO: Got endpoints: latency-svc-9495n [715.324856ms]
Jun  9 12:55:34.671: INFO: Created: latency-svc-scbr7
Jun  9 12:55:34.699: INFO: Got endpoints: latency-svc-kf59s [728.373066ms]
Jun  9 12:55:34.759: INFO: Created: latency-svc-pzvsk
Jun  9 12:55:34.771: INFO: Got endpoints: latency-svc-d4dr4 [736.987584ms]
Jun  9 12:55:34.822: INFO: Got endpoints: latency-svc-5fllz [753.01223ms]
Jun  9 12:55:34.861: INFO: Created: latency-svc-n9nhj
Jun  9 12:55:34.869: INFO: Created: latency-svc-tsvd7
Jun  9 12:55:34.910: INFO: Got endpoints: latency-svc-jddzz [808.930276ms]
Jun  9 12:55:34.919: INFO: Got endpoints: latency-svc-zwmgc [762.716765ms]
Jun  9 12:55:34.965: INFO: Created: latency-svc-75dlp
Jun  9 12:55:34.967: INFO: Got endpoints: latency-svc-4czhf [754.700969ms]
Jun  9 12:55:34.973: INFO: Created: latency-svc-bmrzj
Jun  9 12:55:35.052: INFO: Created: latency-svc-6ns8b
Jun  9 12:55:35.053: INFO: Got endpoints: latency-svc-5nzkc [792.228718ms]
Jun  9 12:55:35.062: INFO: Got endpoints: latency-svc-k87g6 [759.095587ms]
Jun  9 12:55:35.087: INFO: Created: latency-svc-6sk8t
Jun  9 12:55:35.105: INFO: Got endpoints: latency-svc-vml8k [756.062243ms]
Jun  9 12:55:35.166: INFO: Created: latency-svc-jgb47
Jun  9 12:55:35.178: INFO: Created: latency-svc-tmnf9
Jun  9 12:55:35.179: INFO: Got endpoints: latency-svc-m5zz8 [778.670253ms]
Jun  9 12:55:35.316: INFO: Created: latency-svc-9hj5l
Jun  9 12:55:35.322: INFO: Got endpoints: latency-svc-zk8nv [877.175798ms]
Jun  9 12:55:35.402: INFO: Got endpoints: latency-svc-x8d7q [905.047027ms]
Jun  9 12:55:35.407: INFO: Got endpoints: latency-svc-lmhcl [855.752697ms]
Jun  9 12:55:35.425: INFO: Got endpoints: latency-svc-scfcx [820.215168ms]
Jun  9 12:55:35.442: INFO: Got endpoints: latency-svc-scbr7 [797.069899ms]
Jun  9 12:55:35.456: INFO: Got endpoints: latency-svc-pzvsk [757.333879ms]
Jun  9 12:55:35.474: INFO: Created: latency-svc-p9rtn
Jun  9 12:55:35.488: INFO: Created: latency-svc-d4gtc
Jun  9 12:55:35.490: INFO: Created: latency-svc-6crb4
Jun  9 12:55:35.512: INFO: Created: latency-svc-wqsqs
Jun  9 12:55:35.525: INFO: Got endpoints: latency-svc-n9nhj [754.011287ms]
Jun  9 12:55:35.569: INFO: Got endpoints: latency-svc-tsvd7 [747.073444ms]
Jun  9 12:55:35.590: INFO: Created: latency-svc-fdbwp
Jun  9 12:55:35.619: INFO: Got endpoints: latency-svc-bmrzj [708.79068ms]
Jun  9 12:55:35.623: INFO: Created: latency-svc-jmcj6
Jun  9 12:55:35.624: INFO: Created: latency-svc-cg6kc
Jun  9 12:55:35.679: INFO: Got endpoints: latency-svc-75dlp [759.8193ms]
Jun  9 12:55:35.679: INFO: Created: latency-svc-njrbq
Jun  9 12:55:35.686: INFO: Created: latency-svc-4cmwz
Jun  9 12:55:35.700: INFO: Got endpoints: latency-svc-6ns8b [733.023086ms]
Jun  9 12:55:35.709: INFO: Created: latency-svc-5dpds
Jun  9 12:55:35.726: INFO: Created: latency-svc-8nf95
Jun  9 12:55:35.746: INFO: Got endpoints: latency-svc-6sk8t [692.255539ms]
Jun  9 12:55:35.770: INFO: Created: latency-svc-h47wr
Jun  9 12:55:35.798: INFO: Got endpoints: latency-svc-jgb47 [736.241402ms]
Jun  9 12:55:35.827: INFO: Created: latency-svc-4jqdt
Jun  9 12:55:35.847: INFO: Got endpoints: latency-svc-tmnf9 [742.31021ms]
Jun  9 12:55:35.909: INFO: Got endpoints: latency-svc-9hj5l [729.776466ms]
Jun  9 12:55:35.913: INFO: Created: latency-svc-tglcv
Jun  9 12:55:35.933: INFO: Created: latency-svc-6fsbn
Jun  9 12:55:35.944: INFO: Got endpoints: latency-svc-p9rtn [621.769027ms]
Jun  9 12:55:35.969: INFO: Created: latency-svc-5k89k
Jun  9 12:55:36.008: INFO: Got endpoints: latency-svc-d4gtc [606.435462ms]
Jun  9 12:55:36.045: INFO: Created: latency-svc-kz9cz
Jun  9 12:55:36.074: INFO: Got endpoints: latency-svc-6crb4 [667.287663ms]
Jun  9 12:55:36.107: INFO: Got endpoints: latency-svc-wqsqs [681.621529ms]
Jun  9 12:55:36.148: INFO: Got endpoints: latency-svc-fdbwp [706.009869ms]
Jun  9 12:55:36.167: INFO: Created: latency-svc-p4whb
Jun  9 12:55:36.246: INFO: Created: latency-svc-nqdkf
Jun  9 12:55:36.267: INFO: Got endpoints: latency-svc-jmcj6 [810.568309ms]
Jun  9 12:55:36.303: INFO: Created: latency-svc-c9gl5
Jun  9 12:55:36.312: INFO: Got endpoints: latency-svc-cg6kc [786.737557ms]
Jun  9 12:55:36.326: INFO: Got endpoints: latency-svc-njrbq [755.98979ms]
Jun  9 12:55:36.339: INFO: Created: latency-svc-hx7zb
Jun  9 12:55:36.377: INFO: Created: latency-svc-8cg45
Jun  9 12:55:36.381: INFO: Got endpoints: latency-svc-4cmwz [760.832441ms]
Jun  9 12:55:36.427: INFO: Got endpoints: latency-svc-5dpds [748.289336ms]
Jun  9 12:55:36.451: INFO: Created: latency-svc-kr9ch
Jun  9 12:55:36.459: INFO: Got endpoints: latency-svc-8nf95 [753.651987ms]
Jun  9 12:55:36.473: INFO: Created: latency-svc-bsh68
Jun  9 12:55:36.514: INFO: Got endpoints: latency-svc-h47wr [768.041038ms]
Jun  9 12:55:36.521: INFO: Created: latency-svc-btcl5
Jun  9 12:55:36.525: INFO: Created: latency-svc-b7t5w
Jun  9 12:55:36.539: INFO: Created: latency-svc-svg4t
Jun  9 12:55:36.548: INFO: Got endpoints: latency-svc-4jqdt [749.608158ms]
Jun  9 12:55:36.562: INFO: Created: latency-svc-mkphd
Jun  9 12:55:36.607: INFO: Got endpoints: latency-svc-tglcv [758.448399ms]
Jun  9 12:55:36.632: INFO: Created: latency-svc-ppx82
Jun  9 12:55:36.649: INFO: Got endpoints: latency-svc-6fsbn [739.757542ms]
Jun  9 12:55:36.667: INFO: Created: latency-svc-7kj9g
Jun  9 12:55:36.700: INFO: Got endpoints: latency-svc-5k89k [755.650461ms]
Jun  9 12:55:36.721: INFO: Created: latency-svc-gq79j
Jun  9 12:55:36.757: INFO: Got endpoints: latency-svc-kz9cz [748.629158ms]
Jun  9 12:55:36.774: INFO: Created: latency-svc-k4xjg
Jun  9 12:55:36.800: INFO: Got endpoints: latency-svc-p4whb [726.063108ms]
Jun  9 12:55:36.824: INFO: Created: latency-svc-lp7dt
Jun  9 12:55:36.850: INFO: Got endpoints: latency-svc-nqdkf [743.198983ms]
Jun  9 12:55:36.869: INFO: Created: latency-svc-jsszk
Jun  9 12:55:36.898: INFO: Got endpoints: latency-svc-c9gl5 [750.118954ms]
Jun  9 12:55:36.914: INFO: Created: latency-svc-6qs6v
Jun  9 12:55:36.956: INFO: Got endpoints: latency-svc-hx7zb [689.352125ms]
Jun  9 12:55:36.981: INFO: Created: latency-svc-d9xvg
Jun  9 12:55:37.002: INFO: Got endpoints: latency-svc-8cg45 [689.475889ms]
Jun  9 12:55:37.015: INFO: Created: latency-svc-r9djx
Jun  9 12:55:37.048: INFO: Got endpoints: latency-svc-kr9ch [722.225886ms]
Jun  9 12:55:37.067: INFO: Created: latency-svc-ddvc7
Jun  9 12:55:37.095: INFO: Got endpoints: latency-svc-bsh68 [713.934952ms]
Jun  9 12:55:37.109: INFO: Created: latency-svc-k9cgj
Jun  9 12:55:37.155: INFO: Got endpoints: latency-svc-btcl5 [727.773962ms]
Jun  9 12:55:37.215: INFO: Created: latency-svc-9wf46
Jun  9 12:55:37.227: INFO: Got endpoints: latency-svc-b7t5w [767.749262ms]
Jun  9 12:55:37.260: INFO: Got endpoints: latency-svc-svg4t [746.445502ms]
Jun  9 12:55:37.283: INFO: Created: latency-svc-nvn4g
Jun  9 12:55:37.291: INFO: Created: latency-svc-2rb7m
Jun  9 12:55:37.298: INFO: Got endpoints: latency-svc-mkphd [750.279379ms]
Jun  9 12:55:37.316: INFO: Created: latency-svc-htxtd
Jun  9 12:55:37.354: INFO: Got endpoints: latency-svc-ppx82 [746.94507ms]
Jun  9 12:55:37.374: INFO: Created: latency-svc-x9rb5
Jun  9 12:55:37.410: INFO: Got endpoints: latency-svc-7kj9g [761.429803ms]
Jun  9 12:55:37.428: INFO: Created: latency-svc-lj2sm
Jun  9 12:55:37.455: INFO: Got endpoints: latency-svc-gq79j [755.399737ms]
Jun  9 12:55:37.470: INFO: Created: latency-svc-x7hdc
Jun  9 12:55:37.499: INFO: Got endpoints: latency-svc-k4xjg [741.730641ms]
Jun  9 12:55:37.511: INFO: Created: latency-svc-s8gjg
Jun  9 12:55:37.551: INFO: Got endpoints: latency-svc-lp7dt [750.853076ms]
Jun  9 12:55:37.588: INFO: Created: latency-svc-jfdhq
Jun  9 12:55:37.602: INFO: Got endpoints: latency-svc-jsszk [752.156093ms]
Jun  9 12:55:37.631: INFO: Created: latency-svc-bqtvq
Jun  9 12:55:37.658: INFO: Got endpoints: latency-svc-6qs6v [759.873026ms]
Jun  9 12:55:37.709: INFO: Got endpoints: latency-svc-d9xvg [752.379527ms]
Jun  9 12:55:37.715: INFO: Created: latency-svc-x8km6
Jun  9 12:55:37.768: INFO: Created: latency-svc-2phts
Jun  9 12:55:37.783: INFO: Got endpoints: latency-svc-r9djx [781.607557ms]
Jun  9 12:55:37.830: INFO: Got endpoints: latency-svc-ddvc7 [781.501317ms]
Jun  9 12:55:37.852: INFO: Got endpoints: latency-svc-k9cgj [757.297025ms]
Jun  9 12:55:37.871: INFO: Created: latency-svc-8kf7j
Jun  9 12:55:37.878: INFO: Created: latency-svc-qhvd6
Jun  9 12:55:37.895: INFO: Created: latency-svc-ksqsb
Jun  9 12:55:37.902: INFO: Got endpoints: latency-svc-9wf46 [746.873193ms]
Jun  9 12:55:37.950: INFO: Got endpoints: latency-svc-nvn4g [722.639157ms]
Jun  9 12:55:38.002: INFO: Got endpoints: latency-svc-2rb7m [741.710166ms]
Jun  9 12:55:38.049: INFO: Got endpoints: latency-svc-htxtd [750.986374ms]
Jun  9 12:55:38.100: INFO: Got endpoints: latency-svc-x9rb5 [745.74083ms]
Jun  9 12:55:38.148: INFO: Got endpoints: latency-svc-lj2sm [737.657161ms]
Jun  9 12:55:38.201: INFO: Got endpoints: latency-svc-x7hdc [745.55308ms]
Jun  9 12:55:38.252: INFO: Got endpoints: latency-svc-s8gjg [753.11677ms]
Jun  9 12:55:38.299: INFO: Got endpoints: latency-svc-jfdhq [747.902964ms]
Jun  9 12:55:38.354: INFO: Got endpoints: latency-svc-bqtvq [751.921751ms]
Jun  9 12:55:38.398: INFO: Got endpoints: latency-svc-x8km6 [739.645598ms]
Jun  9 12:55:38.451: INFO: Got endpoints: latency-svc-2phts [741.124836ms]
Jun  9 12:55:38.502: INFO: Got endpoints: latency-svc-8kf7j [718.747237ms]
Jun  9 12:55:38.553: INFO: Got endpoints: latency-svc-qhvd6 [722.945196ms]
Jun  9 12:55:38.598: INFO: Got endpoints: latency-svc-ksqsb [746.077613ms]
Jun  9 12:55:38.598: INFO: Latencies: [71.912017ms 119.539301ms 121.44783ms 146.384273ms 180.437288ms 236.476008ms 237.854423ms 350.287631ms 350.630049ms 351.134768ms 359.175309ms 366.150827ms 369.077086ms 372.341619ms 373.911167ms 379.680844ms 384.117553ms 384.430531ms 384.648108ms 385.760355ms 385.760891ms 388.931109ms 389.572749ms 390.205696ms 394.494469ms 399.236243ms 401.362534ms 402.578747ms 406.734266ms 411.977065ms 412.730089ms 414.623764ms 416.657654ms 416.909024ms 417.652523ms 417.911938ms 423.701353ms 424.918542ms 432.424049ms 444.443629ms 444.677144ms 445.431507ms 446.84306ms 452.379554ms 453.819235ms 454.671637ms 455.876905ms 456.980747ms 462.650216ms 463.844119ms 466.532571ms 467.340864ms 467.535785ms 472.699907ms 474.213695ms 478.072358ms 480.315988ms 482.376704ms 487.536116ms 498.251882ms 498.425338ms 502.961474ms 504.699763ms 506.267952ms 512.85888ms 514.389803ms 527.175582ms 530.142389ms 543.253954ms 548.619217ms 555.008356ms 560.23054ms 564.784465ms 574.041347ms 581.426122ms 583.560857ms 588.698104ms 592.967933ms 605.246949ms 606.435462ms 607.456421ms 610.944502ms 619.053572ms 621.769027ms 627.185567ms 667.287663ms 668.141871ms 681.621529ms 684.789834ms 689.352125ms 689.475889ms 692.255539ms 692.773409ms 700.242781ms 700.313126ms 706.009869ms 707.082562ms 707.566431ms 707.79375ms 708.79068ms 709.659069ms 712.862866ms 713.657035ms 713.934952ms 715.324856ms 717.470507ms 718.747237ms 722.225886ms 722.639157ms 722.945196ms 723.25443ms 723.600789ms 726.063108ms 726.879254ms 727.773962ms 728.373066ms 729.204558ms 729.776466ms 733.023086ms 733.55589ms 736.241402ms 736.987584ms 737.657161ms 738.439076ms 739.645598ms 739.757542ms 740.317681ms 741.124836ms 741.710166ms 741.730641ms 742.31021ms 743.173612ms 743.198983ms 745.55308ms 745.74083ms 746.077613ms 746.445502ms 746.873193ms 746.94507ms 747.073444ms 747.902964ms 748.289336ms 748.629158ms 749.608158ms 750.118954ms 750.279379ms 750.73321ms 750.853076ms 750.986374ms 751.425332ms 751.737381ms 751.921751ms 752.156093ms 752.289384ms 752.379527ms 753.01223ms 753.11677ms 753.651987ms 754.011287ms 754.700969ms 755.399737ms 755.650461ms 755.98979ms 756.062243ms 757.168922ms 757.297025ms 757.333879ms 758.448399ms 758.646226ms 758.758871ms 759.095587ms 759.8193ms 759.873026ms 760.832441ms 761.429803ms 762.716765ms 764.30803ms 765.018832ms 765.620701ms 767.532034ms 767.749262ms 768.041038ms 773.409736ms 773.663478ms 778.670253ms 778.85105ms 781.501317ms 781.607557ms 784.424846ms 786.737557ms 787.745145ms 792.228718ms 795.467074ms 797.069899ms 808.930276ms 810.568309ms 820.215168ms 855.752697ms 877.175798ms 905.047027ms]
Jun  9 12:55:38.599: INFO: 50 %ile: 709.659069ms
Jun  9 12:55:38.599: INFO: 90 %ile: 767.749262ms
Jun  9 12:55:38.599: INFO: 99 %ile: 877.175798ms
Jun  9 12:55:38.599: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:55:38.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1760" for this suite.

• [SLOW TEST:12.092 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":34,"skipped":520,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:55:38.624: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8492
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  9 12:55:38.838: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  9 12:55:57.223: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.24.173.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8492 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 12:55:57.224: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 12:55:58.405: INFO: Found all expected endpoints: [netserver-0]
Jun  9 12:55:58.411: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.24.160.85 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8492 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 12:55:58.411: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 12:55:59.599: INFO: Found all expected endpoints: [netserver-1]
Jun  9 12:55:59.606: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.24.106.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8492 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 12:55:59.606: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 12:56:00.810: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:56:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8492" for this suite.

• [SLOW TEST:22.206 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":530,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:56:00.840: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun  9 12:56:41.179: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:56:41.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0609 12:56:41.179239      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6436" for this suite.

• [SLOW TEST:40.356 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":36,"skipped":537,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:56:41.195: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 12:56:41.424: INFO: Waiting up to 5m0s for pod "downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a" in namespace "downward-api-4209" to be "success or failure"
Jun  9 12:56:41.429: INFO: Pod "downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.002156ms
Jun  9 12:56:43.438: INFO: Pod "downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014275968s
Jun  9 12:56:45.444: INFO: Pod "downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020530106s
STEP: Saw pod success
Jun  9 12:56:45.445: INFO: Pod "downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a" satisfied condition "success or failure"
Jun  9 12:56:45.449: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a container client-container: <nil>
STEP: delete the pod
Jun  9 12:56:45.556: INFO: Waiting for pod downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a to disappear
Jun  9 12:56:45.564: INFO: Pod downwardapi-volume-563029d0-f50c-4342-8ed2-69a689fb9c3a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:56:45.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4209" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":37,"skipped":539,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:56:45.600: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 12:56:45.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82" in namespace "projected-3308" to be "success or failure"
Jun  9 12:56:45.867: INFO: Pod "downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.632764ms
Jun  9 12:56:47.875: INFO: Pod "downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020723266s
Jun  9 12:56:49.882: INFO: Pod "downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027379441s
Jun  9 12:56:51.889: INFO: Pod "downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034971513s
STEP: Saw pod success
Jun  9 12:56:51.889: INFO: Pod "downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82" satisfied condition "success or failure"
Jun  9 12:56:51.895: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82 container client-container: <nil>
STEP: delete the pod
Jun  9 12:56:51.952: INFO: Waiting for pod downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82 to disappear
Jun  9 12:56:51.957: INFO: Pod downwardapi-volume-e2d3bcfe-1b9f-409d-872c-8c7ee4389f82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:56:51.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3308" for this suite.

• [SLOW TEST:6.377 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":38,"skipped":555,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:56:51.978: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun  9 12:56:52.177: INFO: Waiting up to 5m0s for pod "downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2" in namespace "downward-api-2123" to be "success or failure"
Jun  9 12:56:52.182: INFO: Pod "downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154458ms
Jun  9 12:56:54.202: INFO: Pod "downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024446472s
Jun  9 12:56:56.210: INFO: Pod "downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03196564s
STEP: Saw pod success
Jun  9 12:56:56.210: INFO: Pod "downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2" satisfied condition "success or failure"
Jun  9 12:56:56.214: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2 container dapi-container: <nil>
STEP: delete the pod
Jun  9 12:56:56.267: INFO: Waiting for pod downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2 to disappear
Jun  9 12:56:56.273: INFO: Pod downward-api-4f4bb0be-bdb0-4c44-99b3-aec9fe4025a2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:56:56.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2123" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":39,"skipped":574,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:56:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4729
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun  9 12:57:00.556: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4729 PodName:pod-sharedvolume-7d1760a8-d9a0-4cdc-b3fe-8abeb18c68a8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 12:57:00.556: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 12:57:00.756: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:57:00.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4729" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":40,"skipped":591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:57:00.775: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-23619f07-0388-4774-a305-f4af024ef04e
STEP: Creating a pod to test consume secrets
Jun  9 12:57:01.004: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8" in namespace "projected-9360" to be "success or failure"
Jun  9 12:57:01.008: INFO: Pod "pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.34347ms
Jun  9 12:57:03.021: INFO: Pod "pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017340723s
Jun  9 12:57:05.033: INFO: Pod "pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028966274s
STEP: Saw pod success
Jun  9 12:57:05.033: INFO: Pod "pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8" satisfied condition "success or failure"
Jun  9 12:57:05.038: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8 container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 12:57:05.073: INFO: Waiting for pod pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8 to disappear
Jun  9 12:57:05.076: INFO: Pod pod-projected-secrets-5092de26-0ad6-4626-b9c2-b6997a14a0d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:57:05.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9360" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":616,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:57:05.092: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun  9 12:57:05.300: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:57:09.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5852" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":42,"skipped":625,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:57:09.908: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-06d875d9-e989-451a-9217-1c44cf2c5337
STEP: Creating a pod to test consume configMaps
Jun  9 12:57:10.177: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e" in namespace "projected-3833" to be "success or failure"
Jun  9 12:57:10.183: INFO: Pod "pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.72951ms
Jun  9 12:57:12.188: INFO: Pod "pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010734694s
STEP: Saw pod success
Jun  9 12:57:12.188: INFO: Pod "pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e" satisfied condition "success or failure"
Jun  9 12:57:12.194: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 12:57:12.223: INFO: Waiting for pod pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e to disappear
Jun  9 12:57:12.226: INFO: Pod pod-projected-configmaps-78bf7438-74ce-4cec-b413-ef351572044e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:57:12.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3833" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":43,"skipped":632,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:57:12.244: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4257, will wait for the garbage collector to delete the pods
Jun  9 12:57:16.575: INFO: Deleting Job.batch foo took: 9.499789ms
Jun  9 12:57:17.476: INFO: Terminating Job.batch foo pods took: 900.938912ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:57:56.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4257" for this suite.

• [SLOW TEST:44.357 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":44,"skipped":647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:57:56.602: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  9 12:57:56.803: INFO: Waiting up to 5m0s for pod "pod-108bf92f-880e-4eca-9608-329a702d4141" in namespace "emptydir-4465" to be "success or failure"
Jun  9 12:57:56.810: INFO: Pod "pod-108bf92f-880e-4eca-9608-329a702d4141": Phase="Pending", Reason="", readiness=false. Elapsed: 6.599359ms
Jun  9 12:57:58.817: INFO: Pod "pod-108bf92f-880e-4eca-9608-329a702d4141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013328762s
Jun  9 12:58:00.825: INFO: Pod "pod-108bf92f-880e-4eca-9608-329a702d4141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021543489s
STEP: Saw pod success
Jun  9 12:58:00.825: INFO: Pod "pod-108bf92f-880e-4eca-9608-329a702d4141" satisfied condition "success or failure"
Jun  9 12:58:00.828: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-108bf92f-880e-4eca-9608-329a702d4141 container test-container: <nil>
STEP: delete the pod
Jun  9 12:58:00.863: INFO: Waiting for pod pod-108bf92f-880e-4eca-9608-329a702d4141 to disappear
Jun  9 12:58:00.867: INFO: Pod pod-108bf92f-880e-4eca-9608-329a702d4141 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:58:00.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4465" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":729,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:58:00.883: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jun  9 12:58:01.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-1467'
Jun  9 12:58:01.388: INFO: stderr: ""
Jun  9 12:58:01.388: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  9 12:58:01.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:01.500: INFO: stderr: ""
Jun  9 12:58:01.500: INFO: stdout: "update-demo-nautilus-jkwcg update-demo-nautilus-sxl5n "
Jun  9 12:58:01.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:01.603: INFO: stderr: ""
Jun  9 12:58:01.603: INFO: stdout: ""
Jun  9 12:58:01.603: INFO: update-demo-nautilus-jkwcg is created but not running
Jun  9 12:58:06.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:06.717: INFO: stderr: ""
Jun  9 12:58:06.718: INFO: stdout: "update-demo-nautilus-jkwcg update-demo-nautilus-sxl5n "
Jun  9 12:58:06.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:06.845: INFO: stderr: ""
Jun  9 12:58:06.845: INFO: stdout: "true"
Jun  9 12:58:06.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:06.985: INFO: stderr: ""
Jun  9 12:58:06.985: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 12:58:06.985: INFO: validating pod update-demo-nautilus-jkwcg
Jun  9 12:58:06.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 12:58:06.996: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 12:58:06.997: INFO: update-demo-nautilus-jkwcg is verified up and running
Jun  9 12:58:06.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-sxl5n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:07.117: INFO: stderr: ""
Jun  9 12:58:07.117: INFO: stdout: "true"
Jun  9 12:58:07.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-sxl5n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:07.236: INFO: stderr: ""
Jun  9 12:58:07.236: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 12:58:07.236: INFO: validating pod update-demo-nautilus-sxl5n
Jun  9 12:58:07.247: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 12:58:07.247: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 12:58:07.247: INFO: update-demo-nautilus-sxl5n is verified up and running
STEP: scaling down the replication controller
Jun  9 12:58:07.251: INFO: scanned /root for discovery docs: <nil>
Jun  9 12:58:07.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1467'
Jun  9 12:58:08.433: INFO: stderr: ""
Jun  9 12:58:08.433: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  9 12:58:08.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:08.575: INFO: stderr: ""
Jun  9 12:58:08.575: INFO: stdout: "update-demo-nautilus-jkwcg update-demo-nautilus-sxl5n "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun  9 12:58:13.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:13.700: INFO: stderr: ""
Jun  9 12:58:13.700: INFO: stdout: "update-demo-nautilus-jkwcg update-demo-nautilus-sxl5n "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun  9 12:58:18.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:19.507: INFO: stderr: ""
Jun  9 12:58:19.507: INFO: stdout: "update-demo-nautilus-jkwcg "
Jun  9 12:58:19.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:19.621: INFO: stderr: ""
Jun  9 12:58:19.621: INFO: stdout: "true"
Jun  9 12:58:19.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:19.755: INFO: stderr: ""
Jun  9 12:58:19.756: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 12:58:19.756: INFO: validating pod update-demo-nautilus-jkwcg
Jun  9 12:58:19.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 12:58:19.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 12:58:19.766: INFO: update-demo-nautilus-jkwcg is verified up and running
STEP: scaling up the replication controller
Jun  9 12:58:19.769: INFO: scanned /root for discovery docs: <nil>
Jun  9 12:58:19.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1467'
Jun  9 12:58:20.922: INFO: stderr: ""
Jun  9 12:58:20.922: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  9 12:58:20.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:21.055: INFO: stderr: ""
Jun  9 12:58:21.056: INFO: stdout: "update-demo-nautilus-jkwcg update-demo-nautilus-pzc7z "
Jun  9 12:58:21.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:21.164: INFO: stderr: ""
Jun  9 12:58:21.165: INFO: stdout: "true"
Jun  9 12:58:21.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:21.277: INFO: stderr: ""
Jun  9 12:58:21.277: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 12:58:21.277: INFO: validating pod update-demo-nautilus-jkwcg
Jun  9 12:58:21.288: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 12:58:21.288: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 12:58:21.288: INFO: update-demo-nautilus-jkwcg is verified up and running
Jun  9 12:58:21.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-pzc7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:21.415: INFO: stderr: ""
Jun  9 12:58:21.415: INFO: stdout: ""
Jun  9 12:58:21.415: INFO: update-demo-nautilus-pzc7z is created but not running
Jun  9 12:58:26.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1467'
Jun  9 12:58:26.528: INFO: stderr: ""
Jun  9 12:58:26.528: INFO: stdout: "update-demo-nautilus-jkwcg update-demo-nautilus-pzc7z "
Jun  9 12:58:26.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:26.640: INFO: stderr: ""
Jun  9 12:58:26.641: INFO: stdout: "true"
Jun  9 12:58:26.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-jkwcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:26.748: INFO: stderr: ""
Jun  9 12:58:26.748: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 12:58:26.748: INFO: validating pod update-demo-nautilus-jkwcg
Jun  9 12:58:26.767: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 12:58:26.767: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 12:58:26.767: INFO: update-demo-nautilus-jkwcg is verified up and running
Jun  9 12:58:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-pzc7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:26.891: INFO: stderr: ""
Jun  9 12:58:26.891: INFO: stdout: "true"
Jun  9 12:58:26.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-pzc7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1467'
Jun  9 12:58:27.010: INFO: stderr: ""
Jun  9 12:58:27.010: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 12:58:27.010: INFO: validating pod update-demo-nautilus-pzc7z
Jun  9 12:58:27.028: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 12:58:27.028: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 12:58:27.028: INFO: update-demo-nautilus-pzc7z is verified up and running
STEP: using delete to clean up resources
Jun  9 12:58:27.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-1467'
Jun  9 12:58:27.142: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 12:58:27.142: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  9 12:58:27.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1467'
Jun  9 12:58:27.274: INFO: stderr: "No resources found in kubectl-1467 namespace.\n"
Jun  9 12:58:27.274: INFO: stdout: ""
Jun  9 12:58:27.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -l name=update-demo --namespace=kubectl-1467 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 12:58:27.431: INFO: stderr: ""
Jun  9 12:58:27.431: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:58:27.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1467" for this suite.

• [SLOW TEST:26.564 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":46,"skipped":742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:58:27.449: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-8445/configmap-test-494649f8-ec66-4e34-a4ef-764a47ebe91c
STEP: Creating a pod to test consume configMaps
Jun  9 12:58:27.673: INFO: Waiting up to 5m0s for pod "pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2" in namespace "configmap-8445" to be "success or failure"
Jun  9 12:58:27.677: INFO: Pod "pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.443917ms
Jun  9 12:58:29.693: INFO: Pod "pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020021267s
Jun  9 12:58:31.700: INFO: Pod "pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026828627s
STEP: Saw pod success
Jun  9 12:58:31.700: INFO: Pod "pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2" satisfied condition "success or failure"
Jun  9 12:58:31.704: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2 container env-test: <nil>
STEP: delete the pod
Jun  9 12:58:31.742: INFO: Waiting for pod pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2 to disappear
Jun  9 12:58:31.745: INFO: Pod pod-configmaps-23585997-c2e5-44c6-bea3-25b1db6b31b2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:58:31.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8445" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:58:31.765: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 12:58:31.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2392'
Jun  9 12:58:32.071: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  9 12:58:32.071: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jun  9 12:58:32.099: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun  9 12:58:32.118: INFO: scanned /root for discovery docs: <nil>
Jun  9 12:58:32.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2392'
Jun  9 12:58:48.023: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun  9 12:58:48.023: INFO: stdout: "Created e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152\nScaling up e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jun  9 12:58:48.023: INFO: stdout: "Created e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152\nScaling up e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jun  9 12:58:48.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2392'
Jun  9 12:58:48.132: INFO: stderr: ""
Jun  9 12:58:48.133: INFO: stdout: "e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152-plwqx "
Jun  9 12:58:48.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152-plwqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2392'
Jun  9 12:58:48.261: INFO: stderr: ""
Jun  9 12:58:48.261: INFO: stdout: "true"
Jun  9 12:58:48.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152-plwqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2392'
Jun  9 12:58:48.366: INFO: stderr: ""
Jun  9 12:58:48.366: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jun  9 12:58:48.366: INFO: e2e-test-httpd-rc-8a15e650226ac7d3db1b0c1483670152-plwqx is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Jun  9 12:58:48.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete rc e2e-test-httpd-rc --namespace=kubectl-2392'
Jun  9 12:58:48.561: INFO: stderr: ""
Jun  9 12:58:48.561: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:58:48.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2392" for this suite.

• [SLOW TEST:16.815 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":48,"skipped":862,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:58:48.580: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-00c63c55-295b-4fcb-bfdd-40662b8a28e4
STEP: Creating a pod to test consume configMaps
Jun  9 12:58:48.819: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59" in namespace "projected-3027" to be "success or failure"
Jun  9 12:58:48.832: INFO: Pod "pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59": Phase="Pending", Reason="", readiness=false. Elapsed: 13.260208ms
Jun  9 12:58:50.840: INFO: Pod "pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021567339s
Jun  9 12:58:52.848: INFO: Pod "pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029374047s
STEP: Saw pod success
Jun  9 12:58:52.848: INFO: Pod "pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59" satisfied condition "success or failure"
Jun  9 12:58:52.854: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 12:58:52.897: INFO: Waiting for pod pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59 to disappear
Jun  9 12:58:52.910: INFO: Pod pod-projected-configmaps-504e49f8-9a09-4338-ae43-b601d1736a59 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:58:52.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3027" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":49,"skipped":864,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:58:52.931: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 12:58:54.122: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun  9 12:58:56.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304334, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304334, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304334, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304334, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 12:58:59.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 12:58:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 12:58:59.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2955" for this suite.
STEP: Destroying namespace "webhook-2955-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.155 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":50,"skipped":878,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 12:59:00.089: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4496
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-b0d421b9-636c-4ea4-91ca-0dcd99f4d569
STEP: Creating configMap with name cm-test-opt-upd-dfb00f33-3c17-446b-b1ec-dc1aacb17d7e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b0d421b9-636c-4ea4-91ca-0dcd99f4d569
STEP: Updating configmap cm-test-opt-upd-dfb00f33-3c17-446b-b1ec-dc1aacb17d7e
STEP: Creating configMap with name cm-test-opt-create-2ccb02fc-1cae-4af5-9e3c-db30c1e4f4fa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:00:15.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4496" for this suite.

• [SLOW TEST:75.392 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":51,"skipped":893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:00:15.494: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-h276
STEP: Creating a pod to test atomic-volume-subpath
Jun  9 13:00:15.745: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h276" in namespace "subpath-6204" to be "success or failure"
Jun  9 13:00:15.755: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Pending", Reason="", readiness=false. Elapsed: 10.037292ms
Jun  9 13:00:17.763: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018044236s
Jun  9 13:00:19.771: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 4.025867304s
Jun  9 13:00:21.778: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 6.032806404s
Jun  9 13:00:23.785: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 8.040115635s
Jun  9 13:00:25.792: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 10.046790821s
Jun  9 13:00:27.801: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 12.055680569s
Jun  9 13:00:29.809: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 14.063613889s
Jun  9 13:00:31.823: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 16.077568097s
Jun  9 13:00:33.831: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 18.085454772s
Jun  9 13:00:35.844: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 20.098771695s
Jun  9 13:00:37.851: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Running", Reason="", readiness=true. Elapsed: 22.105482018s
Jun  9 13:00:39.858: INFO: Pod "pod-subpath-test-downwardapi-h276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.112427469s
STEP: Saw pod success
Jun  9 13:00:39.858: INFO: Pod "pod-subpath-test-downwardapi-h276" satisfied condition "success or failure"
Jun  9 13:00:39.863: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-subpath-test-downwardapi-h276 container test-container-subpath-downwardapi-h276: <nil>
STEP: delete the pod
Jun  9 13:00:39.905: INFO: Waiting for pod pod-subpath-test-downwardapi-h276 to disappear
Jun  9 13:00:39.912: INFO: Pod pod-subpath-test-downwardapi-h276 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h276
Jun  9 13:00:39.912: INFO: Deleting pod "pod-subpath-test-downwardapi-h276" in namespace "subpath-6204"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:00:39.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6204" for this suite.

• [SLOW TEST:24.450 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":52,"skipped":921,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:00:39.944: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Jun  9 13:00:40.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-5092'
Jun  9 13:00:40.620: INFO: stderr: ""
Jun  9 13:00:40.620: INFO: stdout: "pod/pause created\n"
Jun  9 13:00:40.620: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  9 13:00:40.620: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5092" to be "running and ready"
Jun  9 13:00:40.627: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.326924ms
Jun  9 13:00:42.638: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017288614s
Jun  9 13:00:44.644: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.023195653s
Jun  9 13:00:44.644: INFO: Pod "pause" satisfied condition "running and ready"
Jun  9 13:00:44.644: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jun  9 13:00:44.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 label pods pause testing-label=testing-label-value --namespace=kubectl-5092'
Jun  9 13:00:44.775: INFO: stderr: ""
Jun  9 13:00:44.775: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun  9 13:00:44.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pod pause -L testing-label --namespace=kubectl-5092'
Jun  9 13:00:44.927: INFO: stderr: ""
Jun  9 13:00:44.928: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun  9 13:00:44.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 label pods pause testing-label- --namespace=kubectl-5092'
Jun  9 13:00:45.112: INFO: stderr: ""
Jun  9 13:00:45.112: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun  9 13:00:45.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pod pause -L testing-label --namespace=kubectl-5092'
Jun  9 13:00:45.221: INFO: stderr: ""
Jun  9 13:00:45.221: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Jun  9 13:00:45.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-5092'
Jun  9 13:00:45.391: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 13:00:45.391: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  9 13:00:45.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get rc,svc -l name=pause --no-headers --namespace=kubectl-5092'
Jun  9 13:00:45.561: INFO: stderr: "No resources found in kubectl-5092 namespace.\n"
Jun  9 13:00:45.561: INFO: stdout: ""
Jun  9 13:00:45.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -l name=pause --namespace=kubectl-5092 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 13:00:45.694: INFO: stderr: ""
Jun  9 13:00:45.694: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:00:45.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5092" for this suite.

• [SLOW TEST:5.795 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1272
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":53,"skipped":931,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:00:45.739: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun  9 13:00:50.689: INFO: Successfully updated pod "annotationupdate6fab4b18-532f-426f-abc0-846a3a3f2914"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:00:52.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9150" for this suite.

• [SLOW TEST:7.024 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":54,"skipped":933,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:00:52.770: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-shhg
STEP: Creating a pod to test atomic-volume-subpath
Jun  9 13:00:53.047: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-shhg" in namespace "subpath-2517" to be "success or failure"
Jun  9 13:00:53.060: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.957898ms
Jun  9 13:00:55.067: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020539398s
Jun  9 13:00:57.077: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 4.029865319s
Jun  9 13:00:59.083: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 6.036353547s
Jun  9 13:01:01.089: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 8.041911514s
Jun  9 13:01:03.095: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 10.048445963s
Jun  9 13:01:05.106: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 12.05922946s
Jun  9 13:01:07.114: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 14.067156986s
Jun  9 13:01:09.122: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 16.075066569s
Jun  9 13:01:11.129: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 18.08261438s
Jun  9 13:01:13.137: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 20.089969618s
Jun  9 13:01:15.144: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Running", Reason="", readiness=true. Elapsed: 22.097329451s
Jun  9 13:01:17.152: INFO: Pod "pod-subpath-test-secret-shhg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.104968129s
STEP: Saw pod success
Jun  9 13:01:17.152: INFO: Pod "pod-subpath-test-secret-shhg" satisfied condition "success or failure"
Jun  9 13:01:17.156: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-subpath-test-secret-shhg container test-container-subpath-secret-shhg: <nil>
STEP: delete the pod
Jun  9 13:01:17.203: INFO: Waiting for pod pod-subpath-test-secret-shhg to disappear
Jun  9 13:01:17.209: INFO: Pod pod-subpath-test-secret-shhg no longer exists
STEP: Deleting pod pod-subpath-test-secret-shhg
Jun  9 13:01:17.209: INFO: Deleting pod "pod-subpath-test-secret-shhg" in namespace "subpath-2517"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:01:17.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2517" for this suite.

• [SLOW TEST:24.460 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":55,"skipped":939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:01:17.231: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5014
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jun  9 13:01:17.447: INFO: Found 0 stateful pods, waiting for 3
Jun  9 13:01:27.457: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:01:27.457: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:01:27.457: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:01:27.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5014 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:01:27.825: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:01:27.825: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:01:27.825: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun  9 13:01:37.874: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun  9 13:01:47.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5014 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:01:48.237: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:01:48.237: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:01:48.237: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:01:58.272: INFO: Waiting for StatefulSet statefulset-5014/ss2 to complete update
Jun  9 13:01:58.272: INFO: Waiting for Pod statefulset-5014/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  9 13:01:58.272: INFO: Waiting for Pod statefulset-5014/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  9 13:02:08.286: INFO: Waiting for StatefulSet statefulset-5014/ss2 to complete update
Jun  9 13:02:08.286: INFO: Waiting for Pod statefulset-5014/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  9 13:02:18.283: INFO: Waiting for StatefulSet statefulset-5014/ss2 to complete update
Jun  9 13:02:18.283: INFO: Waiting for Pod statefulset-5014/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  9 13:02:28.290: INFO: Waiting for StatefulSet statefulset-5014/ss2 to complete update
STEP: Rolling back to a previous revision
Jun  9 13:02:38.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5014 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:02:38.636: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:02:38.636: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:02:38.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:02:48.690: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun  9 13:02:58.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5014 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:02:59.089: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:02:59.089: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:02:59.089: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun  9 13:03:19.132: INFO: Deleting all statefulset in ns statefulset-5014
Jun  9 13:03:19.142: INFO: Scaling statefulset ss2 to 0
Jun  9 13:03:39.174: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:03:39.179: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:03:39.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5014" for this suite.

• [SLOW TEST:141.981 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":56,"skipped":961,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:03:39.213: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8064.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8064.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8064.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8064.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:03:59.480: INFO: DNS probes using dns-test-ab0228bc-4033-4f1f-8d9a-4f4b0c40c000 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8064.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8064.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8064.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8064.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:04:03.626: INFO: File jessie_udp@dns-test-service-3.dns-8064.svc.cluster.local from pod  dns-8064/dns-test-1c8cd6a6-ddd1-4ab4-a7fb-acd55e371fb5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 13:04:03.627: INFO: Lookups using dns-8064/dns-test-1c8cd6a6-ddd1-4ab4-a7fb-acd55e371fb5 failed for: [jessie_udp@dns-test-service-3.dns-8064.svc.cluster.local]

Jun  9 13:04:08.646: INFO: DNS probes using dns-test-1c8cd6a6-ddd1-4ab4-a7fb-acd55e371fb5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8064.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8064.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8064.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8064.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:04:12.836: INFO: DNS probes using dns-test-88ca8e07-8646-46aa-903e-29dbeba00654 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:12.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8064" for this suite.

• [SLOW TEST:33.711 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":57,"skipped":971,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:12.924: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4216
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:04:13.319: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"981eb2a3-2713-433c-9917-0eb8cfe79ec3", Controller:(*bool)(0xc00318a6ce), BlockOwnerDeletion:(*bool)(0xc00318a6cf)}}
Jun  9 13:04:13.330: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cccb9efa-91e0-44fe-9139-9a5e4dfeaeec", Controller:(*bool)(0xc002ca581e), BlockOwnerDeletion:(*bool)(0xc002ca581f)}}
Jun  9 13:04:13.341: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fc1534dd-f100-4071-8ed2-36fce640af61", Controller:(*bool)(0xc002ca59b6), BlockOwnerDeletion:(*bool)(0xc002ca59b7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:18.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4216" for this suite.

• [SLOW TEST:5.484 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":58,"skipped":987,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun  9 13:04:18.665: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7335 /api/v1/namespaces/watch-7335/configmaps/e2e-watch-test-resource-version 42ebdd30-4d02-445d-bcb3-a608c75d3681 20665 0 2020-06-09 13:04:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  9 13:04:18.665: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7335 /api/v1/namespaces/watch-7335/configmaps/e2e-watch-test-resource-version 42ebdd30-4d02-445d-bcb3-a608c75d3681 20667 0 2020-06-09 13:04:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:18.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7335" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":59,"skipped":988,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:18.715: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:35.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4695" for this suite.

• [SLOW TEST:16.425 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":60,"skipped":1017,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:35.149: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jun  9 13:04:35.374: INFO: Waiting up to 5m0s for pod "var-expansion-127d06d6-ee49-46c7-8824-b89397324d51" in namespace "var-expansion-5132" to be "success or failure"
Jun  9 13:04:35.378: INFO: Pod "var-expansion-127d06d6-ee49-46c7-8824-b89397324d51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037893ms
Jun  9 13:04:37.386: INFO: Pod "var-expansion-127d06d6-ee49-46c7-8824-b89397324d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011314463s
Jun  9 13:04:39.392: INFO: Pod "var-expansion-127d06d6-ee49-46c7-8824-b89397324d51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017454209s
STEP: Saw pod success
Jun  9 13:04:39.392: INFO: Pod "var-expansion-127d06d6-ee49-46c7-8824-b89397324d51" satisfied condition "success or failure"
Jun  9 13:04:39.396: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod var-expansion-127d06d6-ee49-46c7-8824-b89397324d51 container dapi-container: <nil>
STEP: delete the pod
Jun  9 13:04:39.441: INFO: Waiting for pod var-expansion-127d06d6-ee49-46c7-8824-b89397324d51 to disappear
Jun  9 13:04:39.451: INFO: Pod var-expansion-127d06d6-ee49-46c7-8824-b89397324d51 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:39.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5132" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":61,"skipped":1019,"failed":0}

------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:39.467: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun  9 13:04:44.236: INFO: Successfully updated pod "adopt-release-r9dhn"
STEP: Checking that the Job readopts the Pod
Jun  9 13:04:44.236: INFO: Waiting up to 15m0s for pod "adopt-release-r9dhn" in namespace "job-6572" to be "adopted"
Jun  9 13:04:44.241: INFO: Pod "adopt-release-r9dhn": Phase="Running", Reason="", readiness=true. Elapsed: 5.267999ms
Jun  9 13:04:46.249: INFO: Pod "adopt-release-r9dhn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01293003s
Jun  9 13:04:46.249: INFO: Pod "adopt-release-r9dhn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun  9 13:04:46.776: INFO: Successfully updated pod "adopt-release-r9dhn"
STEP: Checking that the Job releases the Pod
Jun  9 13:04:46.776: INFO: Waiting up to 15m0s for pod "adopt-release-r9dhn" in namespace "job-6572" to be "released"
Jun  9 13:04:46.805: INFO: Pod "adopt-release-r9dhn": Phase="Running", Reason="", readiness=true. Elapsed: 28.471028ms
Jun  9 13:04:46.805: INFO: Pod "adopt-release-r9dhn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:46.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6572" for this suite.

• [SLOW TEST:7.393 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":62,"skipped":1019,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:46.861: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun  9 13:04:47.063: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 13:04:47.083: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 13:04:47.088: INFO: 
Logging pods the kubelet thinks is on node worker-2jqhr-6f5dbbb884-vqc7c before test
Jun  9 13:04:47.122: INFO: net-exporter-4n4fk from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:04:47.122: INFO: docker-mem-limit-startup-script-clstp from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:04:47.122: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-sd6nl from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:04:47.122: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:04:47.122: INFO: calico-node-vhcbj from kube-system started at 2020-06-09 11:42:16 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:04:47.122: INFO: node-exporter-nsmh9 from kube-system started at 2020-06-09 11:45:08 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:04:47.122: INFO: nginx-ingress-controller-59b5c95c6d-cb4kl from kube-system started at 2020-06-09 11:45:21 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 13:04:47.122: INFO: kube-proxy-69gzh from kube-system started at 2020-06-09 11:43:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:04:47.122: INFO: chart-operator-5b8b4bcc75-wn4xh from giantswarm started at 2020-06-09 11:44:47 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container chart-operator ready: true, restart count 0
Jun  9 13:04:47.122: INFO: cert-exporter-gkbjk from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:04:47.122: INFO: tiller-deploy-684c6b545b-4w7nq from giantswarm started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container tiller ready: true, restart count 0
Jun  9 13:04:47.122: INFO: coredns-6d56c484c-7fkzm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:04:47.122: INFO: coredns-6d56c484c-txwjd from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:04:47.122: INFO: metrics-server-66df9f5b56-bvwdm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.122: INFO: 	Container metrics-server ready: true, restart count 0
Jun  9 13:04:47.122: INFO: 
Logging pods the kubelet thinks is on node worker-dfhc8-64bc8fc496-xx7cx before test
Jun  9 13:04:47.155: INFO: kube-proxy-tg6rp from kube-system started at 2020-06-09 11:43:24 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.155: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:04:47.156: INFO: nginx-ingress-controller-59b5c95c6d-8z4jm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.156: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 13:04:47.156: INFO: coredns-6d56c484c-s5fsw from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.156: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:04:47.157: INFO: net-exporter-wvnc9 from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.157: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:04:47.157: INFO: calico-node-7t79n from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.158: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:04:47.158: INFO: cert-exporter-v64nm from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.158: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:04:47.159: INFO: node-exporter-k9z45 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.159: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:04:47.159: INFO: sonobuoy from sonobuoy started at 2020-06-09 12:47:13 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.159: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 13:04:47.159: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-p5v44 from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:04:47.160: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:04:47.160: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:04:47.160: INFO: kube-state-metrics-6d998ffd8b-wtr6p from kube-system started at 2020-06-09 11:44:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.160: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  9 13:04:47.160: INFO: docker-mem-limit-startup-script-9x4w8 from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.161: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:04:47.161: INFO: sonobuoy-e2e-job-fb32098c60e64727 from sonobuoy started at 2020-06-09 12:47:22 +0000 UTC (2 container statuses recorded)
Jun  9 13:04:47.161: INFO: 	Container e2e ready: true, restart count 0
Jun  9 13:04:47.161: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:04:47.162: INFO: 
Logging pods the kubelet thinks is on node worker-k8xcg-8bbfd5b68-w4htb before test
Jun  9 13:04:47.186: INFO: net-exporter-hppdh from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.186: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:04:47.187: INFO: adopt-release-r9dhn from job-6572 started at 2020-06-09 13:04:39 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.187: INFO: 	Container c ready: true, restart count 0
Jun  9 13:04:47.188: INFO: cert-exporter-p2wws from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.188: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:04:47.188: INFO: node-exporter-mbg29 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.188: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:04:47.189: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-lbr8h from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:04:47.189: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:04:47.190: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:04:47.190: INFO: adopt-release-sfcfp from job-6572 started at 2020-06-09 13:04:39 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.191: INFO: 	Container c ready: true, restart count 0
Jun  9 13:04:47.191: INFO: kube-proxy-kfmds from kube-system started at 2020-06-09 11:43:19 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.192: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:04:47.192: INFO: calico-node-zqtv2 from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.193: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:04:47.193: INFO: docker-mem-limit-startup-script-74z86 from kube-system started at 2020-06-09 12:55:26 +0000 UTC (1 container statuses recorded)
Jun  9 13:04:47.193: INFO: 	Container startup-script ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-783ec306-4338-4d95-9a79-09424adcbfef 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-783ec306-4338-4d95-9a79-09424adcbfef off the node worker-k8xcg-8bbfd5b68-w4htb
STEP: verifying the node doesn't have the label kubernetes.io/e2e-783ec306-4338-4d95-9a79-09424adcbfef
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:04:55.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8273" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.580 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":63,"skipped":1038,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:04:55.442: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun  9 13:04:55.663: INFO: Created pod &Pod{ObjectMeta:{dns-3892  dns-3892 /api/v1/namespaces/dns-3892/pods/dns-3892 0b36ba92-78e3-4708-a2a2-49ddc635c4e5 21011 0 2020-06-09 13:04:55 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:cert-exporter-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wfccn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wfccn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wfccn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jun  9 13:04:59.678: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3892 PodName:dns-3892 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:04:59.678: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Verifying customized DNS server is configured on pod...
Jun  9 13:04:59.896: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3892 PodName:dns-3892 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:04:59.896: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:05:00.138: INFO: Deleting pod dns-3892...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:00.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3892" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":64,"skipped":1043,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:00.184: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-9889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-9889
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9889
STEP: Deleting pre-stop pod
Jun  9 13:05:13.504: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:13.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9889" for this suite.

• [SLOW TEST:13.359 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":65,"skipped":1045,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:13.542: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:24.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4463" for this suite.

• [SLOW TEST:11.295 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":66,"skipped":1063,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:24.838: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-a857cad8-149d-4323-81ed-7e72ec1a9ee7
STEP: Creating a pod to test consume secrets
Jun  9 13:05:25.047: INFO: Waiting up to 5m0s for pod "pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0" in namespace "secrets-7763" to be "success or failure"
Jun  9 13:05:25.054: INFO: Pod "pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23627ms
Jun  9 13:05:27.059: INFO: Pod "pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011697893s
Jun  9 13:05:29.066: INFO: Pod "pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018573179s
STEP: Saw pod success
Jun  9 13:05:29.066: INFO: Pod "pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0" satisfied condition "success or failure"
Jun  9 13:05:29.071: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0 container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:05:29.109: INFO: Waiting for pod pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0 to disappear
Jun  9 13:05:29.115: INFO: Pod pod-secrets-86f243d4-a85f-4788-8cd3-2f22f70222a0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:29.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7763" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:29.137: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-243aa6af-298f-4345-9a54-86d0e35d82d9
STEP: Creating a pod to test consume secrets
Jun  9 13:05:29.371: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6" in namespace "projected-141" to be "success or failure"
Jun  9 13:05:29.378: INFO: Pod "pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.998859ms
Jun  9 13:05:31.386: INFO: Pod "pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015054123s
Jun  9 13:05:33.395: INFO: Pod "pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024138591s
STEP: Saw pod success
Jun  9 13:05:33.395: INFO: Pod "pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6" satisfied condition "success or failure"
Jun  9 13:05:33.401: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:05:33.433: INFO: Waiting for pod pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6 to disappear
Jun  9 13:05:33.440: INFO: Pod pod-projected-secrets-18d4890a-4a1e-420d-a49a-9a703052f5f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:33.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-141" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":1105,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:33.472: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 13:05:33.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-4983'
Jun  9 13:05:33.860: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  9 13:05:33.861: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Jun  9 13:05:37.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete deployment e2e-test-httpd-deployment --namespace=kubectl-4983'
Jun  9 13:05:38.028: INFO: stderr: ""
Jun  9 13:05:38.028: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:38.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4983" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":69,"skipped":1107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:38.060: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun  9 13:05:42.319: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-503842985 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun  9 13:05:57.479: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:57.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6390" for this suite.

• [SLOW TEST:19.443 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":70,"skipped":1130,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:57.504: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jun  9 13:05:57.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 api-versions'
Jun  9 13:05:57.871: INFO: stderr: ""
Jun  9 13:05:57.871: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napplication.giantswarm.io/v1alpha1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:05:57.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8887" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":71,"skipped":1142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:05:57.893: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-db532b1a-7eb1-4271-8bb8-bddc6e579e7a
STEP: Creating a pod to test consume configMaps
Jun  9 13:05:58.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa" in namespace "configmap-8020" to be "success or failure"
Jun  9 13:05:58.139: INFO: Pod "pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 28.777006ms
Jun  9 13:06:00.155: INFO: Pod "pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04489205s
Jun  9 13:06:02.162: INFO: Pod "pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051776763s
STEP: Saw pod success
Jun  9 13:06:02.162: INFO: Pod "pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa" satisfied condition "success or failure"
Jun  9 13:06:02.166: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 13:06:02.195: INFO: Waiting for pod pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa to disappear
Jun  9 13:06:02.200: INFO: Pod pod-configmaps-9123b85f-3a23-48fb-a81f-49379e294dfa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:02.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8020" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":72,"skipped":1218,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:02.221: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8367.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8367.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 76.69.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.69.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.69.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.69.76_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8367.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8367.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 76.69.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.69.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.69.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.69.76_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:06:06.514: INFO: Unable to read wheezy_udp@dns-test-service.dns-8367.svc.cluster.local from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.532: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8367.svc.cluster.local from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.569: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.574: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.595: INFO: Unable to read jessie_udp@dns-test-service.dns-8367.svc.cluster.local from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-8367.svc.cluster.local from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.626: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8367.svc.cluster.local from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.654: INFO: Unable to read jessie_udp@PodARecord from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.661: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03: the server could not find the requested resource (get pods dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03)
Jun  9 13:06:06.682: INFO: Lookups using dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03 failed for: [wheezy_udp@dns-test-service.dns-8367.svc.cluster.local wheezy_tcp@dns-test-service.dns-8367.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-8367.svc.cluster.local jessie_tcp@dns-test-service.dns-8367.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8367.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  9 13:06:11.837: INFO: DNS probes using dns-8367/dns-test-1a084fd8-1f17-43d4-84aa-ce46476c5b03 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:12.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8367" for this suite.

• [SLOW TEST:9.893 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":73,"skipped":1231,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:06:13.310: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:06:15.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304773, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304773, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304773, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304773, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:06:18.361: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:06:18.369: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7398-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:19.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9882" for this suite.
STEP: Destroying namespace "webhook-9882-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.620 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":74,"skipped":1235,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:19.741: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:06:19.975: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun  9 13:06:22.072: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:22.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4423" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":75,"skipped":1236,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:22.097: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun  9 13:06:24.884: INFO: Successfully updated pod "labelsupdate3e33c265-c48a-4ce2-98cc-fb19c1a9d41c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:26.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1969" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":76,"skipped":1248,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:26.949: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6460
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun  9 13:06:27.175: INFO: Waiting up to 5m0s for pod "pod-fa4944f7-4649-4d33-98f6-e3ee116037ce" in namespace "emptydir-6460" to be "success or failure"
Jun  9 13:06:27.185: INFO: Pod "pod-fa4944f7-4649-4d33-98f6-e3ee116037ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.773587ms
Jun  9 13:06:29.195: INFO: Pod "pod-fa4944f7-4649-4d33-98f6-e3ee116037ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019015097s
Jun  9 13:06:31.201: INFO: Pod "pod-fa4944f7-4649-4d33-98f6-e3ee116037ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02580725s
STEP: Saw pod success
Jun  9 13:06:31.202: INFO: Pod "pod-fa4944f7-4649-4d33-98f6-e3ee116037ce" satisfied condition "success or failure"
Jun  9 13:06:31.220: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-fa4944f7-4649-4d33-98f6-e3ee116037ce container test-container: <nil>
STEP: delete the pod
Jun  9 13:06:31.274: INFO: Waiting for pod pod-fa4944f7-4649-4d33-98f6-e3ee116037ce to disappear
Jun  9 13:06:31.278: INFO: Pod pod-fa4944f7-4649-4d33-98f6-e3ee116037ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:31.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6460" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":77,"skipped":1254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:31.296: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jun  9 13:06:31.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-2132'
Jun  9 13:06:31.780: INFO: stderr: ""
Jun  9 13:06:31.780: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  9 13:06:31.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2132'
Jun  9 13:06:31.893: INFO: stderr: ""
Jun  9 13:06:31.893: INFO: stdout: "update-demo-nautilus-9hrrp update-demo-nautilus-s6q9b "
Jun  9 13:06:31.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-9hrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2132'
Jun  9 13:06:32.017: INFO: stderr: ""
Jun  9 13:06:32.017: INFO: stdout: ""
Jun  9 13:06:32.017: INFO: update-demo-nautilus-9hrrp is created but not running
Jun  9 13:06:37.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2132'
Jun  9 13:06:37.148: INFO: stderr: ""
Jun  9 13:06:37.148: INFO: stdout: "update-demo-nautilus-9hrrp update-demo-nautilus-s6q9b "
Jun  9 13:06:37.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-9hrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2132'
Jun  9 13:06:37.248: INFO: stderr: ""
Jun  9 13:06:37.248: INFO: stdout: "true"
Jun  9 13:06:37.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-9hrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2132'
Jun  9 13:06:37.378: INFO: stderr: ""
Jun  9 13:06:37.378: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 13:06:37.378: INFO: validating pod update-demo-nautilus-9hrrp
Jun  9 13:06:37.395: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 13:06:37.395: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 13:06:37.395: INFO: update-demo-nautilus-9hrrp is verified up and running
Jun  9 13:06:37.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-s6q9b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2132'
Jun  9 13:06:37.565: INFO: stderr: ""
Jun  9 13:06:37.565: INFO: stdout: "true"
Jun  9 13:06:37.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-s6q9b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2132'
Jun  9 13:06:37.681: INFO: stderr: ""
Jun  9 13:06:37.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 13:06:37.681: INFO: validating pod update-demo-nautilus-s6q9b
Jun  9 13:06:37.693: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 13:06:37.693: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 13:06:37.693: INFO: update-demo-nautilus-s6q9b is verified up and running
STEP: using delete to clean up resources
Jun  9 13:06:37.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete --grace-period=0 --force -f - --namespace=kubectl-2132'
Jun  9 13:06:37.835: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 13:06:37.836: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  9 13:06:37.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2132'
Jun  9 13:06:37.950: INFO: stderr: "No resources found in kubectl-2132 namespace.\n"
Jun  9 13:06:37.950: INFO: stdout: ""
Jun  9 13:06:37.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -l name=update-demo --namespace=kubectl-2132 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 13:06:38.059: INFO: stderr: ""
Jun  9 13:06:38.059: INFO: stdout: "update-demo-nautilus-9hrrp\nupdate-demo-nautilus-s6q9b\n"
Jun  9 13:06:38.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2132'
Jun  9 13:06:38.677: INFO: stderr: "No resources found in kubectl-2132 namespace.\n"
Jun  9 13:06:38.677: INFO: stdout: ""
Jun  9 13:06:38.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -l name=update-demo --namespace=kubectl-2132 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 13:06:38.790: INFO: stderr: ""
Jun  9 13:06:38.791: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:38.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2132" for this suite.

• [SLOW TEST:7.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":78,"skipped":1295,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:38.807: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-99ab85d1-0a02-43c2-a1dc-fb7c8c3fb7e8
STEP: Creating a pod to test consume secrets
Jun  9 13:06:39.022: INFO: Waiting up to 5m0s for pod "pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a" in namespace "secrets-592" to be "success or failure"
Jun  9 13:06:39.043: INFO: Pod "pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.22691ms
Jun  9 13:06:41.049: INFO: Pod "pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.022342402s
Jun  9 13:06:43.063: INFO: Pod "pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036212011s
STEP: Saw pod success
Jun  9 13:06:43.063: INFO: Pod "pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a" satisfied condition "success or failure"
Jun  9 13:06:43.072: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:06:43.098: INFO: Waiting for pod pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a to disappear
Jun  9 13:06:43.106: INFO: Pod pod-secrets-b149699a-aaaa-4962-9c83-26d26ed5af6a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:43.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-592" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1295,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:43.123: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8518
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-19a65472-7a1b-4e30-b998-ac377143e328
STEP: Creating configMap with name cm-test-opt-upd-e9858d89-6900-4294-8a29-29573dfd1f19
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-19a65472-7a1b-4e30-b998-ac377143e328
STEP: Updating configmap cm-test-opt-upd-e9858d89-6900-4294-8a29-29573dfd1f19
STEP: Creating configMap with name cm-test-opt-create-db1fc6a9-d9f2-431b-8249-d633a55a6f55
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:51.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8518" for this suite.

• [SLOW TEST:8.481 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1306,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:51.605: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-c6e6b842-f4e9-4dbc-ad0f-784cb4eb6cab
STEP: Creating secret with name secret-projected-all-test-volume-5d5d8167-977b-4e32-9f05-3e8c1c7213ee
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun  9 13:06:51.852: INFO: Waiting up to 5m0s for pod "projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805" in namespace "projected-6896" to be "success or failure"
Jun  9 13:06:51.858: INFO: Pod "projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805": Phase="Pending", Reason="", readiness=false. Elapsed: 5.490929ms
Jun  9 13:06:53.865: INFO: Pod "projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01272693s
Jun  9 13:06:55.875: INFO: Pod "projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022728831s
STEP: Saw pod success
Jun  9 13:06:55.875: INFO: Pod "projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805" satisfied condition "success or failure"
Jun  9 13:06:55.890: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun  9 13:06:56.168: INFO: Waiting for pod projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805 to disappear
Jun  9 13:06:56.174: INFO: Pod projected-volume-b511235d-ffd8-41fa-b7c1-0fba7c4c4805 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:06:56.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6896" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1313,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:06:56.192: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:07:56.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5514" for this suite.

• [SLOW TEST:60.260 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:07:56.454: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:07:56.689: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  9 13:08:01.701: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  9 13:08:01.701: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun  9 13:08:01.775: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9453 /apis/apps/v1/namespaces/deployment-9453/deployments/test-cleanup-deployment 81df2036-53e9-4734-9c2e-3736e981c69f 22476 1 2020-06-09 13:08:01 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f727c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun  9 13:08:01.780: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun  9 13:08:01.780: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun  9 13:08:01.780: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9453 /apis/apps/v1/namespaces/deployment-9453/replicasets/test-cleanup-controller 03bd955b-41ff-4873-82c9-361d9bc92942 22477 1 2020-06-09 13:07:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 81df2036-53e9-4734-9c2e-3736e981c69f 0xc00220d6e7 0xc00220d6e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00220d748 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:08:01.791: INFO: Pod "test-cleanup-controller-qrpl6" is available:
&Pod{ObjectMeta:{test-cleanup-controller-qrpl6 test-cleanup-controller- deployment-9453 /api/v1/namespaces/deployment-9453/pods/test-cleanup-controller-qrpl6 b73e32f9-e06f-4864-b093-f8727a02c169 22468 0 2020-06-09 13:07:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 03bd955b-41ff-4873-82c9-361d9bc92942 0xc00220da77 0xc00220da78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sglqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sglqf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sglqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:07:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:172.24.106.45,StartTime:2020-06-09 13:07:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:07:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4846f6c5a8a3b162827e7298155853c60db8713a93e74b37c1fc7f42c7ce12cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.106.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:01.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9453" for this suite.

• [SLOW TEST:5.396 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":83,"skipped":1348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:01.851: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-70
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun  9 13:08:02.140: INFO: Waiting up to 5m0s for pod "downward-api-e841a85b-3004-410f-8c20-9c2e52092894" in namespace "downward-api-70" to be "success or failure"
Jun  9 13:08:02.147: INFO: Pod "downward-api-e841a85b-3004-410f-8c20-9c2e52092894": Phase="Pending", Reason="", readiness=false. Elapsed: 6.835367ms
Jun  9 13:08:04.154: INFO: Pod "downward-api-e841a85b-3004-410f-8c20-9c2e52092894": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014284104s
Jun  9 13:08:06.162: INFO: Pod "downward-api-e841a85b-3004-410f-8c20-9c2e52092894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022039578s
STEP: Saw pod success
Jun  9 13:08:06.162: INFO: Pod "downward-api-e841a85b-3004-410f-8c20-9c2e52092894" satisfied condition "success or failure"
Jun  9 13:08:06.166: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downward-api-e841a85b-3004-410f-8c20-9c2e52092894 container dapi-container: <nil>
STEP: delete the pod
Jun  9 13:08:06.204: INFO: Waiting for pod downward-api-e841a85b-3004-410f-8c20-9c2e52092894 to disappear
Jun  9 13:08:06.208: INFO: Pod downward-api-e841a85b-3004-410f-8c20-9c2e52092894 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:06.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-70" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:08:06.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377" in namespace "projected-4381" to be "success or failure"
Jun  9 13:08:06.447: INFO: Pod "downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437149ms
Jun  9 13:08:08.453: INFO: Pod "downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012063317s
STEP: Saw pod success
Jun  9 13:08:08.454: INFO: Pod "downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377" satisfied condition "success or failure"
Jun  9 13:08:08.459: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377 container client-container: <nil>
STEP: delete the pod
Jun  9 13:08:08.505: INFO: Waiting for pod downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377 to disappear
Jun  9 13:08:08.511: INFO: Pod downwardapi-volume-3feb31ec-d8fe-4f52-ace7-8c7f93198377 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:08.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4381" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":85,"skipped":1422,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:08.535: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:12.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3632" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":86,"skipped":1438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:12.871: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-8484
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8484 to expose endpoints map[]
Jun  9 13:08:13.132: INFO: Get endpoints failed (13.463243ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun  9 13:08:14.139: INFO: successfully validated that service multi-endpoint-test in namespace services-8484 exposes endpoints map[] (1.020416525s elapsed)
STEP: Creating pod pod1 in namespace services-8484
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8484 to expose endpoints map[pod1:[100]]
Jun  9 13:08:17.227: INFO: successfully validated that service multi-endpoint-test in namespace services-8484 exposes endpoints map[pod1:[100]] (3.064693993s elapsed)
STEP: Creating pod pod2 in namespace services-8484
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8484 to expose endpoints map[pod1:[100] pod2:[101]]
Jun  9 13:08:20.326: INFO: successfully validated that service multi-endpoint-test in namespace services-8484 exposes endpoints map[pod1:[100] pod2:[101]] (3.080812958s elapsed)
STEP: Deleting pod pod1 in namespace services-8484
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8484 to expose endpoints map[pod2:[101]]
Jun  9 13:08:21.368: INFO: successfully validated that service multi-endpoint-test in namespace services-8484 exposes endpoints map[pod2:[101]] (1.035618056s elapsed)
STEP: Deleting pod pod2 in namespace services-8484
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8484 to expose endpoints map[]
Jun  9 13:08:21.390: INFO: successfully validated that service multi-endpoint-test in namespace services-8484 exposes endpoints map[] (9.860169ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:21.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8484" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.590 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":87,"skipped":1514,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:21.467: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun  9 13:08:21.665: INFO: Waiting up to 5m0s for pod "downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9" in namespace "downward-api-9725" to be "success or failure"
Jun  9 13:08:21.670: INFO: Pod "downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145648ms
Jun  9 13:08:23.679: INFO: Pod "downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013676946s
Jun  9 13:08:25.688: INFO: Pod "downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022063987s
STEP: Saw pod success
Jun  9 13:08:25.688: INFO: Pod "downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9" satisfied condition "success or failure"
Jun  9 13:08:25.694: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9 container dapi-container: <nil>
STEP: delete the pod
Jun  9 13:08:25.737: INFO: Waiting for pod downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9 to disappear
Jun  9 13:08:25.746: INFO: Pod downward-api-7daf9d12-8a78-4fe1-a458-0ea80651e7b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9725" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:25.798: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5836
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:08:26.062: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun  9 13:08:30.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 create -f -'
Jun  9 13:08:33.742: INFO: stderr: ""
Jun  9 13:08:33.742: INFO: stdout: "e2e-test-crd-publish-openapi-5966-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  9 13:08:33.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 delete e2e-test-crd-publish-openapi-5966-crds test-foo'
Jun  9 13:08:33.872: INFO: stderr: ""
Jun  9 13:08:33.872: INFO: stdout: "e2e-test-crd-publish-openapi-5966-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  9 13:08:33.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 apply -f -'
Jun  9 13:08:34.187: INFO: stderr: ""
Jun  9 13:08:34.187: INFO: stdout: "e2e-test-crd-publish-openapi-5966-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  9 13:08:34.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 delete e2e-test-crd-publish-openapi-5966-crds test-foo'
Jun  9 13:08:34.297: INFO: stderr: ""
Jun  9 13:08:34.297: INFO: stdout: "e2e-test-crd-publish-openapi-5966-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun  9 13:08:34.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 create -f -'
Jun  9 13:08:35.003: INFO: rc: 1
Jun  9 13:08:35.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 apply -f -'
Jun  9 13:08:35.290: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun  9 13:08:35.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 create -f -'
Jun  9 13:08:35.571: INFO: rc: 1
Jun  9 13:08:35.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-5836 apply -f -'
Jun  9 13:08:36.000: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun  9 13:08:36.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-5966-crds'
Jun  9 13:08:36.401: INFO: stderr: ""
Jun  9 13:08:36.402: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5966-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun  9 13:08:36.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-5966-crds.metadata'
Jun  9 13:08:36.903: INFO: stderr: ""
Jun  9 13:08:36.903: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5966-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  9 13:08:36.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-5966-crds.spec'
Jun  9 13:08:37.185: INFO: stderr: ""
Jun  9 13:08:37.185: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5966-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  9 13:08:37.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-5966-crds.spec.bars'
Jun  9 13:08:37.465: INFO: stderr: ""
Jun  9 13:08:37.465: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5966-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun  9 13:08:37.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-5966-crds.spec.bars2'
Jun  9 13:08:37.758: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:08:42.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5836" for this suite.

• [SLOW TEST:16.765 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":89,"skipped":1571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:08:42.570: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5257
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jun  9 13:08:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:09:08.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5257" for this suite.

• [SLOW TEST:26.121 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":90,"skipped":1598,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:09:08.692: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:09:08.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-2336'
Jun  9 13:09:09.406: INFO: stderr: ""
Jun  9 13:09:09.406: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jun  9 13:09:09.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-2336'
Jun  9 13:09:09.763: INFO: stderr: ""
Jun  9 13:09:09.763: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun  9 13:09:10.772: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:09:10.772: INFO: Found 0 / 1
Jun  9 13:09:11.770: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:09:11.770: INFO: Found 0 / 1
Jun  9 13:09:12.771: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:09:12.772: INFO: Found 1 / 1
Jun  9 13:09:12.772: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  9 13:09:12.780: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:09:12.781: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  9 13:09:12.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 describe pod agnhost-master-z6kbb --namespace=kubectl-2336'
Jun  9 13:09:12.951: INFO: stderr: ""
Jun  9 13:09:12.951: INFO: stdout: "Name:         agnhost-master-z6kbb\nNamespace:    kubectl-2336\nPriority:     0\nNode:         worker-k8xcg-8bbfd5b68-w4htb/172.23.24.246\nStart Time:   Tue, 09 Jun 2020 13:09:09 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.24.106.52\nIPs:\n  IP:           172.24.106.52\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://9d1a70b2257840acd39701286bf0ee7a084391550e93366e5cdb1a64aeda2014\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 Jun 2020 13:09:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ks2f5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ks2f5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ks2f5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                   Message\n  ----    ------     ----  ----                                   -------\n  Normal  Scheduled  3s    default-scheduler                      Successfully assigned kubectl-2336/agnhost-master-z6kbb to worker-k8xcg-8bbfd5b68-w4htb\n  Normal  Pulled     1s    kubelet, worker-k8xcg-8bbfd5b68-w4htb  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s    kubelet, worker-k8xcg-8bbfd5b68-w4htb  Created container agnhost-master\n  Normal  Started    1s    kubelet, worker-k8xcg-8bbfd5b68-w4htb  Started container agnhost-master\n"
Jun  9 13:09:12.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 describe rc agnhost-master --namespace=kubectl-2336'
Jun  9 13:09:13.096: INFO: stderr: ""
Jun  9 13:09:13.096: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2336\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-master-z6kbb\n"
Jun  9 13:09:13.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 describe service agnhost-master --namespace=kubectl-2336'
Jun  9 13:09:13.234: INFO: stderr: ""
Jun  9 13:09:13.234: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2336\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.31.120.229\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.24.106.52:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  9 13:09:13.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 describe node master-ovo8j-6dbbb47d57-c8cf7'
Jun  9 13:09:13.411: INFO: stderr: ""
Jun  9 13:09:13.412: INFO: stdout: "Name:               master-ovo8j-6dbbb47d57-c8cf7\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    giantswarm.io/provider=kvm\n                    ip=172.23.24.182\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-ovo8j-6dbbb47d57-c8cf7\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    kvm-operator.giantswarm.io/version=3.11.2-dev\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/master=\n                    role=master\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 Jun 2020 11:41:33 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-ovo8j-6dbbb47d57-c8cf7\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 09 Jun 2020 13:09:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 Jun 2020 11:43:05 +0000   Tue, 09 Jun 2020 11:43:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 09 Jun 2020 13:08:53 +0000   Tue, 09 Jun 2020 11:41:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 Jun 2020 13:08:53 +0000   Tue, 09 Jun 2020 11:41:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 Jun 2020 13:08:53 +0000   Tue, 09 Jun 2020 11:41:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 Jun 2020 13:08:53 +0000   Tue, 09 Jun 2020 11:44:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.23.24.182\n  Hostname:    master-ovo8j-6dbbb47d57-c8cf7\nCapacity:\n  cpu:                2\n  ephemeral-storage:  5110Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8168288Ki\n  pods:               110\nAllocatable:\n  cpu:                1400m\n  ephemeral-storage:  4086Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6259552Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9b511736c8bf4d13b3935ac90f8af177\n  System UUID:                9b511736c8bf4d13b3935ac90f8af177\n  Boot ID:                    322f0969-8107-41ad-8eda-43cf90f191a9\n  Kernel Version:             4.19.107-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 2345.3.1 (Rhyolite)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.3\n  Kubelet Version:            v1.17.6\n  Kube-Proxy Version:         v1.17.6\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-659d7d6665-kgd94                   250m (17%)    0 (0%)      100Mi (1%)       0 (0%)         85m\n  kube-system                 calico-node-fcvzc                                          250m (17%)    50m (3%)    150Mi (2%)       100Mi (1%)     86m\n  kube-system                 cert-exporter-hlnr7                                        50m (3%)      0 (0%)      50Mi (0%)        50Mi (0%)      84m\n  kube-system                 k8s-api-healthz-master-ovo8j-6dbbb47d57-c8cf7              50m (3%)      0 (0%)      20Mi (0%)        0 (0%)         86m\n  kube-system                 k8s-api-server-master-ovo8j-6dbbb47d57-c8cf7               300m (21%)    0 (0%)      300Mi (4%)       0 (0%)         86m\n  kube-system                 k8s-controller-manager-master-ovo8j-6dbbb47d57-c8cf7       200m (14%)    0 (0%)      200Mi (3%)       0 (0%)         86m\n  kube-system                 k8s-scheduler-master-ovo8j-6dbbb47d57-c8cf7                100m (7%)     0 (0%)      100Mi (1%)       0 (0%)         86m\n  kube-system                 kube-proxy-7ddx8                                           75m (5%)      0 (0%)      80Mi (1%)        0 (0%)         85m\n  kube-system                 net-exporter-tf9ls                                         50m (3%)      0 (0%)      75Mi (1%)        75Mi (1%)      84m\n  kube-system                 node-exporter-v7vsk                                        75m (5%)      0 (0%)      50Mi (0%)        50Mi (0%)      84m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-pdwvb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1400m (100%)  50m (3%)\n  memory             1125Mi (18%)  275Mi (4%)\n  ephemeral-storage  0 (0%)        0 (0%)\nEvents:              <none>\n"
Jun  9 13:09:13.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 describe namespace kubectl-2336'
Jun  9 13:09:13.562: INFO: stderr: ""
Jun  9 13:09:13.562: INFO: stdout: "Name:         kubectl-2336\nLabels:       e2e-framework=kubectl\n              e2e-run=40ee9106-dde4-44a5-b5bb-60b416b406d7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:09:13.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2336" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":91,"skipped":1618,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:09:13.587: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:09:13.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0" in namespace "downward-api-3936" to be "success or failure"
Jun  9 13:09:13.819: INFO: Pod "downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.151054ms
Jun  9 13:09:15.828: INFO: Pod "downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014227437s
Jun  9 13:09:17.835: INFO: Pod "downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020589269s
STEP: Saw pod success
Jun  9 13:09:17.835: INFO: Pod "downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0" satisfied condition "success or failure"
Jun  9 13:09:17.838: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0 container client-container: <nil>
STEP: delete the pod
Jun  9 13:09:17.878: INFO: Waiting for pod downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0 to disappear
Jun  9 13:09:17.887: INFO: Pod downwardapi-volume-2e381e28-005b-4a3f-bdfc-e94b3eeedcf0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:09:17.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3936" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1619,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:09:17.927: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7900
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:09:18.147: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  9 13:09:23.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7900 create -f -'
Jun  9 13:09:24.966: INFO: stderr: ""
Jun  9 13:09:24.966: INFO: stdout: "e2e-test-crd-publish-openapi-8830-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  9 13:09:24.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7900 delete e2e-test-crd-publish-openapi-8830-crds test-cr'
Jun  9 13:09:25.104: INFO: stderr: ""
Jun  9 13:09:25.104: INFO: stdout: "e2e-test-crd-publish-openapi-8830-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  9 13:09:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7900 apply -f -'
Jun  9 13:09:25.436: INFO: stderr: ""
Jun  9 13:09:25.436: INFO: stdout: "e2e-test-crd-publish-openapi-8830-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  9 13:09:25.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7900 delete e2e-test-crd-publish-openapi-8830-crds test-cr'
Jun  9 13:09:25.603: INFO: stderr: ""
Jun  9 13:09:25.603: INFO: stdout: "e2e-test-crd-publish-openapi-8830-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun  9 13:09:25.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-8830-crds'
Jun  9 13:09:26.418: INFO: stderr: ""
Jun  9 13:09:26.418: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8830-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:09:31.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7900" for this suite.

• [SLOW TEST:13.963 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":93,"skipped":1625,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:09:31.890: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:09:32.078: INFO: Creating deployment "test-recreate-deployment"
Jun  9 13:09:32.086: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  9 13:09:32.100: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun  9 13:09:34.114: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  9 13:09:34.118: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304972, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304972, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304972, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727304972, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:09:36.183: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  9 13:09:36.198: INFO: Updating deployment test-recreate-deployment
Jun  9 13:09:36.198: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun  9 13:09:36.478: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4466 /apis/apps/v1/namespaces/deployment-4466/deployments/test-recreate-deployment 7be09a23-59f8-4f89-a66e-b37cecc24bac 23223 2 2020-06-09 13:09:32 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b56b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-09 13:09:36 +0000 UTC,LastTransitionTime:2020-06-09 13:09:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-06-09 13:09:36 +0000 UTC,LastTransitionTime:2020-06-09 13:09:32 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  9 13:09:36.490: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-4466 /apis/apps/v1/namespaces/deployment-4466/replicasets/test-recreate-deployment-5f94c574ff 4d04c2ff-5821-4cb8-b8ea-c0d66678124e 23221 1 2020-06-09 13:09:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 7be09a23-59f8-4f89-a66e-b37cecc24bac 0xc006b56f97 0xc006b56f98}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b56ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:09:36.490: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  9 13:09:36.490: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-4466 /apis/apps/v1/namespaces/deployment-4466/replicasets/test-recreate-deployment-799c574856 72777e51-ec4a-4e62-a8af-e8fbf9095aa3 23210 2 2020-06-09 13:09:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 7be09a23-59f8-4f89-a66e-b37cecc24bac 0xc006b57067 0xc006b57068}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b570d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:09:36.498: INFO: Pod "test-recreate-deployment-5f94c574ff-qvc59" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-qvc59 test-recreate-deployment-5f94c574ff- deployment-4466 /api/v1/namespaces/deployment-4466/pods/test-recreate-deployment-5f94c574ff-qvc59 d4bc9ae7-ea16-4f80-b7a2-99161729bd27 23220 0 2020-06-09 13:09:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 4d04c2ff-5821-4cb8-b8ea-c0d66678124e 0xc006b575d7 0xc006b575d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lbpnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lbpnv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lbpnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:09:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:09:36.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4466" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":94,"skipped":1633,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:09:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5337
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jun  9 13:09:36.778: INFO: Found 0 stateful pods, waiting for 3
Jun  9 13:09:46.785: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:09:46.785: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:09:46.785: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun  9 13:09:46.832: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun  9 13:09:56.884: INFO: Updating stateful set ss2
Jun  9 13:09:56.899: INFO: Waiting for Pod statefulset-5337/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun  9 13:10:06.998: INFO: Found 1 stateful pods, waiting for 3
Jun  9 13:10:17.007: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:10:17.007: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:10:17.007: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun  9 13:10:17.040: INFO: Updating stateful set ss2
Jun  9 13:10:17.055: INFO: Waiting for Pod statefulset-5337/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  9 13:10:37.087: INFO: Updating stateful set ss2
Jun  9 13:10:37.118: INFO: Waiting for StatefulSet statefulset-5337/ss2 to complete update
Jun  9 13:10:37.118: INFO: Waiting for Pod statefulset-5337/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  9 13:10:47.131: INFO: Waiting for StatefulSet statefulset-5337/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun  9 13:10:57.137: INFO: Deleting all statefulset in ns statefulset-5337
Jun  9 13:10:57.141: INFO: Scaling statefulset ss2 to 0
Jun  9 13:11:17.189: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:11:17.193: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:11:17.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5337" for this suite.

• [SLOW TEST:100.720 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":95,"skipped":1633,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:11:17.253: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:11:33.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2400" for this suite.

• [SLOW TEST:16.342 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":96,"skipped":1636,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:11:33.596: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-30c0df81-45d3-4bd2-a607-7346b11d845a
STEP: Creating a pod to test consume secrets
Jun  9 13:11:33.818: INFO: Waiting up to 5m0s for pod "pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da" in namespace "secrets-1328" to be "success or failure"
Jun  9 13:11:33.823: INFO: Pod "pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.561509ms
Jun  9 13:11:35.846: INFO: Pod "pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02782237s
STEP: Saw pod success
Jun  9 13:11:35.846: INFO: Pod "pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da" satisfied condition "success or failure"
Jun  9 13:11:35.853: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da container secret-env-test: <nil>
STEP: delete the pod
Jun  9 13:11:35.924: INFO: Waiting for pod pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da to disappear
Jun  9 13:11:35.932: INFO: Pod pod-secrets-1a78775e-e9bc-496a-990c-601e52d335da no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:11:35.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1328" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":97,"skipped":1646,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:11:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-72f2bfae-8c3b-4c7d-8907-f91b1fc87d22
STEP: Creating a pod to test consume configMaps
Jun  9 13:11:36.215: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71" in namespace "configmap-2472" to be "success or failure"
Jun  9 13:11:36.225: INFO: Pod "pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71": Phase="Pending", Reason="", readiness=false. Elapsed: 9.825428ms
Jun  9 13:11:38.231: INFO: Pod "pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0159439s
Jun  9 13:11:40.239: INFO: Pod "pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024510027s
STEP: Saw pod success
Jun  9 13:11:40.239: INFO: Pod "pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71" satisfied condition "success or failure"
Jun  9 13:11:40.245: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 13:11:40.313: INFO: Waiting for pod pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71 to disappear
Jun  9 13:11:40.317: INFO: Pod pod-configmaps-ec1f0541-f6ee-4c47-ad91-4c9e4ebe6d71 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:11:40.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2472" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1647,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:11:40.370: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:11:41.431: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun  9 13:11:43.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305101, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305101, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305101, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305101, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:11:46.520: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:11:46.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2840" for this suite.
STEP: Destroying namespace "webhook-2840-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.534 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":99,"skipped":1647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:11:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun  9 13:11:47.159: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 13:11:47.195: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 13:11:47.205: INFO: 
Logging pods the kubelet thinks is on node worker-2jqhr-6f5dbbb884-vqc7c before test
Jun  9 13:11:47.230: INFO: coredns-6d56c484c-txwjd from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.231: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:11:47.231: INFO: metrics-server-66df9f5b56-bvwdm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.231: INFO: 	Container metrics-server ready: true, restart count 0
Jun  9 13:11:47.231: INFO: net-exporter-4n4fk from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.231: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:11:47.231: INFO: docker-mem-limit-startup-script-clstp from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.231: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:11:47.232: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-sd6nl from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:11:47.232: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:11:47.232: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:11:47.232: INFO: calico-node-vhcbj from kube-system started at 2020-06-09 11:42:16 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.232: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:11:47.232: INFO: node-exporter-nsmh9 from kube-system started at 2020-06-09 11:45:08 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.232: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:11:47.232: INFO: nginx-ingress-controller-59b5c95c6d-cb4kl from kube-system started at 2020-06-09 11:45:21 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.233: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 13:11:47.233: INFO: coredns-6d56c484c-7fkzm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.233: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:11:47.233: INFO: kube-proxy-69gzh from kube-system started at 2020-06-09 11:43:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.233: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:11:47.233: INFO: chart-operator-5b8b4bcc75-wn4xh from giantswarm started at 2020-06-09 11:44:47 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.233: INFO: 	Container chart-operator ready: true, restart count 0
Jun  9 13:11:47.234: INFO: cert-exporter-gkbjk from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.234: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:11:47.234: INFO: tiller-deploy-684c6b545b-4w7nq from giantswarm started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.234: INFO: 	Container tiller ready: true, restart count 0
Jun  9 13:11:47.234: INFO: 
Logging pods the kubelet thinks is on node worker-dfhc8-64bc8fc496-xx7cx before test
Jun  9 13:11:47.258: INFO: sonobuoy-e2e-job-fb32098c60e64727 from sonobuoy started at 2020-06-09 12:47:22 +0000 UTC (2 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container e2e ready: true, restart count 0
Jun  9 13:11:47.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:11:47.259: INFO: kube-state-metrics-6d998ffd8b-wtr6p from kube-system started at 2020-06-09 11:44:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  9 13:11:47.259: INFO: docker-mem-limit-startup-script-9x4w8 from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:11:47.259: INFO: kube-proxy-tg6rp from kube-system started at 2020-06-09 11:43:24 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:11:47.259: INFO: nginx-ingress-controller-59b5c95c6d-8z4jm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 13:11:47.259: INFO: coredns-6d56c484c-s5fsw from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:11:47.259: INFO: net-exporter-wvnc9 from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:11:47.259: INFO: node-exporter-k9z45 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:11:47.259: INFO: sonobuoy from sonobuoy started at 2020-06-09 12:47:13 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 13:11:47.259: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-p5v44 from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:11:47.259: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:11:47.259: INFO: calico-node-7t79n from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:11:47.259: INFO: cert-exporter-v64nm from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.259: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:11:47.259: INFO: 
Logging pods the kubelet thinks is on node worker-k8xcg-8bbfd5b68-w4htb before test
Jun  9 13:11:47.279: INFO: docker-mem-limit-startup-script-74z86 from kube-system started at 2020-06-09 12:55:26 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.279: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:11:47.279: INFO: sample-webhook-deployment-5f65f8c764-l7zpp from webhook-2840 started at 2020-06-09 13:11:41 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.280: INFO: 	Container sample-webhook ready: true, restart count 0
Jun  9 13:11:47.280: INFO: cert-exporter-p2wws from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.280: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:11:47.280: INFO: net-exporter-hppdh from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.280: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:11:47.280: INFO: kube-proxy-kfmds from kube-system started at 2020-06-09 11:43:19 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.280: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:11:47.280: INFO: calico-node-zqtv2 from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.280: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:11:47.280: INFO: node-exporter-mbg29 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 13:11:47.280: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:11:47.281: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-lbr8h from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:11:47.281: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:11:47.281: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node worker-2jqhr-6f5dbbb884-vqc7c
STEP: verifying the node has the label node worker-dfhc8-64bc8fc496-xx7cx
STEP: verifying the node has the label node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod chart-operator-5b8b4bcc75-wn4xh requesting resource cpu=250m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod tiller-deploy-684c6b545b-4w7nq requesting resource cpu=0m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod calico-node-7t79n requesting resource cpu=250m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod calico-node-vhcbj requesting resource cpu=250m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod calico-node-zqtv2 requesting resource cpu=250m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod cert-exporter-gkbjk requesting resource cpu=50m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod cert-exporter-p2wws requesting resource cpu=50m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod cert-exporter-v64nm requesting resource cpu=50m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod coredns-6d56c484c-7fkzm requesting resource cpu=250m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod coredns-6d56c484c-s5fsw requesting resource cpu=250m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod coredns-6d56c484c-txwjd requesting resource cpu=250m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod docker-mem-limit-startup-script-74z86 requesting resource cpu=10m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod docker-mem-limit-startup-script-9x4w8 requesting resource cpu=10m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod docker-mem-limit-startup-script-clstp requesting resource cpu=10m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod kube-proxy-69gzh requesting resource cpu=75m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod kube-proxy-kfmds requesting resource cpu=75m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod kube-proxy-tg6rp requesting resource cpu=75m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod kube-state-metrics-6d998ffd8b-wtr6p requesting resource cpu=500m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod metrics-server-66df9f5b56-bvwdm requesting resource cpu=0m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod net-exporter-4n4fk requesting resource cpu=50m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod net-exporter-hppdh requesting resource cpu=50m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod net-exporter-wvnc9 requesting resource cpu=50m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod nginx-ingress-controller-59b5c95c6d-8z4jm requesting resource cpu=500m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod nginx-ingress-controller-59b5c95c6d-cb4kl requesting resource cpu=500m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod node-exporter-k9z45 requesting resource cpu=75m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod node-exporter-mbg29 requesting resource cpu=75m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod node-exporter-nsmh9 requesting resource cpu=75m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod sonobuoy-e2e-job-fb32098c60e64727 requesting resource cpu=0m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-lbr8h requesting resource cpu=0m on Node worker-k8xcg-8bbfd5b68-w4htb
Jun  9 13:11:47.464: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-p5v44 requesting resource cpu=0m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.464: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-sd6nl requesting resource cpu=0m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.464: INFO: Pod sample-webhook-deployment-5f65f8c764-l7zpp requesting resource cpu=0m on Node worker-k8xcg-8bbfd5b68-w4htb
STEP: Starting Pods to consume most of the cluster CPU.
Jun  9 13:11:47.464: INFO: Creating a pod which consumes cpu=1218m on Node worker-2jqhr-6f5dbbb884-vqc7c
Jun  9 13:11:47.487: INFO: Creating a pod which consumes cpu=1218m on Node worker-dfhc8-64bc8fc496-xx7cx
Jun  9 13:11:47.504: INFO: Creating a pod which consumes cpu=2093m on Node worker-k8xcg-8bbfd5b68-w4htb
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576.1616e245c1949ee2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7120/filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576 to worker-dfhc8-64bc8fc496-xx7cx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576.1616e246081ac07a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576.1616e2460dcbed45], Reason = [Created], Message = [Created container filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576.1616e246270420b8], Reason = [Started], Message = [Started container filler-pod-4cae6c26-3a80-489c-8e73-b4d6d843a576]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269.1616e245c025bb0b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7120/filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269 to worker-2jqhr-6f5dbbb884-vqc7c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269.1616e2460b4ee8f1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269.1616e24611d61cae], Reason = [Created], Message = [Created container filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269.1616e24630894cb2], Reason = [Started], Message = [Started container filler-pod-ea02c169-b85c-4d8c-b7d0-7e2bb177f269]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed.1616e245c3f478ad], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7120/filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed to worker-k8xcg-8bbfd5b68-w4htb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed.1616e24614152dce], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed.1616e2463d0a9801], Reason = [Created], Message = [Created container filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed.1616e24659800792], Reason = [Started], Message = [Started container filler-pod-ebaf13bc-06ed-4520-92a5-4c23d23285ed]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1616e246b47d9265], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node worker-2jqhr-6f5dbbb884-vqc7c
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-dfhc8-64bc8fc496-xx7cx
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-k8xcg-8bbfd5b68-w4htb
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:11:52.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7120" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:5.890 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":100,"skipped":1680,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:11:52.795: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6441
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6441
STEP: creating replication controller externalsvc in namespace services-6441
I0609 13:11:53.110179      24 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6441, replica count: 2
I0609 13:11:56.161078      24 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 13:11:59.161479      24 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun  9 13:11:59.216: INFO: Creating new exec pod
Jun  9 13:12:03.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-6441 execpodrzhw5 -- /bin/sh -x -c nslookup clusterip-service'
Jun  9 13:12:03.596: INFO: stderr: "+ nslookup clusterip-service\n"
Jun  9 13:12:03.596: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nclusterip-service.services-6441.svc.cluster.local\tcanonical name = externalsvc.services-6441.svc.cluster.local.\nName:\texternalsvc.services-6441.svc.cluster.local\nAddress: 172.31.15.129\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6441, will wait for the garbage collector to delete the pods
Jun  9 13:12:03.667: INFO: Deleting ReplicationController externalsvc took: 13.76688ms
Jun  9 13:12:03.768: INFO: Terminating ReplicationController externalsvc pods took: 100.779215ms
Jun  9 13:12:17.200: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:12:17.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6441" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:24.451 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":101,"skipped":1685,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:12:17.246: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:12:17.491: INFO: Creating ReplicaSet my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4
Jun  9 13:12:17.522: INFO: Pod name my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4: Found 0 pods out of 1
Jun  9 13:12:22.537: INFO: Pod name my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4: Found 1 pods out of 1
Jun  9 13:12:22.537: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4" is running
Jun  9 13:12:22.542: INFO: Pod "my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4-pd6nh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:12:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:12:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:12:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:12:17 +0000 UTC Reason: Message:}])
Jun  9 13:12:22.543: INFO: Trying to dial the pod
Jun  9 13:12:27.575: INFO: Controller my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4: Got expected result from replica 1 [my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4-pd6nh]: "my-hostname-basic-8d5c7f72-4a1e-4cba-8d38-ea13241bcba4-pd6nh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:12:27.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9327" for this suite.

• [SLOW TEST:10.350 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":102,"skipped":1686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:12:27.597: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9217.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9217.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9217.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9217.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9217.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9217.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:12:31.903: INFO: Unable to read jessie_udp@PodARecord from pod dns-9217/dns-test-72f6f9af-ef8e-4c49-8090-3d8dc8d53c45: the server could not find the requested resource (get pods dns-test-72f6f9af-ef8e-4c49-8090-3d8dc8d53c45)
Jun  9 13:12:31.908: INFO: Lookups using dns-9217/dns-test-72f6f9af-ef8e-4c49-8090-3d8dc8d53c45 failed for: [jessie_udp@PodARecord]

Jun  9 13:12:36.972: INFO: DNS probes using dns-9217/dns-test-72f6f9af-ef8e-4c49-8090-3d8dc8d53c45 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:12:37.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9217" for this suite.

• [SLOW TEST:9.465 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":103,"skipped":1738,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:12:37.062: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-51b99c09-f91e-4f5f-87f5-a9beed7ff9b3
STEP: Creating a pod to test consume configMaps
Jun  9 13:12:37.263: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93" in namespace "projected-4438" to be "success or failure"
Jun  9 13:12:37.269: INFO: Pod "pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909616ms
Jun  9 13:12:39.274: INFO: Pod "pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010643131s
STEP: Saw pod success
Jun  9 13:12:39.274: INFO: Pod "pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93" satisfied condition "success or failure"
Jun  9 13:12:39.277: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 13:12:39.331: INFO: Waiting for pod pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93 to disappear
Jun  9 13:12:39.346: INFO: Pod pod-projected-configmaps-48406c43-2471-4d70-808c-9e63c63f7f93 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:12:39.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4438" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:12:39.369: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-192
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-192.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-192.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-192.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-192.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:12:43.695: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.703: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.711: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-192.svc.cluster.local from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.724: INFO: Unable to read wheezy_udp@PodARecord from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.730: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.739: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.760: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-192.svc.cluster.local from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.767: INFO: Unable to read jessie_udp@PodARecord from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.776: INFO: Unable to read jessie_tcp@PodARecord from pod dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7: the server could not find the requested resource (get pods dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7)
Jun  9 13:12:43.776: INFO: Lookups using dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local wheezy_udp@dns-test-service-2.dns-192.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-192.svc.cluster.local jessie_tcp@dns-test-service-2.dns-192.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  9 13:12:48.869: INFO: DNS probes using dns-192/dns-test-1332a1c5-e760-4964-ab54-dfe3a27302a7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:12:48.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-192" for this suite.

• [SLOW TEST:9.580 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":105,"skipped":1768,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:12:48.949: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:12:49.168: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3eb18be9-2b9f-4c29-a687-37ae76d7465f" in namespace "security-context-test-3321" to be "success or failure"
Jun  9 13:12:49.173: INFO: Pod "busybox-readonly-false-3eb18be9-2b9f-4c29-a687-37ae76d7465f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.132527ms
Jun  9 13:12:51.184: INFO: Pod "busybox-readonly-false-3eb18be9-2b9f-4c29-a687-37ae76d7465f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016026321s
Jun  9 13:12:53.191: INFO: Pod "busybox-readonly-false-3eb18be9-2b9f-4c29-a687-37ae76d7465f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022914512s
Jun  9 13:12:53.191: INFO: Pod "busybox-readonly-false-3eb18be9-2b9f-4c29-a687-37ae76d7465f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:12:53.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3321" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1769,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:12:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:12:53.525: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  9 13:12:58.531: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  9 13:12:58.532: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  9 13:13:00.538: INFO: Creating deployment "test-rollover-deployment"
Jun  9 13:13:00.555: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  9 13:13:02.577: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  9 13:13:02.587: INFO: Ensure that both replica sets have 1 created replica
Jun  9 13:13:02.599: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  9 13:13:02.611: INFO: Updating deployment test-rollover-deployment
Jun  9 13:13:02.611: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  9 13:13:04.637: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  9 13:13:04.646: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  9 13:13:04.654: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 13:13:04.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305182, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:06.664: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 13:13:06.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305185, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:08.667: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 13:13:08.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305185, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:10.664: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 13:13:10.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305185, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:12.665: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 13:13:12.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305185, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:14.664: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 13:13:14.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305185, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305180, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:16.666: INFO: 
Jun  9 13:13:16.666: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun  9 13:13:16.679: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6701 /apis/apps/v1/namespaces/deployment-6701/deployments/test-rollover-deployment 39489145-d187-48e6-aae6-c464e46f2f93 24913 2 2020-06-09 13:13:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002086d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-09 13:13:00 +0000 UTC,LastTransitionTime:2020-06-09 13:13:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-06-09 13:13:15 +0000 UTC,LastTransitionTime:2020-06-09 13:13:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 13:13:16.684: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-6701 /apis/apps/v1/namespaces/deployment-6701/replicasets/test-rollover-deployment-574d6dfbff 1fccfab1-0ea5-4378-931a-aa3b50683a0f 24902 2 2020-06-09 13:13:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 39489145-d187-48e6-aae6-c464e46f2f93 0xc0020871e7 0xc0020871e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002087258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:13:16.684: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  9 13:13:16.684: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6701 /apis/apps/v1/namespaces/deployment-6701/replicasets/test-rollover-controller b3edbc97-88bb-46c0-a0d3-2c6777875b35 24912 2 2020-06-09 13:12:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 39489145-d187-48e6-aae6-c464e46f2f93 0xc002087117 0xc002087118}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002087178 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:13:16.684: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-6701 /apis/apps/v1/namespaces/deployment-6701/replicasets/test-rollover-deployment-f6c94f66c 7752fe28-65f9-4066-ae61-ac0988881a0d 24852 2 2020-06-09 13:13:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 39489145-d187-48e6-aae6-c464e46f2f93 0xc0020872c0 0xc0020872c1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002087338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:13:16.689: INFO: Pod "test-rollover-deployment-574d6dfbff-t55wt" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-t55wt test-rollover-deployment-574d6dfbff- deployment-6701 /api/v1/namespaces/deployment-6701/pods/test-rollover-deployment-574d6dfbff-t55wt 549d439e-1ac7-4bf4-88aa-046ed6df89d8 24873 0 2020-06-09 13:13:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 1fccfab1-0ea5-4378-931a-aa3b50683a0f 0xc0020878c7 0xc0020878c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n4cb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n4cb4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n4cb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:172.24.106.15,StartTime:2020-06-09 13:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://8163c1eda9b8b844f9e1c94cd33bd48c0c200160248351251b6f3ad6e4947f28,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.106.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:16.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6701" for this suite.

• [SLOW TEST:23.504 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":107,"skipped":1769,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:16.709: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun  9 13:13:16.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-673 /api/v1/namespaces/watch-673/configmaps/e2e-watch-test-watch-closed 2f6de13e-7fc5-47cd-960f-dba9c3a7cf6b 24933 0 2020-06-09 13:13:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  9 13:13:16.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-673 /api/v1/namespaces/watch-673/configmaps/e2e-watch-test-watch-closed 2f6de13e-7fc5-47cd-960f-dba9c3a7cf6b 24934 0 2020-06-09 13:13:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun  9 13:13:16.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-673 /api/v1/namespaces/watch-673/configmaps/e2e-watch-test-watch-closed 2f6de13e-7fc5-47cd-960f-dba9c3a7cf6b 24935 0 2020-06-09 13:13:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  9 13:13:16.939: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-673 /api/v1/namespaces/watch-673/configmaps/e2e-watch-test-watch-closed 2f6de13e-7fc5-47cd-960f-dba9c3a7cf6b 24936 0 2020-06-09 13:13:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:16.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-673" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":108,"skipped":1776,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:17.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8941" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":109,"skipped":1780,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:17.194: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-5130
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:17.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5130" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":110,"skipped":1792,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:17.469: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3146/configmap-test-c8d5aa73-01e6-40bb-a1da-914019299800
STEP: Creating a pod to test consume configMaps
Jun  9 13:13:17.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b" in namespace "configmap-3146" to be "success or failure"
Jun  9 13:13:17.723: INFO: Pod "pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.252561ms
Jun  9 13:13:19.730: INFO: Pod "pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028365982s
Jun  9 13:13:21.760: INFO: Pod "pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058613441s
STEP: Saw pod success
Jun  9 13:13:21.760: INFO: Pod "pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b" satisfied condition "success or failure"
Jun  9 13:13:21.775: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b container env-test: <nil>
STEP: delete the pod
Jun  9 13:13:21.872: INFO: Waiting for pod pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b to disappear
Jun  9 13:13:21.893: INFO: Pod pod-configmaps-e27a40a9-1282-4b3f-9727-16bac7faca8b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:21.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3146" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1807,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:21.920: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6952.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6952.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:13:26.320: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6952/dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0: the server could not find the requested resource (get pods dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0)
Jun  9 13:13:26.326: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6952/dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0: the server could not find the requested resource (get pods dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0)
Jun  9 13:13:26.345: INFO: Unable to read jessie_udp@PodARecord from pod dns-6952/dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0: the server could not find the requested resource (get pods dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0)
Jun  9 13:13:26.353: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6952/dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0: the server could not find the requested resource (get pods dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0)
Jun  9 13:13:26.354: INFO: Lookups using dns-6952/dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  9 13:13:31.407: INFO: DNS probes using dns-6952/dns-test-e4a5b72d-03f8-42ab-89b6-51c706ce94f0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:31.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6952" for this suite.

• [SLOW TEST:9.524 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":112,"skipped":1811,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun  9 13:13:31.642: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jun  9 13:13:32.455: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun  9 13:13:34.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:36.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:38.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:40.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:42.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:44.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:46.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:48.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:50.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:52.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305212, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:13:56.409: INFO: Waited 1.839302342s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:13:57.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6235" for this suite.

• [SLOW TEST:26.104 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":113,"skipped":1814,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:13:57.550: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  9 13:13:57.969: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:13:57.976: INFO: Number of nodes with available pods: 0
Jun  9 13:13:57.976: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:13:58.986: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:13:58.993: INFO: Number of nodes with available pods: 0
Jun  9 13:13:58.993: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:13:59.987: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:13:59.999: INFO: Number of nodes with available pods: 0
Jun  9 13:13:59.999: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:14:00.984: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:00.990: INFO: Number of nodes with available pods: 3
Jun  9 13:14:00.990: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun  9 13:14:01.018: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:01.024: INFO: Number of nodes with available pods: 2
Jun  9 13:14:01.024: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:02.035: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:02.041: INFO: Number of nodes with available pods: 2
Jun  9 13:14:02.041: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:03.033: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:03.040: INFO: Number of nodes with available pods: 2
Jun  9 13:14:03.040: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:04.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:04.039: INFO: Number of nodes with available pods: 2
Jun  9 13:14:04.040: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:05.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:05.041: INFO: Number of nodes with available pods: 2
Jun  9 13:14:05.041: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:06.045: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:06.056: INFO: Number of nodes with available pods: 2
Jun  9 13:14:06.056: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:07.033: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:07.039: INFO: Number of nodes with available pods: 2
Jun  9 13:14:07.039: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:08.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:08.039: INFO: Number of nodes with available pods: 2
Jun  9 13:14:08.039: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:09.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:09.040: INFO: Number of nodes with available pods: 2
Jun  9 13:14:09.040: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:10.043: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:10.055: INFO: Number of nodes with available pods: 2
Jun  9 13:14:10.055: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:11.042: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:11.049: INFO: Number of nodes with available pods: 2
Jun  9 13:14:11.049: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:12.033: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:12.039: INFO: Number of nodes with available pods: 2
Jun  9 13:14:12.039: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:13.038: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:13.048: INFO: Number of nodes with available pods: 2
Jun  9 13:14:13.049: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:14.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:14.038: INFO: Number of nodes with available pods: 2
Jun  9 13:14:14.038: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:15.035: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:15.041: INFO: Number of nodes with available pods: 2
Jun  9 13:14:15.041: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:16.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:16.040: INFO: Number of nodes with available pods: 2
Jun  9 13:14:16.040: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:17.033: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:17.038: INFO: Number of nodes with available pods: 2
Jun  9 13:14:17.038: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:18.035: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:18.044: INFO: Number of nodes with available pods: 2
Jun  9 13:14:18.044: INFO: Node worker-k8xcg-8bbfd5b68-w4htb is running more than one daemon pod
Jun  9 13:14:19.034: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:14:19.041: INFO: Number of nodes with available pods: 3
Jun  9 13:14:19.041: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1004, will wait for the garbage collector to delete the pods
Jun  9 13:14:19.112: INFO: Deleting DaemonSet.extensions daemon-set took: 10.92923ms
Jun  9 13:14:19.213: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.559407ms
Jun  9 13:14:23.119: INFO: Number of nodes with available pods: 0
Jun  9 13:14:23.119: INFO: Number of running nodes: 0, number of available pods: 0
Jun  9 13:14:23.130: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1004/daemonsets","resourceVersion":"25504"},"items":null}

Jun  9 13:14:23.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1004/pods","resourceVersion":"25504"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:14:23.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1004" for this suite.

• [SLOW TEST:25.633 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":114,"skipped":1817,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:14:23.183: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-1446f113-df72-410f-8aa7-2173ccfa83bf
STEP: Creating a pod to test consume configMaps
Jun  9 13:14:23.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294" in namespace "projected-2184" to be "success or failure"
Jun  9 13:14:23.452: INFO: Pod "pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294": Phase="Pending", Reason="", readiness=false. Elapsed: 4.911331ms
Jun  9 13:14:25.459: INFO: Pod "pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012039425s
Jun  9 13:14:27.465: INFO: Pod "pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018512175s
STEP: Saw pod success
Jun  9 13:14:27.465: INFO: Pod "pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294" satisfied condition "success or failure"
Jun  9 13:14:27.470: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 13:14:27.497: INFO: Waiting for pod pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294 to disappear
Jun  9 13:14:27.501: INFO: Pod pod-projected-configmaps-5201d413-bac9-4317-9aaf-c730d89ef294 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:14:27.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2184" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":115,"skipped":1824,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:14:27.520: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  9 13:14:27.738: INFO: Waiting up to 5m0s for pod "pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210" in namespace "emptydir-5409" to be "success or failure"
Jun  9 13:14:27.750: INFO: Pod "pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210": Phase="Pending", Reason="", readiness=false. Elapsed: 11.273071ms
Jun  9 13:14:29.759: INFO: Pod "pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019930682s
Jun  9 13:14:31.766: INFO: Pod "pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027197521s
STEP: Saw pod success
Jun  9 13:14:31.766: INFO: Pod "pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210" satisfied condition "success or failure"
Jun  9 13:14:31.771: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210 container test-container: <nil>
STEP: delete the pod
Jun  9 13:14:31.809: INFO: Waiting for pod pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210 to disappear
Jun  9 13:14:31.814: INFO: Pod pod-a93a1d18-cc7c-4c8b-9d5b-d425df738210 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:14:31.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5409" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":1824,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:14:31.830: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1814
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1814
I0609 13:14:32.055141      24 runners.go:189] Created replication controller with name: externalname-service, namespace: services-1814, replica count: 2
I0609 13:14:35.106017      24 runners.go:189] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 13:14:38.106: INFO: Creating new exec pod
I0609 13:14:38.106558      24 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 13:14:43.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-1814 execpodtj2j5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun  9 13:14:43.544: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  9 13:14:43.544: INFO: stdout: ""
Jun  9 13:14:43.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-1814 execpodtj2j5 -- /bin/sh -x -c nc -zv -t -w 2 172.31.212.218 80'
Jun  9 13:14:43.896: INFO: stderr: "+ nc -zv -t -w 2 172.31.212.218 80\nConnection to 172.31.212.218 80 port [tcp/http] succeeded!\n"
Jun  9 13:14:43.896: INFO: stdout: ""
Jun  9 13:14:43.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-1814 execpodtj2j5 -- /bin/sh -x -c nc -zv -t -w 2 172.23.24.38 31777'
Jun  9 13:14:44.208: INFO: stderr: "+ nc -zv -t -w 2 172.23.24.38 31777\nConnection to 172.23.24.38 31777 port [tcp/31777] succeeded!\n"
Jun  9 13:14:44.209: INFO: stdout: ""
Jun  9 13:14:44.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-1814 execpodtj2j5 -- /bin/sh -x -c nc -zv -t -w 2 172.23.24.46 31777'
Jun  9 13:14:44.537: INFO: stderr: "+ nc -zv -t -w 2 172.23.24.46 31777\nConnection to 172.23.24.46 31777 port [tcp/31777] succeeded!\n"
Jun  9 13:14:44.537: INFO: stdout: ""
Jun  9 13:14:44.537: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:14:44.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1814" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:12.775 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":117,"skipped":1831,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:14:44.605: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:14:51.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7586" for this suite.

• [SLOW TEST:7.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":118,"skipped":1831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:14:51.854: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  9 13:14:52.085: INFO: Waiting up to 5m0s for pod "pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832" in namespace "emptydir-1245" to be "success or failure"
Jun  9 13:14:52.091: INFO: Pod "pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832": Phase="Pending", Reason="", readiness=false. Elapsed: 6.518816ms
Jun  9 13:14:54.099: INFO: Pod "pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013630799s
Jun  9 13:14:56.105: INFO: Pod "pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019691026s
STEP: Saw pod success
Jun  9 13:14:56.105: INFO: Pod "pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832" satisfied condition "success or failure"
Jun  9 13:14:56.111: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832 container test-container: <nil>
STEP: delete the pod
Jun  9 13:14:56.172: INFO: Waiting for pod pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832 to disappear
Jun  9 13:14:56.177: INFO: Pod pod-c63da019-5fbc-42c3-87ac-f6e1f2a9e832 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:14:56.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1245" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1860,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:14:56.195: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun  9 13:14:56.393: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun  9 13:15:05.501: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:15:05.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8802" for this suite.

• [SLOW TEST:9.326 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":120,"skipped":1873,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:15:05.521: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun  9 13:15:10.799: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:15:11.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9810" for this suite.

• [SLOW TEST:6.354 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":121,"skipped":1874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:15:11.877: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun  9 13:15:12.084: INFO: PodSpec: initContainers in spec.initContainers
Jun  9 13:15:58.600: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d4bfcab9-bba0-4f16-9868-cfe419fec918", GenerateName:"", Namespace:"init-container-2809", SelfLink:"/api/v1/namespaces/init-container-2809/pods/pod-init-d4bfcab9-bba0-4f16-9868-cfe419fec918", UID:"d060f8ce-170c-4106-bbcb-b546e3ffda83", ResourceVersion:"26171", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63727305312, loc:(*time.Location)(0x7925200)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"84797232"}, Annotations:map[string]string{"kubernetes.io/psp":"cert-exporter-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zz5nn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000528fc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zz5nn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zz5nn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zz5nn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001be2088), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-k8xcg-8bbfd5b68-w4htb", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0039c0000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001be2100)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001be2120)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001be2128), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001be212c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305312, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305312, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305312, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305312, loc:(*time.Location)(0x7925200)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.23.24.246", PodIP:"172.24.106.31", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.24.106.31"}}, StartTime:(*v1.Time)(0xc002012060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00248a070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00248a150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://9ad4284729fe9f2e3f5e38f9bc69757710f3a38daacbabfaef6b49c49a82bf08", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0020120a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002012080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc001be22e4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:15:58.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2809" for this suite.

• [SLOW TEST:46.769 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":122,"skipped":1904,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:15:58.645: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:15:59.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:16:01.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305359, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305359, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305359, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305359, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:16:04.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:04.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4540" for this suite.
STEP: Destroying namespace "webhook-4540-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.406 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":123,"skipped":1920,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:05.053: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:18.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9831" for this suite.

• [SLOW TEST:13.434 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":124,"skipped":1920,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:18.486: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 13:16:18.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8107'
Jun  9 13:16:18.807: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  9 13:16:18.807: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Jun  9 13:16:20.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8107'
Jun  9 13:16:20.967: INFO: stderr: ""
Jun  9 13:16:20.967: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:20.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8107" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":125,"skipped":1922,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:20.986: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun  9 13:16:21.195: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:24.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8286" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":126,"skipped":1926,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-1d6b2018-5221-4f85-b35c-a37abac02954
STEP: Creating a pod to test consume secrets
Jun  9 13:16:25.143: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b" in namespace "projected-1975" to be "success or failure"
Jun  9 13:16:25.184: INFO: Pod "pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.772589ms
Jun  9 13:16:27.193: INFO: Pod "pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049480435s
Jun  9 13:16:29.199: INFO: Pod "pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055901211s
STEP: Saw pod success
Jun  9 13:16:29.199: INFO: Pod "pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b" satisfied condition "success or failure"
Jun  9 13:16:29.203: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:16:29.238: INFO: Waiting for pod pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b to disappear
Jun  9 13:16:29.244: INFO: Pod pod-projected-secrets-9f258d2f-fc33-44c8-97c0-6d43f969951b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:29.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1975" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":1944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:29.262: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  9 13:16:34.002: INFO: Successfully updated pod "pod-update-2da58681-b918-4183-87fa-3991c150bfa8"
STEP: verifying the updated pod is in kubernetes
Jun  9 13:16:34.015: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:34.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4728" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":1987,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 13:16:34.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7238'
Jun  9 13:16:34.371: INFO: stderr: ""
Jun  9 13:16:34.371: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Jun  9 13:16:34.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete pods e2e-test-httpd-pod --namespace=kubectl-7238'
Jun  9 13:16:46.500: INFO: stderr: ""
Jun  9 13:16:46.500: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:16:46.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7238" for this suite.

• [SLOW TEST:12.505 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1750
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":129,"skipped":2007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:16:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun  9 13:16:46.727: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26651 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  9 13:16:46.727: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26651 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun  9 13:16:56.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26694 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun  9 13:16:56.744: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26694 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun  9 13:17:06.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26724 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  9 13:17:06.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26724 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun  9 13:17:16.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26752 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun  9 13:17:16.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-a 7d651ff9-3fcb-40d1-9a2d-0315d8e9a13c 26752 0 2020-06-09 13:16:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun  9 13:17:26.790: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-b 7b49715e-debd-466d-8299-8a454a6b23c1 26781 0 2020-06-09 13:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  9 13:17:26.790: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-b 7b49715e-debd-466d-8299-8a454a6b23c1 26781 0 2020-06-09 13:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun  9 13:17:36.805: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-b 7b49715e-debd-466d-8299-8a454a6b23c1 26809 0 2020-06-09 13:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun  9 13:17:36.805: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2171 /api/v1/namespaces/watch-2171/configmaps/e2e-watch-test-configmap-b 7b49715e-debd-466d-8299-8a454a6b23c1 26809 0 2020-06-09 13:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:17:46.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2171" for this suite.

• [SLOW TEST:60.296 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":130,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:17:46.832: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:17:47.054: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74" in namespace "downward-api-957" to be "success or failure"
Jun  9 13:17:47.065: INFO: Pod "downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74": Phase="Pending", Reason="", readiness=false. Elapsed: 11.316147ms
Jun  9 13:17:49.074: INFO: Pod "downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020155804s
Jun  9 13:17:51.083: INFO: Pod "downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029155659s
STEP: Saw pod success
Jun  9 13:17:51.083: INFO: Pod "downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74" satisfied condition "success or failure"
Jun  9 13:17:51.088: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74 container client-container: <nil>
STEP: delete the pod
Jun  9 13:17:51.116: INFO: Waiting for pod downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74 to disappear
Jun  9 13:17:51.120: INFO: Pod downwardapi-volume-4152595e-47ab-422e-a740-546b64e73c74 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:17:51.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-957" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:17:51.137: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun  9 13:18:21.952: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:18:21.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0609 13:18:21.952450      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9841" for this suite.

• [SLOW TEST:30.832 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":132,"skipped":2094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:18:21.971: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:18:22.954: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun  9 13:18:24.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305502, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305502, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305503, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305502, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:18:28.015: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:18:28.023: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:18:28.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-161" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:6.889 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":133,"skipped":2116,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:18:28.861: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:18:54.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5250" for this suite.

• [SLOW TEST:25.719 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:18:54.581: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:18:54.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410" in namespace "downward-api-2801" to be "success or failure"
Jun  9 13:18:54.835: INFO: Pod "downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38223ms
Jun  9 13:18:56.843: INFO: Pod "downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015497029s
Jun  9 13:18:58.852: INFO: Pod "downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024582259s
STEP: Saw pod success
Jun  9 13:18:58.852: INFO: Pod "downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410" satisfied condition "success or failure"
Jun  9 13:18:58.857: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410 container client-container: <nil>
STEP: delete the pod
Jun  9 13:18:58.900: INFO: Waiting for pod downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410 to disappear
Jun  9 13:18:58.906: INFO: Pod downwardapi-volume-70c50146-e5ef-4bdc-bb27-ab47c5bd5410 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:18:58.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2801" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:18:58.925: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:18:59.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4494" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":136,"skipped":2178,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:18:59.149: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-df6df6e5-9046-47e8-b945-de4ff478a5c7
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:18:59.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4494" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":137,"skipped":2195,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:18:59.361: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:19:00.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:19:02.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305540, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305540, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305540, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305540, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:19:05.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:19:05.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3089" for this suite.
STEP: Destroying namespace "webhook-3089-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.113 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":138,"skipped":2203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:19:05.475: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5157
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  9 13:19:05.809: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  9 13:19:27.996: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.24.106.44:8080/dial?request=hostname&protocol=http&host=172.24.173.40&port=8080&tries=1'] Namespace:pod-network-test-5157 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:19:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:19:28.224: INFO: Waiting for responses: map[]
Jun  9 13:19:28.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.24.106.44:8080/dial?request=hostname&protocol=http&host=172.24.160.97&port=8080&tries=1'] Namespace:pod-network-test-5157 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:19:28.231: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:19:28.442: INFO: Waiting for responses: map[]
Jun  9 13:19:28.450: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.24.106.44:8080/dial?request=hostname&protocol=http&host=172.24.106.1&port=8080&tries=1'] Namespace:pod-network-test-5157 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:19:28.450: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:19:28.680: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:19:28.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5157" for this suite.

• [SLOW TEST:23.229 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2230,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:19:28.704: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-wdvr
STEP: Creating a pod to test atomic-volume-subpath
Jun  9 13:19:28.935: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wdvr" in namespace "subpath-3321" to be "success or failure"
Jun  9 13:19:28.939: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848294ms
Jun  9 13:19:30.949: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013819698s
Jun  9 13:19:32.958: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 4.023298278s
Jun  9 13:19:34.984: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 6.048589742s
Jun  9 13:19:36.991: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 8.055817467s
Jun  9 13:19:38.998: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 10.062951951s
Jun  9 13:19:41.006: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 12.071500163s
Jun  9 13:19:43.014: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 14.079130624s
Jun  9 13:19:45.020: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 16.08487998s
Jun  9 13:19:47.027: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 18.091676826s
Jun  9 13:19:49.032: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 20.097261731s
Jun  9 13:19:51.040: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Running", Reason="", readiness=true. Elapsed: 22.105128191s
Jun  9 13:19:53.053: INFO: Pod "pod-subpath-test-configmap-wdvr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.118088068s
STEP: Saw pod success
Jun  9 13:19:53.054: INFO: Pod "pod-subpath-test-configmap-wdvr" satisfied condition "success or failure"
Jun  9 13:19:53.060: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-subpath-test-configmap-wdvr container test-container-subpath-configmap-wdvr: <nil>
STEP: delete the pod
Jun  9 13:19:53.100: INFO: Waiting for pod pod-subpath-test-configmap-wdvr to disappear
Jun  9 13:19:53.104: INFO: Pod pod-subpath-test-configmap-wdvr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wdvr
Jun  9 13:19:53.105: INFO: Deleting pod "pod-subpath-test-configmap-wdvr" in namespace "subpath-3321"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:19:53.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3321" for this suite.

• [SLOW TEST:24.441 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":140,"skipped":2236,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:19:53.151: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 in namespace container-probe-1345
Jun  9 13:19:57.450: INFO: Started pod liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 in namespace container-probe-1345
STEP: checking the pod's current state and verifying that restartCount is present
Jun  9 13:19:57.454: INFO: Initial restart count of pod liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 is 0
Jun  9 13:20:15.521: INFO: Restart count of pod container-probe-1345/liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 is now 1 (18.067071866s elapsed)
Jun  9 13:20:35.592: INFO: Restart count of pod container-probe-1345/liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 is now 2 (38.137470349s elapsed)
Jun  9 13:20:55.690: INFO: Restart count of pod container-probe-1345/liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 is now 3 (58.235778049s elapsed)
Jun  9 13:21:15.765: INFO: Restart count of pod container-probe-1345/liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 is now 4 (1m18.310733416s elapsed)
Jun  9 13:22:18.042: INFO: Restart count of pod container-probe-1345/liveness-2d9e2e9a-bc2a-4972-90f2-8a5d6d70e352 is now 5 (2m20.588073231s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:22:18.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1345" for this suite.

• [SLOW TEST:144.935 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2250,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:22:18.086: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  9 13:22:21.338: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:22:21.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9602" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2258,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:22:21.394: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7027
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-16656e06-62af-447a-af58-f440f36f4838
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-16656e06-62af-447a-af58-f440f36f4838
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:23:42.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7027" for this suite.

• [SLOW TEST:81.124 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2276,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:23:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  9 13:23:42.744: INFO: Waiting up to 5m0s for pod "pod-9df2ea5a-f636-4deb-b452-7e2724dde777" in namespace "emptydir-4905" to be "success or failure"
Jun  9 13:23:42.757: INFO: Pod "pod-9df2ea5a-f636-4deb-b452-7e2724dde777": Phase="Pending", Reason="", readiness=false. Elapsed: 13.806777ms
Jun  9 13:23:44.764: INFO: Pod "pod-9df2ea5a-f636-4deb-b452-7e2724dde777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020270893s
STEP: Saw pod success
Jun  9 13:23:44.764: INFO: Pod "pod-9df2ea5a-f636-4deb-b452-7e2724dde777" satisfied condition "success or failure"
Jun  9 13:23:44.770: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-9df2ea5a-f636-4deb-b452-7e2724dde777 container test-container: <nil>
STEP: delete the pod
Jun  9 13:23:44.799: INFO: Waiting for pod pod-9df2ea5a-f636-4deb-b452-7e2724dde777 to disappear
Jun  9 13:23:44.819: INFO: Pod pod-9df2ea5a-f636-4deb-b452-7e2724dde777 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:23:44.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4905" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":144,"skipped":2282,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:23:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Jun  9 13:23:45.781: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:23:45.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0609 13:23:45.781263      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5424" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":145,"skipped":2293,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:23:45.798: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:23:46.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:23:48.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305826, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305826, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305826, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305826, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:23:51.630: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun  9 13:23:51.676: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:23:51.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8035" for this suite.
STEP: Destroying namespace "webhook-8035-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.001 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":146,"skipped":2293,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:23:51.800: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-839
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:23:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  9 13:23:57.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-839 create -f -'
Jun  9 13:23:59.114: INFO: stderr: ""
Jun  9 13:23:59.114: INFO: stdout: "e2e-test-crd-publish-openapi-4243-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  9 13:23:59.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-839 delete e2e-test-crd-publish-openapi-4243-crds test-cr'
Jun  9 13:23:59.243: INFO: stderr: ""
Jun  9 13:23:59.243: INFO: stdout: "e2e-test-crd-publish-openapi-4243-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  9 13:23:59.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-839 apply -f -'
Jun  9 13:23:59.620: INFO: stderr: ""
Jun  9 13:23:59.620: INFO: stdout: "e2e-test-crd-publish-openapi-4243-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  9 13:23:59.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-839 delete e2e-test-crd-publish-openapi-4243-crds test-cr'
Jun  9 13:23:59.768: INFO: stderr: ""
Jun  9 13:23:59.768: INFO: stdout: "e2e-test-crd-publish-openapi-4243-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  9 13:23:59.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-4243-crds'
Jun  9 13:24:00.212: INFO: stderr: ""
Jun  9 13:24:00.212: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4243-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:05.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-839" for this suite.

• [SLOW TEST:13.267 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":147,"skipped":2312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:05.068: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-d39644cb-274e-40da-9e6e-e728bf090949
STEP: Creating a pod to test consume secrets
Jun  9 13:24:05.267: INFO: Waiting up to 5m0s for pod "pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a" in namespace "secrets-3645" to be "success or failure"
Jun  9 13:24:05.281: INFO: Pod "pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.864195ms
Jun  9 13:24:07.293: INFO: Pod "pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025308722s
STEP: Saw pod success
Jun  9 13:24:07.293: INFO: Pod "pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a" satisfied condition "success or failure"
Jun  9 13:24:07.296: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:24:07.326: INFO: Waiting for pod pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a to disappear
Jun  9 13:24:07.330: INFO: Pod pod-secrets-06c872c1-f7eb-4d82-912e-86c3a0358f8a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3645" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2341,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:07.345: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:24:07.543: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab" in namespace "projected-1145" to be "success or failure"
Jun  9 13:24:07.559: INFO: Pod "downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007149ms
Jun  9 13:24:09.566: INFO: Pod "downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023156533s
Jun  9 13:24:11.595: INFO: Pod "downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051876596s
STEP: Saw pod success
Jun  9 13:24:11.595: INFO: Pod "downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab" satisfied condition "success or failure"
Jun  9 13:24:11.603: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab container client-container: <nil>
STEP: delete the pod
Jun  9 13:24:11.653: INFO: Waiting for pod downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab to disappear
Jun  9 13:24:11.660: INFO: Pod downwardapi-volume-f5d5e471-54c7-4bb4-80fd-d8917bab89ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:11.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1145" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2356,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jun  9 13:24:11.920: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-503842985 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:12.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8582" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":150,"skipped":2366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:12.037: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:24:12.246: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:16.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2530" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":151,"skipped":2396,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:16.554: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:24:17.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:24:19.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305857, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305857, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305857, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727305857, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:24:22.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:22.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8893" for this suite.
STEP: Destroying namespace "webhook-8893-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.444 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":152,"skipped":2398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:23.017: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jun  9 13:24:23.322: INFO: Waiting up to 5m0s for pod "downward-api-8c254f18-1e64-4711-aa76-0151b3475b96" in namespace "downward-api-5247" to be "success or failure"
Jun  9 13:24:23.351: INFO: Pod "downward-api-8c254f18-1e64-4711-aa76-0151b3475b96": Phase="Pending", Reason="", readiness=false. Elapsed: 28.803952ms
Jun  9 13:24:25.358: INFO: Pod "downward-api-8c254f18-1e64-4711-aa76-0151b3475b96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036015559s
Jun  9 13:24:27.365: INFO: Pod "downward-api-8c254f18-1e64-4711-aa76-0151b3475b96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042952626s
STEP: Saw pod success
Jun  9 13:24:27.365: INFO: Pod "downward-api-8c254f18-1e64-4711-aa76-0151b3475b96" satisfied condition "success or failure"
Jun  9 13:24:27.371: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downward-api-8c254f18-1e64-4711-aa76-0151b3475b96 container dapi-container: <nil>
STEP: delete the pod
Jun  9 13:24:27.409: INFO: Waiting for pod downward-api-8c254f18-1e64-4711-aa76-0151b3475b96 to disappear
Jun  9 13:24:27.413: INFO: Pod downward-api-8c254f18-1e64-4711-aa76-0151b3475b96 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:27.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5247" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:27.430: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun  9 13:24:32.220: INFO: Successfully updated pod "annotationupdate103e5c39-5844-4193-b903-311297d696d7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:34.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2660" for this suite.

• [SLOW TEST:6.852 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":154,"skipped":2480,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:34.283: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  9 13:24:34.513: INFO: Waiting up to 5m0s for pod "pod-336d1153-a796-4049-8952-6882de8fd7b8" in namespace "emptydir-6" to be "success or failure"
Jun  9 13:24:34.519: INFO: Pod "pod-336d1153-a796-4049-8952-6882de8fd7b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.037862ms
Jun  9 13:24:36.524: INFO: Pod "pod-336d1153-a796-4049-8952-6882de8fd7b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010413451s
Jun  9 13:24:38.532: INFO: Pod "pod-336d1153-a796-4049-8952-6882de8fd7b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018461998s
STEP: Saw pod success
Jun  9 13:24:38.532: INFO: Pod "pod-336d1153-a796-4049-8952-6882de8fd7b8" satisfied condition "success or failure"
Jun  9 13:24:38.536: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-336d1153-a796-4049-8952-6882de8fd7b8 container test-container: <nil>
STEP: delete the pod
Jun  9 13:24:38.569: INFO: Waiting for pod pod-336d1153-a796-4049-8952-6882de8fd7b8 to disappear
Jun  9 13:24:38.576: INFO: Pod pod-336d1153-a796-4049-8952-6882de8fd7b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:24:38.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":155,"skipped":2495,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:24:38.593: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jun  9 13:24:38.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-7885'
Jun  9 13:24:39.288: INFO: stderr: ""
Jun  9 13:24:39.288: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  9 13:24:39.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7885'
Jun  9 13:24:39.446: INFO: stderr: ""
Jun  9 13:24:39.446: INFO: stdout: "update-demo-nautilus-frbj2 update-demo-nautilus-j7bc9 "
Jun  9 13:24:39.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-frbj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:24:39.585: INFO: stderr: ""
Jun  9 13:24:39.585: INFO: stdout: ""
Jun  9 13:24:39.585: INFO: update-demo-nautilus-frbj2 is created but not running
Jun  9 13:24:44.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7885'
Jun  9 13:24:44.701: INFO: stderr: ""
Jun  9 13:24:44.701: INFO: stdout: "update-demo-nautilus-frbj2 update-demo-nautilus-j7bc9 "
Jun  9 13:24:44.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-frbj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:24:44.817: INFO: stderr: ""
Jun  9 13:24:44.817: INFO: stdout: "true"
Jun  9 13:24:44.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-frbj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:24:44.971: INFO: stderr: ""
Jun  9 13:24:44.971: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 13:24:44.971: INFO: validating pod update-demo-nautilus-frbj2
Jun  9 13:24:44.986: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 13:24:44.986: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 13:24:44.986: INFO: update-demo-nautilus-frbj2 is verified up and running
Jun  9 13:24:44.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-j7bc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:24:45.122: INFO: stderr: ""
Jun  9 13:24:45.122: INFO: stdout: "true"
Jun  9 13:24:45.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-nautilus-j7bc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:24:45.252: INFO: stderr: ""
Jun  9 13:24:45.252: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  9 13:24:45.252: INFO: validating pod update-demo-nautilus-j7bc9
Jun  9 13:24:45.265: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 13:24:45.265: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 13:24:45.265: INFO: update-demo-nautilus-j7bc9 is verified up and running
STEP: rolling-update to new replication controller
Jun  9 13:24:45.269: INFO: scanned /root for discovery docs: <nil>
Jun  9 13:24:45.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7885'
Jun  9 13:25:09.112: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun  9 13:25:09.112: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  9 13:25:09.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7885'
Jun  9 13:25:09.250: INFO: stderr: ""
Jun  9 13:25:09.250: INFO: stdout: "update-demo-kitten-nzv2b update-demo-kitten-plcxp "
Jun  9 13:25:09.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-kitten-nzv2b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:25:09.363: INFO: stderr: ""
Jun  9 13:25:09.363: INFO: stdout: "true"
Jun  9 13:25:09.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-kitten-nzv2b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:25:09.478: INFO: stderr: ""
Jun  9 13:25:09.478: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun  9 13:25:09.478: INFO: validating pod update-demo-kitten-nzv2b
Jun  9 13:25:09.488: INFO: got data: {
  "image": "kitten.jpg"
}

Jun  9 13:25:09.488: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun  9 13:25:09.489: INFO: update-demo-kitten-nzv2b is verified up and running
Jun  9 13:25:09.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-kitten-plcxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:25:09.601: INFO: stderr: ""
Jun  9 13:25:09.601: INFO: stdout: "true"
Jun  9 13:25:09.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 get pods update-demo-kitten-plcxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7885'
Jun  9 13:25:09.738: INFO: stderr: ""
Jun  9 13:25:09.738: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun  9 13:25:09.738: INFO: validating pod update-demo-kitten-plcxp
Jun  9 13:25:09.746: INFO: got data: {
  "image": "kitten.jpg"
}

Jun  9 13:25:09.746: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun  9 13:25:09.746: INFO: update-demo-kitten-plcxp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:25:09.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7885" for this suite.

• [SLOW TEST:31.173 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":156,"skipped":2511,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:25:09.767: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-6355
STEP: creating replication controller nodeport-test in namespace services-6355
I0609 13:25:09.996438      24 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-6355, replica count: 2
Jun  9 13:25:13.049: INFO: Creating new exec pod
I0609 13:25:13.049055      24 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 13:25:18.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-6355 execpod8s6nf -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun  9 13:25:18.434: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  9 13:25:18.435: INFO: stdout: ""
Jun  9 13:25:18.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-6355 execpod8s6nf -- /bin/sh -x -c nc -zv -t -w 2 172.31.242.67 80'
Jun  9 13:25:18.762: INFO: stderr: "+ nc -zv -t -w 2 172.31.242.67 80\nConnection to 172.31.242.67 80 port [tcp/http] succeeded!\n"
Jun  9 13:25:18.762: INFO: stdout: ""
Jun  9 13:25:18.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-6355 execpod8s6nf -- /bin/sh -x -c nc -zv -t -w 2 172.23.24.38 31603'
Jun  9 13:25:19.084: INFO: stderr: "+ nc -zv -t -w 2 172.23.24.38 31603\nConnection to 172.23.24.38 31603 port [tcp/31603] succeeded!\n"
Jun  9 13:25:19.084: INFO: stdout: ""
Jun  9 13:25:19.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-6355 execpod8s6nf -- /bin/sh -x -c nc -zv -t -w 2 172.23.24.246 31603'
Jun  9 13:25:19.405: INFO: stderr: "+ nc -zv -t -w 2 172.23.24.246 31603\nConnection to 172.23.24.246 31603 port [tcp/31603] succeeded!\n"
Jun  9 13:25:19.405: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:25:19.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6355" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.660 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":157,"skipped":2529,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:25:19.427: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun  9 13:25:19.607: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 13:25:19.631: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 13:25:19.636: INFO: 
Logging pods the kubelet thinks is on node worker-2jqhr-6f5dbbb884-vqc7c before test
Jun  9 13:25:19.660: INFO: coredns-6d56c484c-txwjd from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:25:19.660: INFO: metrics-server-66df9f5b56-bvwdm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container metrics-server ready: true, restart count 0
Jun  9 13:25:19.660: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-sd6nl from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:25:19.660: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:25:19.660: INFO: nodeport-test-xwfx2 from services-6355 started at 2020-06-09 13:25:10 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container nodeport-test ready: true, restart count 0
Jun  9 13:25:19.660: INFO: net-exporter-4n4fk from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:25:19.660: INFO: docker-mem-limit-startup-script-clstp from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:25:19.660: INFO: nginx-ingress-controller-59b5c95c6d-cb4kl from kube-system started at 2020-06-09 11:45:21 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 13:25:19.660: INFO: calico-node-vhcbj from kube-system started at 2020-06-09 11:42:16 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:25:19.660: INFO: node-exporter-nsmh9 from kube-system started at 2020-06-09 11:45:08 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:25:19.660: INFO: cert-exporter-gkbjk from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:25:19.660: INFO: tiller-deploy-684c6b545b-4w7nq from giantswarm started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container tiller ready: true, restart count 0
Jun  9 13:25:19.660: INFO: coredns-6d56c484c-7fkzm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:25:19.660: INFO: kube-proxy-69gzh from kube-system started at 2020-06-09 11:43:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:25:19.660: INFO: chart-operator-5b8b4bcc75-wn4xh from giantswarm started at 2020-06-09 11:44:47 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.660: INFO: 	Container chart-operator ready: true, restart count 0
Jun  9 13:25:19.660: INFO: 
Logging pods the kubelet thinks is on node worker-dfhc8-64bc8fc496-xx7cx before test
Jun  9 13:25:19.692: INFO: kube-proxy-tg6rp from kube-system started at 2020-06-09 11:43:24 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:25:19.692: INFO: nginx-ingress-controller-59b5c95c6d-8z4jm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 13:25:19.692: INFO: update-demo-kitten-plcxp from kubectl-7885 started at 2020-06-09 13:24:52 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container update-demo ready: false, restart count 0
Jun  9 13:25:19.692: INFO: coredns-6d56c484c-s5fsw from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container coredns ready: true, restart count 0
Jun  9 13:25:19.692: INFO: net-exporter-wvnc9 from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 13:25:19.692: INFO: sonobuoy from sonobuoy started at 2020-06-09 12:47:13 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 13:25:19.692: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-p5v44 from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:25:19.692: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:25:19.692: INFO: calico-node-7t79n from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:25:19.692: INFO: cert-exporter-v64nm from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:25:19.692: INFO: node-exporter-k9z45 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:25:19.692: INFO: kube-state-metrics-6d998ffd8b-wtr6p from kube-system started at 2020-06-09 11:44:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  9 13:25:19.692: INFO: docker-mem-limit-startup-script-9x4w8 from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:25:19.692: INFO: sonobuoy-e2e-job-fb32098c60e64727 from sonobuoy started at 2020-06-09 12:47:22 +0000 UTC (2 container statuses recorded)
Jun  9 13:25:19.692: INFO: 	Container e2e ready: true, restart count 0
Jun  9 13:25:19.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:25:19.692: INFO: 
Logging pods the kubelet thinks is on node worker-k8xcg-8bbfd5b68-w4htb before test
Jun  9 13:25:19.702: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-lbr8h from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 13:25:19.702: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 13:25:19.702: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 13:25:19.702: INFO: update-demo-kitten-nzv2b from kubectl-7885 started at 2020-06-09 13:24:45 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container update-demo ready: false, restart count 0
Jun  9 13:25:19.703: INFO: kube-proxy-kfmds from kube-system started at 2020-06-09 11:43:19 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 13:25:19.703: INFO: calico-node-zqtv2 from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 13:25:19.703: INFO: node-exporter-mbg29 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 13:25:19.703: INFO: docker-mem-limit-startup-script-74z86 from kube-system started at 2020-06-09 12:55:26 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 13:25:19.703: INFO: execpod8s6nf from services-6355 started at 2020-06-09 13:25:13 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container agnhost-pause ready: true, restart count 0
Jun  9 13:25:19.703: INFO: nodeport-test-vmmsl from services-6355 started at 2020-06-09 13:25:10 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container nodeport-test ready: true, restart count 0
Jun  9 13:25:19.703: INFO: cert-exporter-p2wws from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 13:25:19.703: INFO: net-exporter-hppdh from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 13:25:19.703: INFO: 	Container net-exporter ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e282bb70-bcb7-4b28-ae54-2e2c0360a932 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e282bb70-bcb7-4b28-ae54-2e2c0360a932 off the node worker-k8xcg-8bbfd5b68-w4htb
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e282bb70-bcb7-4b28-ae54-2e2c0360a932
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:30:27.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5344" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:308.509 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":158,"skipped":2531,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:30:27.937: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:30:28.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c" in namespace "projected-3323" to be "success or failure"
Jun  9 13:30:28.150: INFO: Pod "downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.257039ms
Jun  9 13:30:30.171: INFO: Pod "downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026629964s
Jun  9 13:30:32.178: INFO: Pod "downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033890296s
Jun  9 13:30:34.189: INFO: Pod "downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04485738s
STEP: Saw pod success
Jun  9 13:30:34.189: INFO: Pod "downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c" satisfied condition "success or failure"
Jun  9 13:30:34.197: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c container client-container: <nil>
STEP: delete the pod
Jun  9 13:30:34.256: INFO: Waiting for pod downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c to disappear
Jun  9 13:30:34.264: INFO: Pod downwardapi-volume-f1b00217-9154-4f49-9f69-0cf090d4988c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:30:34.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3323" for this suite.

• [SLOW TEST:6.360 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":159,"skipped":2536,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:30:34.297: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  9 13:30:42.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  9 13:30:42.596: INFO: Pod pod-with-poststart-http-hook still exists
Jun  9 13:30:44.597: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  9 13:30:44.602: INFO: Pod pod-with-poststart-http-hook still exists
Jun  9 13:30:46.596: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  9 13:30:46.602: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:30:46.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5849" for this suite.

• [SLOW TEST:12.326 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2546,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:30:46.624: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-847
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:30:46.818: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:30:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-847" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":161,"skipped":2546,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:30:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-02cf18d8-65b5-4805-9b92-0f0a5bcc2a48
STEP: Creating a pod to test consume secrets
Jun  9 13:30:48.014: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311" in namespace "projected-3896" to be "success or failure"
Jun  9 13:30:48.022: INFO: Pod "pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311": Phase="Pending", Reason="", readiness=false. Elapsed: 7.681412ms
Jun  9 13:30:50.035: INFO: Pod "pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021181598s
Jun  9 13:30:52.041: INFO: Pod "pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027478304s
STEP: Saw pod success
Jun  9 13:30:52.042: INFO: Pod "pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311" satisfied condition "success or failure"
Jun  9 13:30:52.048: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:30:52.086: INFO: Waiting for pod pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311 to disappear
Jun  9 13:30:52.093: INFO: Pod pod-projected-secrets-389e63c9-b5c1-486a-9800-49c67e4a7311 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:30:52.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3896" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:30:52.115: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:30:52.332: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-17d0d73f-d09d-4717-89c4-47154d1a2240" in namespace "security-context-test-6819" to be "success or failure"
Jun  9 13:30:52.372: INFO: Pod "alpine-nnp-false-17d0d73f-d09d-4717-89c4-47154d1a2240": Phase="Pending", Reason="", readiness=false. Elapsed: 39.55357ms
Jun  9 13:30:54.381: INFO: Pod "alpine-nnp-false-17d0d73f-d09d-4717-89c4-47154d1a2240": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049087135s
Jun  9 13:30:56.388: INFO: Pod "alpine-nnp-false-17d0d73f-d09d-4717-89c4-47154d1a2240": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055990626s
Jun  9 13:30:58.397: INFO: Pod "alpine-nnp-false-17d0d73f-d09d-4717-89c4-47154d1a2240": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0651951s
Jun  9 13:30:58.397: INFO: Pod "alpine-nnp-false-17d0d73f-d09d-4717-89c4-47154d1a2240" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:30:58.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6819" for this suite.

• [SLOW TEST:6.323 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2591,"failed":0}
SSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:30:58.439: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-2320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jun  9 13:30:58.626: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 13:31:58.667: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:31:58.673: INFO: Starting informer...
STEP: Starting pod...
Jun  9 13:31:58.915: INFO: Pod is running on worker-k8xcg-8bbfd5b68-w4htb. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun  9 13:31:58.945: INFO: Pod wasn't evicted. Proceeding
Jun  9 13:31:58.945: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun  9 13:33:14.014: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:33:14.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2320" for this suite.

• [SLOW TEST:135.590 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":164,"skipped":2594,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:33:14.029: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  9 13:33:14.241: INFO: Waiting up to 5m0s for pod "pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93" in namespace "emptydir-1750" to be "success or failure"
Jun  9 13:33:14.245: INFO: Pod "pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526004ms
Jun  9 13:33:16.253: INFO: Pod "pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012016199s
Jun  9 13:33:18.264: INFO: Pod "pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023099353s
STEP: Saw pod success
Jun  9 13:33:18.264: INFO: Pod "pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93" satisfied condition "success or failure"
Jun  9 13:33:18.270: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93 container test-container: <nil>
STEP: delete the pod
Jun  9 13:33:18.346: INFO: Waiting for pod pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93 to disappear
Jun  9 13:33:18.351: INFO: Pod pod-4cb98060-1d8e-4ab3-8807-03235c4d2e93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:33:18.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1750" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":165,"skipped":2602,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:33:18.368: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5705 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5705;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5705 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5705;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5705.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5705.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5705.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5705.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5705.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5705.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5705.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5705.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5705.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 70.52.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.52.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.52.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.52.70_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5705 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5705;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5705 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5705;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5705.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5705.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5705.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5705.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5705.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5705.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5705.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5705.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5705.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5705.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 70.52.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.52.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.52.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.52.70_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 13:33:22.642: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.650: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.656: INFO: Unable to read wheezy_udp@dns-test-service.dns-5705 from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.668: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5705 from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.674: INFO: Unable to read wheezy_udp@dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.681: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.694: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.718: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.727: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.742: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.747: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.765: INFO: Unable to read jessie_udp@dns-test-service.dns-5705 from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-5705 from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.785: INFO: Unable to read jessie_udp@dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.795: INFO: Unable to read jessie_tcp@dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.802: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.810: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5705.svc from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.833: INFO: Unable to read jessie_udp@PodARecord from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.841: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a: the server could not find the requested resource (get pods dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a)
Jun  9 13:33:22.857: INFO: Lookups using dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5705 wheezy_tcp@dns-test-service.dns-5705 wheezy_udp@dns-test-service.dns-5705.svc wheezy_tcp@dns-test-service.dns-5705.svc wheezy_udp@_http._tcp.dns-test-service.dns-5705.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5705.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5705 jessie_tcp@dns-test-service.dns-5705 jessie_udp@dns-test-service.dns-5705.svc jessie_tcp@dns-test-service.dns-5705.svc jessie_udp@_http._tcp.dns-test-service.dns-5705.svc jessie_tcp@_http._tcp.dns-test-service.dns-5705.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  9 13:33:28.026: INFO: DNS probes using dns-5705/dns-test-b5b13d3b-08b2-405d-9e9e-85c43193bf8a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:33:28.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5705" for this suite.

• [SLOW TEST:9.800 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":166,"skipped":2616,"failed":0}
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:33:28.169: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:33:32.474: INFO: Waiting up to 5m0s for pod "client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb" in namespace "pods-1796" to be "success or failure"
Jun  9 13:33:32.485: INFO: Pod "client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.843884ms
Jun  9 13:33:34.493: INFO: Pod "client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01859244s
Jun  9 13:33:36.500: INFO: Pod "client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025278066s
STEP: Saw pod success
Jun  9 13:33:36.500: INFO: Pod "client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb" satisfied condition "success or failure"
Jun  9 13:33:36.504: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb container env3cont: <nil>
STEP: delete the pod
Jun  9 13:33:36.547: INFO: Waiting for pod client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb to disappear
Jun  9 13:33:36.557: INFO: Pod client-envvars-3145654b-4045-4f94-89bf-5552ce2a95eb no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:33:36.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1796" for this suite.

• [SLOW TEST:8.407 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:33:36.579: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5585
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5585
STEP: creating replication controller externalsvc in namespace services-5585
I0609 13:33:36.829261      24 runners.go:189] Created replication controller with name: externalsvc, namespace: services-5585, replica count: 2
I0609 13:33:39.880147      24 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 13:33:42.880682      24 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun  9 13:33:42.939: INFO: Creating new exec pod
Jun  9 13:33:46.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-5585 execpodfm5cq -- /bin/sh -x -c nslookup nodeport-service'
Jun  9 13:33:47.382: INFO: stderr: "+ nslookup nodeport-service\n"
Jun  9 13:33:47.382: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nnodeport-service.services-5585.svc.cluster.local\tcanonical name = externalsvc.services-5585.svc.cluster.local.\nName:\texternalsvc.services-5585.svc.cluster.local\nAddress: 172.31.163.91\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5585, will wait for the garbage collector to delete the pods
Jun  9 13:33:47.448: INFO: Deleting ReplicationController externalsvc took: 9.740529ms
Jun  9 13:33:47.548: INFO: Terminating ReplicationController externalsvc pods took: 100.355668ms
Jun  9 13:33:57.197: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:33:57.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5585" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:20.699 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":168,"skipped":2647,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:33:57.284: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5729
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5729
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5729
Jun  9 13:33:57.670: INFO: Found 0 stateful pods, waiting for 1
Jun  9 13:34:07.678: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun  9 13:34:07.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:34:08.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:34:08.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:34:08.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:34:08.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  9 13:34:18.679: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:34:18.679: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:34:18.700: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998008s
Jun  9 13:34:19.708: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991203122s
Jun  9 13:34:20.720: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984682388s
Jun  9 13:34:21.729: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973027296s
Jun  9 13:34:22.738: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.963787129s
Jun  9 13:34:23.745: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955140118s
Jun  9 13:34:24.754: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.948096676s
Jun  9 13:34:25.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.939245867s
Jun  9 13:34:26.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.932089559s
Jun  9 13:34:27.777: INFO: Verifying statefulset ss doesn't scale past 1 for another 923.144812ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5729
Jun  9 13:34:28.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:34:29.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:34:29.128: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:34:29.129: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:34:29.135: INFO: Found 1 stateful pods, waiting for 3
Jun  9 13:34:39.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:34:39.145: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:34:39.145: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun  9 13:34:39.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:34:39.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:34:39.503: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:34:39.503: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:34:39.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:34:39.854: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:34:39.854: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:34:39.854: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:34:39.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:34:40.348: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:34:40.348: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:34:40.348: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:34:40.348: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:34:40.375: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  9 13:34:50.385: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:34:50.385: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:34:50.385: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:34:50.415: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999799s
Jun  9 13:34:51.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98034576s
Jun  9 13:34:52.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965549146s
Jun  9 13:34:53.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957158619s
Jun  9 13:34:54.456: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947351172s
Jun  9 13:34:55.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.93917672s
Jun  9 13:34:56.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.930093412s
Jun  9 13:34:57.488: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.91352836s
Jun  9 13:34:58.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.907147585s
Jun  9 13:34:59.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 897.721952ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5729
Jun  9 13:35:00.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:35:00.885: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:35:00.885: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:35:00.885: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:35:00.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:35:01.268: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:35:01.268: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:35:01.268: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:35:01.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-5729 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:35:01.625: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:35:01.625: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:35:01.625: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:35:01.625: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun  9 13:35:41.650: INFO: Deleting all statefulset in ns statefulset-5729
Jun  9 13:35:41.657: INFO: Scaling statefulset ss to 0
Jun  9 13:35:41.672: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:35:41.678: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:35:41.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5729" for this suite.

• [SLOW TEST:104.446 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":169,"skipped":2665,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:35:41.731: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4261
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jun  9 13:35:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:36:10.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4261" for this suite.

• [SLOW TEST:28.788 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":170,"skipped":2674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:36:10.520: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad
Jun  9 13:36:10.739: INFO: Pod name my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad: Found 0 pods out of 1
Jun  9 13:36:15.745: INFO: Pod name my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad: Found 1 pods out of 1
Jun  9 13:36:15.745: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad" are running
Jun  9 13:36:15.751: INFO: Pod "my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad-2dzqb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:36:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:36:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:36:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-09 13:36:10 +0000 UTC Reason: Message:}])
Jun  9 13:36:15.751: INFO: Trying to dial the pod
Jun  9 13:36:20.771: INFO: Controller my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad: Got expected result from replica 1 [my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad-2dzqb]: "my-hostname-basic-fd4721c8-6ddb-411d-8c55-5e6dcc3559ad-2dzqb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:36:20.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4737" for this suite.

• [SLOW TEST:10.267 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":171,"skipped":2704,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:36:20.787: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Jun  9 13:36:20.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-4115 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  9 13:36:21.158: INFO: stderr: ""
Jun  9 13:36:21.158: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jun  9 13:36:21.158: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  9 13:36:21.158: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4115" to be "running and ready, or succeeded"
Jun  9 13:36:21.173: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 15.021432ms
Jun  9 13:36:23.187: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028910481s
Jun  9 13:36:25.194: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.036388149s
Jun  9 13:36:25.195: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  9 13:36:25.195: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun  9 13:36:25.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs logs-generator logs-generator --namespace=kubectl-4115'
Jun  9 13:36:25.333: INFO: stderr: ""
Jun  9 13:36:25.333: INFO: stdout: "I0609 13:36:22.705459       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/bdt 582\nI0609 13:36:22.905977       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wn8 305\nI0609 13:36:23.105914       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/8wc 321\nI0609 13:36:23.305991       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/fqj 377\nI0609 13:36:23.505880       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/2wr 219\nI0609 13:36:23.705786       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/cwc 284\nI0609 13:36:23.905736       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/lh2 278\nI0609 13:36:24.105793       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hhdt 219\nI0609 13:36:24.305760       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/vqsd 240\nI0609 13:36:24.505938       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/l8l 296\nI0609 13:36:24.705903       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/l2zb 540\nI0609 13:36:24.906070       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/4psm 207\nI0609 13:36:25.105989       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/5rlc 437\nI0609 13:36:25.305874       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/pzm 585\n"
STEP: limiting log lines
Jun  9 13:36:25.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs logs-generator logs-generator --namespace=kubectl-4115 --tail=1'
Jun  9 13:36:25.479: INFO: stderr: ""
Jun  9 13:36:25.479: INFO: stdout: "I0609 13:36:25.305874       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/pzm 585\n"
Jun  9 13:36:25.479: INFO: got output "I0609 13:36:25.305874       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/pzm 585\n"
STEP: limiting log bytes
Jun  9 13:36:25.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs logs-generator logs-generator --namespace=kubectl-4115 --limit-bytes=1'
Jun  9 13:36:25.624: INFO: stderr: ""
Jun  9 13:36:25.624: INFO: stdout: "I"
Jun  9 13:36:25.624: INFO: got output "I"
STEP: exposing timestamps
Jun  9 13:36:25.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs logs-generator logs-generator --namespace=kubectl-4115 --tail=1 --timestamps'
Jun  9 13:36:25.791: INFO: stderr: ""
Jun  9 13:36:25.791: INFO: stdout: "2020-06-09T13:36:25.706645197Z I0609 13:36:25.705882       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/fmgf 473\n"
Jun  9 13:36:25.791: INFO: got output "2020-06-09T13:36:25.706645197Z I0609 13:36:25.705882       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/fmgf 473\n"
STEP: restricting to a time range
Jun  9 13:36:28.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs logs-generator logs-generator --namespace=kubectl-4115 --since=1s'
Jun  9 13:36:28.447: INFO: stderr: ""
Jun  9 13:36:28.447: INFO: stdout: "I0609 13:36:27.505698       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/r9km 583\nI0609 13:36:27.705690       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/r8t 455\nI0609 13:36:27.905743       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/j8x 215\nI0609 13:36:28.105674       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/ns8f 426\nI0609 13:36:28.305628       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/dpc 378\n"
Jun  9 13:36:28.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs logs-generator logs-generator --namespace=kubectl-4115 --since=24h'
Jun  9 13:36:28.606: INFO: stderr: ""
Jun  9 13:36:28.606: INFO: stdout: "I0609 13:36:22.705459       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/bdt 582\nI0609 13:36:22.905977       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wn8 305\nI0609 13:36:23.105914       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/8wc 321\nI0609 13:36:23.305991       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/fqj 377\nI0609 13:36:23.505880       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/2wr 219\nI0609 13:36:23.705786       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/cwc 284\nI0609 13:36:23.905736       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/lh2 278\nI0609 13:36:24.105793       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hhdt 219\nI0609 13:36:24.305760       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/vqsd 240\nI0609 13:36:24.505938       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/l8l 296\nI0609 13:36:24.705903       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/l2zb 540\nI0609 13:36:24.906070       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/4psm 207\nI0609 13:36:25.105989       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/5rlc 437\nI0609 13:36:25.305874       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/pzm 585\nI0609 13:36:25.505930       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/rqn 543\nI0609 13:36:25.705882       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/fmgf 473\nI0609 13:36:25.905894       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/8bx 426\nI0609 13:36:26.105767       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/dsdk 367\nI0609 13:36:26.306004       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/lgd 585\nI0609 13:36:26.505748       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/z5l 341\nI0609 13:36:26.705787       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/bzl 388\nI0609 13:36:26.905771       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/vx6l 502\nI0609 13:36:27.105663       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/jw4d 568\nI0609 13:36:27.305726       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/gbh 503\nI0609 13:36:27.505698       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/r9km 583\nI0609 13:36:27.705690       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/r8t 455\nI0609 13:36:27.905743       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/j8x 215\nI0609 13:36:28.105674       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/ns8f 426\nI0609 13:36:28.305628       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/dpc 378\nI0609 13:36:28.505666       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/64fb 377\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Jun  9 13:36:28.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete pod logs-generator --namespace=kubectl-4115'
Jun  9 13:36:36.486: INFO: stderr: ""
Jun  9 13:36:36.486: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:36:36.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4115" for this suite.

• [SLOW TEST:15.714 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":172,"skipped":2724,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:36:36.501: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:36:36.715: INFO: Creating deployment "webserver-deployment"
Jun  9 13:36:36.727: INFO: Waiting for observed generation 1
Jun  9 13:36:38.776: INFO: Waiting for all required pods to come up
Jun  9 13:36:38.806: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun  9 13:36:42.899: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  9 13:36:42.908: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  9 13:36:42.925: INFO: Updating deployment webserver-deployment
Jun  9 13:36:42.925: INFO: Waiting for observed generation 2
Jun  9 13:36:44.957: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  9 13:36:44.975: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  9 13:36:44.982: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  9 13:36:45.007: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  9 13:36:45.007: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  9 13:36:45.011: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  9 13:36:45.021: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  9 13:36:45.021: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  9 13:36:45.032: INFO: Updating deployment webserver-deployment
Jun  9 13:36:45.032: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  9 13:36:45.039: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  9 13:36:45.052: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun  9 13:36:45.080: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-517 /apis/apps/v1/namespaces/deployment-517/deployments/webserver-deployment 798a8213-6b6c-45d3-9c20-737e0e537d96 32575 3 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035211e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-06-09 13:36:43 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-09 13:36:45 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  9 13:36:45.129: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-517 /apis/apps/v1/namespaces/deployment-517/replicasets/webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 32572 3 2020-06-09 13:36:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 798a8213-6b6c-45d3-9c20-737e0e537d96 0xc003521877 0xc003521878}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035218e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:36:45.129: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  9 13:36:45.129: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-517 /apis/apps/v1/namespaces/deployment-517/replicasets/webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 32570 3 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 798a8213-6b6c-45d3-9c20-737e0e537d96 0xc003521787 0xc003521788}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035217f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:36:45.191: INFO: Pod "webserver-deployment-595b5b9587-2sck2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2sck2 webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-2sck2 63542cea-73aa-42d8-8973-1bfad3dd6dc5 32467 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc001be3e17 0xc001be3e18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2jqhr-6f5dbbb884-vqc7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.38,PodIP:172.24.173.46,StartTime:2020-06-09 13:36:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7d45cd2b2fe2d8c4a3918221138a0560054b38ef6a6099a70de574dbb34c7dfc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.173.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.193: INFO: Pod "webserver-deployment-595b5b9587-56trn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-56trn webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-56trn 504105d1-eadc-4367-a3c8-b7ed6329a7f7 32450 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f20a7 0xc0077f20a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:172.24.106.22,StartTime:2020-06-09 13:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4549f97c37cfdd0b986ed934d402580451e146e9e735fdf40dd4ca57a0e985ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.106.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.194: INFO: Pod "webserver-deployment-595b5b9587-6zfx7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6zfx7 webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-6zfx7 2e9f6b08-2e09-4a68-97eb-cffe4acdf4e3 32453 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f22a7 0xc0077f22a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:172.24.106.24,StartTime:2020-06-09 13:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b5014f51028e76d554179505f222743bac4968affe2657e62f6008831a2feac0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.106.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.195: INFO: Pod "webserver-deployment-595b5b9587-7rgd6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7rgd6 webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-7rgd6 527868d1-fa33-422c-b445-42171a77e416 32461 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f24d7 0xc0077f24d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2jqhr-6f5dbbb884-vqc7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.38,PodIP:172.24.173.47,StartTime:2020-06-09 13:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://56a40664f86023f98963c30bcfd5a8313f1fd39203a56917a58049bfd0740d37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.173.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.196: INFO: Pod "webserver-deployment-595b5b9587-9bjzm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9bjzm webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-9bjzm ed9affb4-d282-4ee4-b7c2-d614c4717f40 32475 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f2687 0xc0077f2688}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-dfhc8-64bc8fc496-xx7cx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.46,PodIP:172.24.160.101,StartTime:2020-06-09 13:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7321ced31311106ad32cc018e5427318ad9e3a38ef6fe9775dcf7cad9fc3da42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.160.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.197: INFO: Pod "webserver-deployment-595b5b9587-l6wgm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-l6wgm webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-l6wgm 3115ba63-6d97-4200-87c6-4785a23b59e6 32478 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f28c7 0xc0077f28c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-dfhc8-64bc8fc496-xx7cx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.46,PodIP:172.24.160.102,StartTime:2020-06-09 13:36:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://67ae414fca873738a601e0f16caf1c3fe404738aefcaa45ddc4b4473031b8c33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.160.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.198: INFO: Pod "webserver-deployment-595b5b9587-njh9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-njh9q webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-njh9q 9d764164-3147-454c-ab17-42b766b75f61 32587 0 2020-06-09 13:36:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f2a77 0xc0077f2a78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.199: INFO: Pod "webserver-deployment-595b5b9587-nwc9g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nwc9g webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-nwc9g 84aae301-97f5-412c-af29-7eba39fbfe84 32589 0 2020-06-09 13:36:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f2b80 0xc0077f2b81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.200: INFO: Pod "webserver-deployment-595b5b9587-qj5pl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qj5pl webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-qj5pl 10d5b82f-ec30-47d1-9991-50f0cfe4ea3a 32442 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f2c80 0xc0077f2c81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-dfhc8-64bc8fc496-xx7cx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.46,PodIP:172.24.160.100,StartTime:2020-06-09 13:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://56d27f15ab3f1f3b8101d0828acf2489c55ecf68d000466c890c29817302383e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.160.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.200: INFO: Pod "webserver-deployment-595b5b9587-xh94t" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xh94t webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-xh94t 78702629-9a2a-4736-bcba-6362bb44a5df 32464 0 2020-06-09 13:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f2e07 0xc0077f2e08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2jqhr-6f5dbbb884-vqc7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.38,PodIP:172.24.173.45,StartTime:2020-06-09 13:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:36:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1e692af144ff6c4fe107e4d4935164ad2c2aea0ecac88dcb561d71512d6278c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.173.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.201: INFO: Pod "webserver-deployment-595b5b9587-z5mbl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z5mbl webserver-deployment-595b5b9587- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-595b5b9587-z5mbl 6e29c7b1-2713-41c6-a3e3-35ed51024200 32581 0 2020-06-09 13:36:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3714ebe0-81aa-4554-b02d-9557980e1295 0xc0077f2f77 0xc0077f2f78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.201: INFO: Pod "webserver-deployment-c7997dcc8-42qst" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-42qst webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-42qst d3839feb-56e9-40b9-a5d8-7adc2168018b 32583 0 2020-06-09 13:36:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f3090 0xc0077f3091}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2jqhr-6f5dbbb884-vqc7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.202: INFO: Pod "webserver-deployment-c7997dcc8-7dwd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7dwd6 webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-7dwd6 0bb34433-8e09-420c-8e6f-1cf1a30d2a7e 32543 0 2020-06-09 13:36:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f31c0 0xc0077f31c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:,StartTime:2020-06-09 13:36:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.202: INFO: Pod "webserver-deployment-c7997dcc8-8btwr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8btwr webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-8btwr ca743357-d23a-4e76-a234-fe38879bd375 32513 0 2020-06-09 13:36:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f33b7 0xc0077f33b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:,StartTime:2020-06-09 13:36:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.203: INFO: Pod "webserver-deployment-c7997dcc8-8d98b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8d98b webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-8d98b eb8a5bdc-36c6-4f9c-ab35-5df02c994148 32588 0 2020-06-09 13:36:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f3567 0xc0077f3568}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-dfhc8-64bc8fc496-xx7cx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.204: INFO: Pod "webserver-deployment-c7997dcc8-d6bvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-d6bvd webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-d6bvd e1d2a343-4cd8-4cf1-8122-60b2647a57fd 32515 0 2020-06-09 13:36:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f3700 0xc0077f3701}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-dfhc8-64bc8fc496-xx7cx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.46,PodIP:,StartTime:2020-06-09 13:36:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.204: INFO: Pod "webserver-deployment-c7997dcc8-m2twg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-m2twg webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-m2twg 7fbd7cea-67a2-4473-97a3-09e9f87b6071 32585 0 2020-06-09 13:36:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f38e7 0xc0077f38e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.205: INFO: Pod "webserver-deployment-c7997dcc8-nslcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nslcl webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-nslcl 232ef1e9-649a-4839-87b6-053fbea98059 32547 0 2020-06-09 13:36:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f3aa0 0xc0077f3aa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:,StartTime:2020-06-09 13:36:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 13:36:45.205: INFO: Pod "webserver-deployment-c7997dcc8-rq4qs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rq4qs webserver-deployment-c7997dcc8- deployment-517 /api/v1/namespaces/deployment-517/pods/webserver-deployment-c7997dcc8-rq4qs 67d6a892-86df-4980-833d-c91a4dc7ab8b 32518 0 2020-06-09 13:36:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 798e04a1-77e9-4517-8d5f-27d50d04a6dd 0xc0077f3cb7 0xc0077f3cb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjdrq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2jqhr-6f5dbbb884-vqc7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:36:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.38,PodIP:,StartTime:2020-06-09 13:36:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:36:45.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-517" for this suite.

• [SLOW TEST:8.885 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":173,"skipped":2737,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:36:45.387: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun  9 13:36:58.277: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 13:36:58.286: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  9 13:37:00.287: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 13:37:00.294: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  9 13:37:02.287: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 13:37:02.296: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  9 13:37:04.287: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 13:37:04.293: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:37:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4516" for this suite.

• [SLOW TEST:18.940 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":174,"skipped":2738,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:37:04.328: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:37:04.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e" in namespace "projected-1617" to be "success or failure"
Jun  9 13:37:04.535: INFO: Pod "downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.119594ms
Jun  9 13:37:06.541: INFO: Pod "downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012926852s
Jun  9 13:37:08.548: INFO: Pod "downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020402875s
STEP: Saw pod success
Jun  9 13:37:08.548: INFO: Pod "downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e" satisfied condition "success or failure"
Jun  9 13:37:08.553: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e container client-container: <nil>
STEP: delete the pod
Jun  9 13:37:08.600: INFO: Waiting for pod downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e to disappear
Jun  9 13:37:08.607: INFO: Pod downwardapi-volume-c34aa377-2803-4ffd-94eb-a95db34bd17e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:37:08.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1617" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2738,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:37:08.631: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-804
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  9 13:37:08.843: INFO: Waiting up to 5m0s for pod "pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622" in namespace "emptydir-804" to be "success or failure"
Jun  9 13:37:08.853: INFO: Pod "pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622": Phase="Pending", Reason="", readiness=false. Elapsed: 9.255371ms
Jun  9 13:37:10.860: INFO: Pod "pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016440848s
Jun  9 13:37:12.868: INFO: Pod "pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024334391s
STEP: Saw pod success
Jun  9 13:37:12.868: INFO: Pod "pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622" satisfied condition "success or failure"
Jun  9 13:37:12.872: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622 container test-container: <nil>
STEP: delete the pod
Jun  9 13:37:12.919: INFO: Waiting for pod pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622 to disappear
Jun  9 13:37:12.924: INFO: Pod pod-dd90bd65-b1ee-4e92-83aa-cdcca93b7622 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:37:12.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-804" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2752,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:37:12.943: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:37:17.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8253" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":2754,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:37:17.229: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5895
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun  9 13:37:17.408: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun  9 13:37:36.172: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:37:41.044: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:00.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5895" for this suite.

• [SLOW TEST:43.144 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":178,"skipped":2767,"failed":0}
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:00.374: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jun  9 13:38:00.569: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:04.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7167" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":179,"skipped":2767,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:38:05.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:38:07.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306685, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306685, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306685, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306685, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:38:10.646: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun  9 13:38:14.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 attach --namespace=webhook-2236 to-be-attached-pod -i -c=container1'
Jun  9 13:38:14.870: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:14.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2236" for this suite.
STEP: Destroying namespace "webhook-2236-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.292 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":180,"skipped":2778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:15.039: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  9 13:38:17.369: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1060" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":2806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:17.434: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:38:17.623: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  9 13:38:17.648: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  9 13:38:22.665: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  9 13:38:22.665: INFO: Creating deployment "test-rolling-update-deployment"
Jun  9 13:38:22.690: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  9 13:38:22.715: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  9 13:38:24.729: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  9 13:38:24.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306702, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306702, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306702, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727306702, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 13:38:26.745: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jun  9 13:38:26.761: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1599 /apis/apps/v1/namespaces/deployment-1599/deployments/test-rolling-update-deployment 76cb4d94-dddf-4b50-a234-54f64e9cfe9d 33750 1 2020-06-09 13:38:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00616ee68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-09 13:38:22 +0000 UTC,LastTransitionTime:2020-06-09 13:38:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-06-09 13:38:25 +0000 UTC,LastTransitionTime:2020-06-09 13:38:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 13:38:26.766: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-1599 /apis/apps/v1/namespaces/deployment-1599/replicasets/test-rolling-update-deployment-67cf4f6444 10344281-8a86-4548-9494-a0370b4ff4fa 33739 1 2020-06-09 13:38:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 76cb4d94-dddf-4b50-a234-54f64e9cfe9d 0xc00616f5b7 0xc00616f5b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00616f6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:38:26.766: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  9 13:38:26.766: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1599 /apis/apps/v1/namespaces/deployment-1599/replicasets/test-rolling-update-controller f10f9201-418d-49d6-b0b0-6a28dbe6f788 33749 2 2020-06-09 13:38:17 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 76cb4d94-dddf-4b50-a234-54f64e9cfe9d 0xc00616f437 0xc00616f438}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00616f4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 13:38:26.772: INFO: Pod "test-rolling-update-deployment-67cf4f6444-xq2s7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-xq2s7 test-rolling-update-deployment-67cf4f6444- deployment-1599 /api/v1/namespaces/deployment-1599/pods/test-rolling-update-deployment-67cf4f6444-xq2s7 b567ecd6-f62c-4d67-a91d-fbc7d49100bd 33738 0 2020-06-09 13:38:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 10344281-8a86-4548-9494-a0370b4ff4fa 0xc00616fe37 0xc00616fe38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tfmnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tfmnl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tfmnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:38:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:38:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:172.24.106.1,StartTime:2020-06-09 13:38:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:38:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://f3c7b8dc5fd60ccfcfedaf02a40dfd286b95c513b9d52c27150c0434bb578157,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.106.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:26.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1599" for this suite.

• [SLOW TEST:9.368 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":182,"skipped":2830,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:26.803: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-2163/secret-test-e153c7f7-51d0-4235-8ca4-05a19bac4f45
STEP: Creating a pod to test consume secrets
Jun  9 13:38:27.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d" in namespace "secrets-2163" to be "success or failure"
Jun  9 13:38:27.086: INFO: Pod "pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875163ms
Jun  9 13:38:29.094: INFO: Pod "pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012860775s
Jun  9 13:38:31.104: INFO: Pod "pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022525486s
STEP: Saw pod success
Jun  9 13:38:31.104: INFO: Pod "pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d" satisfied condition "success or failure"
Jun  9 13:38:31.108: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d container env-test: <nil>
STEP: delete the pod
Jun  9 13:38:31.142: INFO: Waiting for pod pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d to disappear
Jun  9 13:38:31.147: INFO: Pod pod-configmaps-7844c659-a926-4c19-a008-97acce994a5d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:31.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2163" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:31.164: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-1b0afd54-1b7d-4819-886e-15fd37f8695d
STEP: Creating a pod to test consume secrets
Jun  9 13:38:31.395: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156" in namespace "projected-7453" to be "success or failure"
Jun  9 13:38:31.410: INFO: Pod "pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156": Phase="Pending", Reason="", readiness=false. Elapsed: 15.694653ms
Jun  9 13:38:33.436: INFO: Pod "pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041258509s
Jun  9 13:38:35.448: INFO: Pod "pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053461735s
STEP: Saw pod success
Jun  9 13:38:35.448: INFO: Pod "pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156" satisfied condition "success or failure"
Jun  9 13:38:35.456: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:38:35.492: INFO: Waiting for pod pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156 to disappear
Jun  9 13:38:35.496: INFO: Pod pod-projected-secrets-cead0588-f1b5-4dbc-a1db-1908c18bd156 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:38:35.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7453" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":184,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:38:35.521: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-758dbf07-ef14-4132-91cd-6e63a1e32370 in namespace container-probe-1157
Jun  9 13:38:39.739: INFO: Started pod busybox-758dbf07-ef14-4132-91cd-6e63a1e32370 in namespace container-probe-1157
STEP: checking the pod's current state and verifying that restartCount is present
Jun  9 13:38:39.744: INFO: Initial restart count of pod busybox-758dbf07-ef14-4132-91cd-6e63a1e32370 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:42:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1157" for this suite.

• [SLOW TEST:245.191 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2896,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:42:40.713: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:42:40.968: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-18028784-7f77-4667-a592-1069a0405df3" in namespace "security-context-test-504" to be "success or failure"
Jun  9 13:42:40.974: INFO: Pod "busybox-privileged-false-18028784-7f77-4667-a592-1069a0405df3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323006ms
Jun  9 13:42:42.985: INFO: Pod "busybox-privileged-false-18028784-7f77-4667-a592-1069a0405df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017547577s
Jun  9 13:42:44.993: INFO: Pod "busybox-privileged-false-18028784-7f77-4667-a592-1069a0405df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025230757s
Jun  9 13:42:44.994: INFO: Pod "busybox-privileged-false-18028784-7f77-4667-a592-1069a0405df3" satisfied condition "success or failure"
Jun  9 13:42:45.015: INFO: Got logs for pod "busybox-privileged-false-18028784-7f77-4667-a592-1069a0405df3": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:42:45.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-504" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":186,"skipped":2915,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:42:45.036: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-d50474d2-b238-46c8-8f06-8aea494a8b3d in namespace container-probe-1500
Jun  9 13:42:47.280: INFO: Started pod test-webserver-d50474d2-b238-46c8-8f06-8aea494a8b3d in namespace container-probe-1500
STEP: checking the pod's current state and verifying that restartCount is present
Jun  9 13:42:47.285: INFO: Initial restart count of pod test-webserver-d50474d2-b238-46c8-8f06-8aea494a8b3d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:46:48.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1500" for this suite.

• [SLOW TEST:243.290 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":2917,"failed":0}
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:46:48.326: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1609
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f1496d4c-73a5-4239-8eb3-df8147b42bfd
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f1496d4c-73a5-4239-8eb3-df8147b42bfd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:46:54.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1609" for this suite.

• [SLOW TEST:6.310 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":188,"skipped":2917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:46:54.646: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 13:46:54.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9897'
Jun  9 13:46:56.188: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  9 13:46:56.188: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jun  9 13:46:56.213: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-99vdq]
Jun  9 13:46:56.213: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-99vdq" in namespace "kubectl-9897" to be "running and ready"
Jun  9 13:46:56.230: INFO: Pod "e2e-test-httpd-rc-99vdq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.512002ms
Jun  9 13:46:58.237: INFO: Pod "e2e-test-httpd-rc-99vdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.024267759s
Jun  9 13:46:58.237: INFO: Pod "e2e-test-httpd-rc-99vdq" satisfied condition "running and ready"
Jun  9 13:46:58.237: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-99vdq]
Jun  9 13:46:58.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs rc/e2e-test-httpd-rc --namespace=kubectl-9897'
Jun  9 13:46:58.405: INFO: stderr: ""
Jun  9 13:46:58.405: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.24.106.54. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.24.106.54. Set the 'ServerName' directive globally to suppress this message\n[Tue Jun 09 13:46:58.095442 2020] [mpm_event:notice] [pid 1:tid 140005111245672] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue Jun 09 13:46:58.095528 2020] [core:notice] [pid 1:tid 140005111245672] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Jun  9 13:46:58.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete rc e2e-test-httpd-rc --namespace=kubectl-9897'
Jun  9 13:46:58.549: INFO: stderr: ""
Jun  9 13:46:58.549: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:46:58.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9897" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":189,"skipped":3008,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:46:58.571: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jun  9 13:46:58.792: INFO: Waiting up to 5m0s for pod "client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c" in namespace "containers-557" to be "success or failure"
Jun  9 13:46:58.797: INFO: Pod "client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.832294ms
Jun  9 13:47:00.804: INFO: Pod "client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012305714s
Jun  9 13:47:02.813: INFO: Pod "client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021433075s
STEP: Saw pod success
Jun  9 13:47:02.813: INFO: Pod "client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c" satisfied condition "success or failure"
Jun  9 13:47:02.818: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c container test-container: <nil>
STEP: delete the pod
Jun  9 13:47:02.877: INFO: Waiting for pod client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c to disappear
Jun  9 13:47:02.881: INFO: Pod client-containers-8cd3da81-835c-407a-84d7-afab0dc7fd2c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:02.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-557" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3010,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:02.903: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9361
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  9 13:47:03.122: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  9 13:47:29.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.24.106.52:8080/dial?request=hostname&protocol=udp&host=172.24.173.53&port=8081&tries=1'] Namespace:pod-network-test-9361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:47:29.345: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:47:29.576: INFO: Waiting for responses: map[]
Jun  9 13:47:29.583: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.24.106.52:8080/dial?request=hostname&protocol=udp&host=172.24.160.107&port=8081&tries=1'] Namespace:pod-network-test-9361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:47:29.583: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:47:29.808: INFO: Waiting for responses: map[]
Jun  9 13:47:29.813: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.24.106.52:8080/dial?request=hostname&protocol=udp&host=172.24.106.48&port=8081&tries=1'] Namespace:pod-network-test-9361 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:47:29.814: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:47:30.034: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:30.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9361" for this suite.

• [SLOW TEST:27.160 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:30.064: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-671
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-671 to expose endpoints map[]
Jun  9 13:47:30.313: INFO: Get endpoints failed (19.409929ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun  9 13:47:31.318: INFO: successfully validated that service endpoint-test2 in namespace services-671 exposes endpoints map[] (1.024790111s elapsed)
STEP: Creating pod pod1 in namespace services-671
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-671 to expose endpoints map[pod1:[80]]
Jun  9 13:47:34.390: INFO: successfully validated that service endpoint-test2 in namespace services-671 exposes endpoints map[pod1:[80]] (3.06009295s elapsed)
STEP: Creating pod pod2 in namespace services-671
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-671 to expose endpoints map[pod1:[80] pod2:[80]]
Jun  9 13:47:37.525: INFO: successfully validated that service endpoint-test2 in namespace services-671 exposes endpoints map[pod1:[80] pod2:[80]] (3.114041254s elapsed)
STEP: Deleting pod pod1 in namespace services-671
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-671 to expose endpoints map[pod2:[80]]
Jun  9 13:47:38.580: INFO: successfully validated that service endpoint-test2 in namespace services-671 exposes endpoints map[pod2:[80]] (1.046849621s elapsed)
STEP: Deleting pod pod2 in namespace services-671
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-671 to expose endpoints map[]
Jun  9 13:47:38.608: INFO: successfully validated that service endpoint-test2 in namespace services-671 exposes endpoints map[] (9.423727ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:38.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-671" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.685 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":192,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5294
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:47:38.978: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:40.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5294" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":193,"skipped":3078,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:40.769: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:52.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1886" for this suite.

• [SLOW TEST:12.256 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":194,"skipped":3080,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:53.025: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-6984a0ad-bf48-47f1-930d-34490e20b9f7
STEP: Creating a pod to test consume secrets
Jun  9 13:47:53.344: INFO: Waiting up to 5m0s for pod "pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9" in namespace "secrets-4037" to be "success or failure"
Jun  9 13:47:53.363: INFO: Pod "pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.798711ms
Jun  9 13:47:55.376: INFO: Pod "pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031864442s
Jun  9 13:47:57.383: INFO: Pod "pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038940206s
STEP: Saw pod success
Jun  9 13:47:57.383: INFO: Pod "pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9" satisfied condition "success or failure"
Jun  9 13:47:57.387: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9 container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 13:47:57.426: INFO: Waiting for pod pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9 to disappear
Jun  9 13:47:57.435: INFO: Pod pod-secrets-1cf9eb58-d63a-4cd7-a5c7-37c0de0b2dd9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:57.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4037" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":195,"skipped":3086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:57.452: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  9 13:47:57.667: INFO: Waiting up to 5m0s for pod "pod-cc4e05c8-71cc-465b-9540-cddddf122dfc" in namespace "emptydir-1694" to be "success or failure"
Jun  9 13:47:57.677: INFO: Pod "pod-cc4e05c8-71cc-465b-9540-cddddf122dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.677648ms
Jun  9 13:47:59.689: INFO: Pod "pod-cc4e05c8-71cc-465b-9540-cddddf122dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021594666s
STEP: Saw pod success
Jun  9 13:47:59.689: INFO: Pod "pod-cc4e05c8-71cc-465b-9540-cddddf122dfc" satisfied condition "success or failure"
Jun  9 13:47:59.695: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-cc4e05c8-71cc-465b-9540-cddddf122dfc container test-container: <nil>
STEP: delete the pod
Jun  9 13:47:59.753: INFO: Waiting for pod pod-cc4e05c8-71cc-465b-9540-cddddf122dfc to disappear
Jun  9 13:47:59.769: INFO: Pod pod-cc4e05c8-71cc-465b-9540-cddddf122dfc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:59.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1694" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":196,"skipped":3141,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:47:59.786: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7804
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-d0bcb741-416f-4335-8c15-0f7fef31527c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:47:59.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7804" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":197,"skipped":3151,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:00.022: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:48:00.790: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:48:02.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307280, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307280, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307280, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307280, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:48:05.876: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:48:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-862" for this suite.
STEP: Destroying namespace "webhook-862-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":198,"skipped":3157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:06.507: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3443
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jun  9 13:48:06.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-3443'
Jun  9 13:48:07.345: INFO: stderr: ""
Jun  9 13:48:07.345: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun  9 13:48:08.352: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:48:08.352: INFO: Found 0 / 1
Jun  9 13:48:09.352: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:48:09.352: INFO: Found 0 / 1
Jun  9 13:48:10.352: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:48:10.352: INFO: Found 1 / 1
Jun  9 13:48:10.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun  9 13:48:10.356: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:48:10.357: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  9 13:48:10.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 patch pod agnhost-master-t67dd --namespace=kubectl-3443 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  9 13:48:10.483: INFO: stderr: ""
Jun  9 13:48:10.483: INFO: stdout: "pod/agnhost-master-t67dd patched\n"
STEP: checking annotations
Jun  9 13:48:10.486: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 13:48:10.487: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:48:10.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3443" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":199,"skipped":3217,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:10.500: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3101
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3101
I0609 13:48:10.719201      24 runners.go:189] Created replication controller with name: externalname-service, namespace: services-3101, replica count: 2
I0609 13:48:13.770175      24 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 13:48:13.770: INFO: Creating new exec pod
Jun  9 13:48:18.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-3101 execpod9j9gc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun  9 13:48:19.177: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  9 13:48:19.177: INFO: stdout: ""
Jun  9 13:48:19.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=services-3101 execpod9j9gc -- /bin/sh -x -c nc -zv -t -w 2 172.31.149.86 80'
Jun  9 13:48:19.515: INFO: stderr: "+ nc -zv -t -w 2 172.31.149.86 80\nConnection to 172.31.149.86 80 port [tcp/http] succeeded!\n"
Jun  9 13:48:19.515: INFO: stdout: ""
Jun  9 13:48:19.515: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:48:19.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3101" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.093 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":200,"skipped":3229,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:19.594: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:48:21.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:48:23.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307301, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307301, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307301, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307301, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:48:26.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:48:26.501: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7415-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:48:27.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6620" for this suite.
STEP: Destroying namespace "webhook-6620-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.879 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":201,"skipped":3245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:27.487: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2055
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:48:27.870: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:48:28.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2055" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":202,"skipped":3301,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:28.936: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 13:48:29.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225" in namespace "downward-api-8255" to be "success or failure"
Jun  9 13:48:29.152: INFO: Pod "downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225": Phase="Pending", Reason="", readiness=false. Elapsed: 4.550248ms
Jun  9 13:48:31.161: INFO: Pod "downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014036291s
Jun  9 13:48:33.183: INFO: Pod "downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035485491s
STEP: Saw pod success
Jun  9 13:48:33.183: INFO: Pod "downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225" satisfied condition "success or failure"
Jun  9 13:48:33.202: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225 container client-container: <nil>
STEP: delete the pod
Jun  9 13:48:33.250: INFO: Waiting for pod downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225 to disappear
Jun  9 13:48:33.267: INFO: Pod downwardapi-volume-3d1cb75b-0e83-4af5-8d39-939b85d60225 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:48:33.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8255" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":203,"skipped":3302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:48:33.298: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun  9 13:48:33.941: INFO: Pod name wrapped-volume-race-28325586-514f-47db-99d5-2040f9a7d4d5: Found 0 pods out of 5
Jun  9 13:48:38.955: INFO: Pod name wrapped-volume-race-28325586-514f-47db-99d5-2040f9a7d4d5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-28325586-514f-47db-99d5-2040f9a7d4d5 in namespace emptydir-wrapper-8741, will wait for the garbage collector to delete the pods
Jun  9 13:48:53.097: INFO: Deleting ReplicationController wrapped-volume-race-28325586-514f-47db-99d5-2040f9a7d4d5 took: 13.921174ms
Jun  9 13:48:53.297: INFO: Terminating ReplicationController wrapped-volume-race-28325586-514f-47db-99d5-2040f9a7d4d5 pods took: 200.606463ms
STEP: Creating RC which spawns configmap-volume pods
Jun  9 13:49:07.244: INFO: Pod name wrapped-volume-race-555e861e-da24-453c-8013-7b3c83520107: Found 0 pods out of 5
Jun  9 13:49:12.259: INFO: Pod name wrapped-volume-race-555e861e-da24-453c-8013-7b3c83520107: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-555e861e-da24-453c-8013-7b3c83520107 in namespace emptydir-wrapper-8741, will wait for the garbage collector to delete the pods
Jun  9 13:49:24.370: INFO: Deleting ReplicationController wrapped-volume-race-555e861e-da24-453c-8013-7b3c83520107 took: 10.940876ms
Jun  9 13:49:24.470: INFO: Terminating ReplicationController wrapped-volume-race-555e861e-da24-453c-8013-7b3c83520107 pods took: 100.551887ms
STEP: Creating RC which spawns configmap-volume pods
Jun  9 13:49:36.679: INFO: Pod name wrapped-volume-race-e2b77188-6d2f-40d7-a906-18ff76949c5b: Found 0 pods out of 5
Jun  9 13:49:41.697: INFO: Pod name wrapped-volume-race-e2b77188-6d2f-40d7-a906-18ff76949c5b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e2b77188-6d2f-40d7-a906-18ff76949c5b in namespace emptydir-wrapper-8741, will wait for the garbage collector to delete the pods
Jun  9 13:49:51.805: INFO: Deleting ReplicationController wrapped-volume-race-e2b77188-6d2f-40d7-a906-18ff76949c5b took: 15.89157ms
Jun  9 13:49:51.906: INFO: Terminating ReplicationController wrapped-volume-race-e2b77188-6d2f-40d7-a906-18ff76949c5b pods took: 101.170711ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:49:58.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8741" for this suite.

• [SLOW TEST:85.646 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":204,"skipped":3326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:49:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7650
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-06f10311-e97a-4f53-88d8-c912c4830df2
STEP: Creating secret with name s-test-opt-upd-9432ba8e-ac99-4771-a84a-c30e49ca6c0f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-06f10311-e97a-4f53-88d8-c912c4830df2
STEP: Updating secret s-test-opt-upd-9432ba8e-ac99-4771-a84a-c30e49ca6c0f
STEP: Creating secret with name s-test-opt-create-1a081ac8-1438-4c83-92bf-ba3d3f40cb20
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:07.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7650" for this suite.

• [SLOW TEST:8.450 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":205,"skipped":3349,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:07.397: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jun  9 13:50:07.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 cluster-info'
Jun  9 13:50:07.705: INFO: stderr: ""
Jun  9 13:50:07.705: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8208" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":206,"skipped":3354,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:07.722: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-5c353045-f75b-489a-8e4d-e03becab86bf
STEP: Creating a pod to test consume configMaps
Jun  9 13:50:07.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf" in namespace "projected-4241" to be "success or failure"
Jun  9 13:50:07.952: INFO: Pod "pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810259ms
Jun  9 13:50:09.960: INFO: Pod "pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012513541s
STEP: Saw pod success
Jun  9 13:50:09.960: INFO: Pod "pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf" satisfied condition "success or failure"
Jun  9 13:50:09.967: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 13:50:10.028: INFO: Waiting for pod pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf to disappear
Jun  9 13:50:10.035: INFO: Pod pod-projected-configmaps-ff93d6cd-f652-4df9-8bc2-71e56094f1bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:10.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4241" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3363,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:10.063: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:14.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9810" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":208,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:14.428: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1269
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1269
STEP: Creating statefulset with conflicting port in namespace statefulset-1269
STEP: Waiting until pod test-pod will start running in namespace statefulset-1269
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1269
Jun  9 13:50:20.658: INFO: Observed stateful pod in namespace: statefulset-1269, name: ss-0, uid: d1d66710-04b7-420b-89ba-3f895634bb15, status phase: Pending. Waiting for statefulset controller to delete.
Jun  9 13:50:20.850: INFO: Observed stateful pod in namespace: statefulset-1269, name: ss-0, uid: d1d66710-04b7-420b-89ba-3f895634bb15, status phase: Failed. Waiting for statefulset controller to delete.
Jun  9 13:50:20.865: INFO: Observed stateful pod in namespace: statefulset-1269, name: ss-0, uid: d1d66710-04b7-420b-89ba-3f895634bb15, status phase: Failed. Waiting for statefulset controller to delete.
Jun  9 13:50:20.874: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1269
STEP: Removing pod with conflicting port in namespace statefulset-1269
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1269 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun  9 13:50:24.944: INFO: Deleting all statefulset in ns statefulset-1269
Jun  9 13:50:24.951: INFO: Scaling statefulset ss to 0
Jun  9 13:50:34.979: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:50:34.983: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:35.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1269" for this suite.

• [SLOW TEST:20.631 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":209,"skipped":3400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:35.064: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  9 13:50:35.313: INFO: Waiting up to 5m0s for pod "pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6" in namespace "emptydir-6293" to be "success or failure"
Jun  9 13:50:35.317: INFO: Pod "pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195867ms
Jun  9 13:50:37.325: INFO: Pod "pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012308761s
Jun  9 13:50:39.332: INFO: Pod "pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019233369s
STEP: Saw pod success
Jun  9 13:50:39.332: INFO: Pod "pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6" satisfied condition "success or failure"
Jun  9 13:50:39.337: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6 container test-container: <nil>
STEP: delete the pod
Jun  9 13:50:39.368: INFO: Waiting for pod pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6 to disappear
Jun  9 13:50:39.372: INFO: Pod pod-975cbc51-c055-4ed7-9ae2-54fc988b33d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:39.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6293" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:39.390: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-146" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":211,"skipped":3486,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:43.681: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun  9 13:50:53.915: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:50:53.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0609 13:50:53.915546      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4971" for this suite.

• [SLOW TEST:10.251 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":212,"skipped":3492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:50:53.934: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-hc5g
STEP: Creating a pod to test atomic-volume-subpath
Jun  9 13:50:54.158: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hc5g" in namespace "subpath-7324" to be "success or failure"
Jun  9 13:50:54.177: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Pending", Reason="", readiness=false. Elapsed: 18.614312ms
Jun  9 13:50:56.184: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025126269s
Jun  9 13:50:58.192: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.033527378s
Jun  9 13:51:00.201: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 6.042645328s
Jun  9 13:51:02.208: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 8.048914079s
Jun  9 13:51:04.213: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.054089207s
Jun  9 13:51:06.221: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 12.062592935s
Jun  9 13:51:08.241: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.081851441s
Jun  9 13:51:10.249: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.090249002s
Jun  9 13:51:12.257: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.098639292s
Jun  9 13:51:14.268: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 20.109120713s
Jun  9 13:51:16.277: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Running", Reason="", readiness=true. Elapsed: 22.118544965s
Jun  9 13:51:18.285: INFO: Pod "pod-subpath-test-configmap-hc5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.126071989s
STEP: Saw pod success
Jun  9 13:51:18.285: INFO: Pod "pod-subpath-test-configmap-hc5g" satisfied condition "success or failure"
Jun  9 13:51:18.292: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-subpath-test-configmap-hc5g container test-container-subpath-configmap-hc5g: <nil>
STEP: delete the pod
Jun  9 13:51:18.331: INFO: Waiting for pod pod-subpath-test-configmap-hc5g to disappear
Jun  9 13:51:18.338: INFO: Pod pod-subpath-test-configmap-hc5g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hc5g
Jun  9 13:51:18.338: INFO: Deleting pod "pod-subpath-test-configmap-hc5g" in namespace "subpath-7324"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:51:18.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7324" for this suite.

• [SLOW TEST:24.424 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":213,"skipped":3527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:51:18.365: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:51:35.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4482" for this suite.

• [SLOW TEST:17.292 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":214,"skipped":3552,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:51:35.670: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 13:51:36.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 13:51:38.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307496, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307496, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307496, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727307496, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 13:51:41.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:51:41.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9497" for this suite.
STEP: Destroying namespace "webhook-9497-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.298 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":215,"skipped":3570,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:51:41.968: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:51:42.236: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun  9 13:51:42.249: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:42.258: INFO: Number of nodes with available pods: 0
Jun  9 13:51:42.258: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:51:43.266: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:43.286: INFO: Number of nodes with available pods: 0
Jun  9 13:51:43.286: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:51:44.265: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:44.277: INFO: Number of nodes with available pods: 0
Jun  9 13:51:44.277: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:51:45.267: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:45.279: INFO: Number of nodes with available pods: 3
Jun  9 13:51:45.279: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun  9 13:51:45.324: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:45.324: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:45.324: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:45.332: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:46.340: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:46.341: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:46.341: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:46.350: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:47.343: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:47.343: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:47.343: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:47.362: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:48.340: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:48.340: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:48.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:48.347: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:49.353: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:49.353: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:49.353: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:49.353: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:49.367: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:50.342: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:50.343: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:50.343: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:50.343: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:50.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:51.341: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:51.341: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:51.341: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:51.341: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:51.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:52.339: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:52.339: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:52.339: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:52.339: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:52.345: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:53.339: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:53.339: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:53.339: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:53.339: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:53.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:54.341: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:54.341: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:54.341: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:54.341: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:54.348: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:55.339: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:55.339: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:55.339: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:55.339: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:55.345: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:56.339: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:56.340: INFO: Wrong image for pod: daemon-set-snwbb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:56.340: INFO: Pod daemon-set-snwbb is not available
Jun  9 13:51:56.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:56.346: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:57.340: INFO: Pod daemon-set-fpsw7 is not available
Jun  9 13:51:57.340: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:57.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:57.347: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:58.340: INFO: Pod daemon-set-fpsw7 is not available
Jun  9 13:51:58.340: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:58.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:58.348: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:51:59.339: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:59.343: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:51:59.353: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:00.343: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:00.343: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:00.350: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:01.340: INFO: Wrong image for pod: daemon-set-pvtrd. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:01.340: INFO: Pod daemon-set-pvtrd is not available
Jun  9 13:52:01.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:01.346: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:02.367: INFO: Pod daemon-set-hfv2b is not available
Jun  9 13:52:02.367: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:02.374: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:03.346: INFO: Pod daemon-set-hfv2b is not available
Jun  9 13:52:03.347: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:03.361: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:04.345: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:04.355: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:05.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:05.347: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:06.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:06.340: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:06.346: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:07.350: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:07.350: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:07.364: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:08.342: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:08.342: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:08.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:09.340: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:09.340: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:09.346: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:10.339: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:10.339: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:10.347: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:11.341: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:11.341: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:11.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:12.346: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:12.346: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:12.353: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:13.342: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:13.342: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:13.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:14.343: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:14.343: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:14.358: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:15.339: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:15.339: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:15.348: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:16.338: INFO: Wrong image for pod: daemon-set-wlcnc. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jun  9 13:52:16.338: INFO: Pod daemon-set-wlcnc is not available
Jun  9 13:52:16.349: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:17.339: INFO: Pod daemon-set-7rpjl is not available
Jun  9 13:52:17.346: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun  9 13:52:17.356: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:17.368: INFO: Number of nodes with available pods: 2
Jun  9 13:52:17.368: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:52:18.381: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:18.394: INFO: Number of nodes with available pods: 2
Jun  9 13:52:18.394: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 13:52:19.377: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 13:52:19.398: INFO: Number of nodes with available pods: 3
Jun  9 13:52:19.398: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3337, will wait for the garbage collector to delete the pods
Jun  9 13:52:19.493: INFO: Deleting DaemonSet.extensions daemon-set took: 11.601493ms
Jun  9 13:52:19.594: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.708248ms
Jun  9 13:52:26.598: INFO: Number of nodes with available pods: 0
Jun  9 13:52:26.599: INFO: Number of running nodes: 0, number of available pods: 0
Jun  9 13:52:26.604: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3337/daemonsets","resourceVersion":"38916"},"items":null}

Jun  9 13:52:26.611: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3337/pods","resourceVersion":"38916"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:52:26.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3337" for this suite.

• [SLOW TEST:44.679 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":216,"skipped":3583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:52:26.652: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-hs67
STEP: Creating a pod to test atomic-volume-subpath
Jun  9 13:52:26.884: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hs67" in namespace "subpath-7040" to be "success or failure"
Jun  9 13:52:26.887: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963026ms
Jun  9 13:52:28.894: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010059878s
Jun  9 13:52:30.904: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 4.020325686s
Jun  9 13:52:32.911: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 6.027217452s
Jun  9 13:52:34.918: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 8.034286822s
Jun  9 13:52:36.925: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 10.041057565s
Jun  9 13:52:38.932: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 12.04795857s
Jun  9 13:52:40.939: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 14.055055632s
Jun  9 13:52:42.946: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 16.061977355s
Jun  9 13:52:44.955: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 18.070919147s
Jun  9 13:52:46.960: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 20.075884016s
Jun  9 13:52:48.968: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Running", Reason="", readiness=true. Elapsed: 22.084592147s
Jun  9 13:52:50.977: INFO: Pod "pod-subpath-test-projected-hs67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.092994709s
STEP: Saw pod success
Jun  9 13:52:50.977: INFO: Pod "pod-subpath-test-projected-hs67" satisfied condition "success or failure"
Jun  9 13:52:50.983: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-subpath-test-projected-hs67 container test-container-subpath-projected-hs67: <nil>
STEP: delete the pod
Jun  9 13:52:51.075: INFO: Waiting for pod pod-subpath-test-projected-hs67 to disappear
Jun  9 13:52:51.081: INFO: Pod pod-subpath-test-projected-hs67 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hs67
Jun  9 13:52:51.081: INFO: Deleting pod "pod-subpath-test-projected-hs67" in namespace "subpath-7040"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:52:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7040" for this suite.

• [SLOW TEST:24.457 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":217,"skipped":3629,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:52:51.113: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-e25cf042-6029-4077-9893-180230802c36
STEP: Creating a pod to test consume configMaps
Jun  9 13:52:51.328: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0" in namespace "projected-1340" to be "success or failure"
Jun  9 13:52:51.332: INFO: Pod "pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337036ms
Jun  9 13:52:53.340: INFO: Pod "pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01168362s
Jun  9 13:52:55.347: INFO: Pod "pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019141721s
Jun  9 13:52:57.356: INFO: Pod "pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027811511s
STEP: Saw pod success
Jun  9 13:52:57.356: INFO: Pod "pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0" satisfied condition "success or failure"
Jun  9 13:52:57.360: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 13:52:57.401: INFO: Waiting for pod pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0 to disappear
Jun  9 13:52:57.418: INFO: Pod pod-projected-configmaps-df1d2d0f-e3e9-4b79-bd3f-3bb2d645d1c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:52:57.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1340" for this suite.

• [SLOW TEST:6.326 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":218,"skipped":3634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:52:57.443: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:52:57.625: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:53:01.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8230" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":219,"skipped":3683,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:53:01.740: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:53:05.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4776" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:53:06.008: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8252
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun  9 13:53:06.211: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:53:11.094: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:53:30.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8252" for this suite.

• [SLOW TEST:24.869 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":221,"skipped":3706,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:53:30.877: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-aa4df606-b407-41a6-a4d5-98890b41c6be in namespace container-probe-9724
Jun  9 13:53:35.136: INFO: Started pod liveness-aa4df606-b407-41a6-a4d5-98890b41c6be in namespace container-probe-9724
STEP: checking the pod's current state and verifying that restartCount is present
Jun  9 13:53:35.140: INFO: Initial restart count of pod liveness-aa4df606-b407-41a6-a4d5-98890b41c6be is 0
Jun  9 13:53:57.238: INFO: Restart count of pod container-probe-9724/liveness-aa4df606-b407-41a6-a4d5-98890b41c6be is now 1 (22.098163435s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:53:57.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9724" for this suite.

• [SLOW TEST:26.402 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":222,"skipped":3709,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:53:57.279: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1149
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun  9 13:53:57.474: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:02.246: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:54:21.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1149" for this suite.

• [SLOW TEST:24.120 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":223,"skipped":3714,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:54:21.401: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun  9 13:54:29.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:29.673: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:29.868: INFO: Exec stderr: ""
Jun  9 13:54:29.868: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:29.868: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:30.068: INFO: Exec stderr: ""
Jun  9 13:54:30.068: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:30.068: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:30.304: INFO: Exec stderr: ""
Jun  9 13:54:30.304: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:30.304: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:30.517: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun  9 13:54:30.518: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:30.518: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:30.698: INFO: Exec stderr: ""
Jun  9 13:54:30.698: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:30.698: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:30.950: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun  9 13:54:30.950: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:30.950: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:31.136: INFO: Exec stderr: ""
Jun  9 13:54:31.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:31.136: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:31.334: INFO: Exec stderr: ""
Jun  9 13:54:31.334: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:31.334: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:31.518: INFO: Exec stderr: ""
Jun  9 13:54:31.518: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5432 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 13:54:31.518: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 13:54:31.713: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:54:31.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5432" for this suite.

• [SLOW TEST:10.329 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":224,"skipped":3728,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:54:31.730: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 13:54:31.942: INFO: (0) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.45071ms)
Jun  9 13:54:31.950: INFO: (1) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.422047ms)
Jun  9 13:54:31.956: INFO: (2) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.32961ms)
Jun  9 13:54:31.964: INFO: (3) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.549857ms)
Jun  9 13:54:31.971: INFO: (4) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.711221ms)
Jun  9 13:54:31.981: INFO: (5) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.072276ms)
Jun  9 13:54:31.987: INFO: (6) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.481415ms)
Jun  9 13:54:31.994: INFO: (7) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.916812ms)
Jun  9 13:54:32.000: INFO: (8) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.322666ms)
Jun  9 13:54:32.005: INFO: (9) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.32861ms)
Jun  9 13:54:32.011: INFO: (10) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.172539ms)
Jun  9 13:54:32.017: INFO: (11) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.916797ms)
Jun  9 13:54:32.022: INFO: (12) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.664233ms)
Jun  9 13:54:32.032: INFO: (13) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.389774ms)
Jun  9 13:54:32.040: INFO: (14) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.885491ms)
Jun  9 13:54:32.051: INFO: (15) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 10.931578ms)
Jun  9 13:54:32.059: INFO: (16) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.038378ms)
Jun  9 13:54:32.066: INFO: (17) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.99752ms)
Jun  9 13:54:32.073: INFO: (18) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.142352ms)
Jun  9 13:54:32.080: INFO: (19) /api/v1/nodes/worker-k8xcg-8bbfd5b68-w4htb:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.401316ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:54:32.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1792" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":225,"skipped":3740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:54:32.096: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun  9 13:54:34.326: INFO: &Pod{ObjectMeta:{send-events-e33b16b4-cc28-4d56-bdbb-ddd90290bb32  events-1421 /api/v1/namespaces/events-1421/pods/send-events-e33b16b4-cc28-4d56-bdbb-ddd90290bb32 ba636b64-5d01-4e86-a790-bc93c3945abf 39629 0 2020-06-09 13:54:32 +0000 UTC <nil> <nil> map[name:foo time:286048861] map[kubernetes.io/psp:cert-exporter-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sz2xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sz2xh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sz2xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-k8xcg-8bbfd5b68-w4htb,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:54:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:54:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:54:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-09 13:54:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.23.24.246,PodIP:172.24.106.33,StartTime:2020-06-09 13:54:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-09 13:54:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://4898144b7f13533f2791aaefa1b4c15171888a2585a6cccdaa9fa02c9346cdb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.24.106.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun  9 13:54:36.335: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun  9 13:54:38.343: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:54:38.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1421" for this suite.

• [SLOW TEST:6.271 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":226,"skipped":3773,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:54:38.368: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:54:54.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2740" for this suite.

• [SLOW TEST:16.403 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":227,"skipped":3780,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:54:54.772: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  9 13:54:55.005: INFO: Waiting up to 5m0s for pod "pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4" in namespace "emptydir-9866" to be "success or failure"
Jun  9 13:54:55.037: INFO: Pod "pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4": Phase="Pending", Reason="", readiness=false. Elapsed: 31.324135ms
Jun  9 13:54:57.044: INFO: Pod "pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038941534s
STEP: Saw pod success
Jun  9 13:54:57.044: INFO: Pod "pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4" satisfied condition "success or failure"
Jun  9 13:54:57.050: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4 container test-container: <nil>
STEP: delete the pod
Jun  9 13:54:57.089: INFO: Waiting for pod pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4 to disappear
Jun  9 13:54:57.094: INFO: Pod pod-a6c02d5d-a07b-43a1-b5ab-c61d2f11abb4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 13:54:57.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9866" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 13:54:57.108: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3983
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-3983
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3983
Jun  9 13:54:57.341: INFO: Found 0 stateful pods, waiting for 1
Jun  9 13:55:07.348: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun  9 13:55:07.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:55:07.921: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:55:07.921: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:55:07.921: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:55:07.927: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  9 13:55:17.949: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:55:17.950: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:55:17.989: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jun  9 13:55:17.989: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:17.989: INFO: 
Jun  9 13:55:17.989: INFO: StatefulSet ss has not reached scale 3, at 1
Jun  9 13:55:18.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983621103s
Jun  9 13:55:20.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97266026s
Jun  9 13:55:21.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.94457131s
Jun  9 13:55:22.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.932987191s
Jun  9 13:55:23.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.922566874s
Jun  9 13:55:24.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914874367s
Jun  9 13:55:25.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907481278s
Jun  9 13:55:26.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898475612s
Jun  9 13:55:27.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 891.225171ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3983
Jun  9 13:55:28.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:55:28.470: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 13:55:28.470: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:55:28.470: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:55:28.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:55:28.839: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  9 13:55:28.839: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:55:28.839: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:55:28.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:55:29.183: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  9 13:55:29.183: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 13:55:29.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 13:55:29.209: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:55:29.209: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 13:55:29.209: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun  9 13:55:29.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:55:29.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:55:29.630: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:55:29.630: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:55:29.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:55:30.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:55:30.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:55:30.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:55:30.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 13:55:30.467: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 13:55:30.467: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 13:55:30.467: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 13:55:30.467: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 13:55:30.475: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  9 13:55:40.488: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:55:40.488: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:55:40.488: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 13:55:40.513: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:40.513: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:40.513: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:40.513: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:40.513: INFO: 
Jun  9 13:55:40.513: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 13:55:41.524: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:41.524: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:41.524: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:41.524: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:41.524: INFO: 
Jun  9 13:55:41.524: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 13:55:42.533: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:42.533: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:42.533: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:42.533: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:42.533: INFO: 
Jun  9 13:55:42.533: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 13:55:43.544: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:43.544: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:43.544: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:43.544: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:43.544: INFO: 
Jun  9 13:55:43.544: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 13:55:44.558: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:44.559: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:44.559: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:44.559: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:44.559: INFO: 
Jun  9 13:55:44.560: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 13:55:45.576: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:45.576: INFO: ss-0  worker-k8xcg-8bbfd5b68-w4htb   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:54:57 +0000 UTC  }]
Jun  9 13:55:45.576: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:45.576: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:45.576: INFO: 
Jun  9 13:55:45.576: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 13:55:46.583: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:46.583: INFO: ss-1  worker-2jqhr-6f5dbbb884-vqc7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:46.583: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:46.583: INFO: 
Jun  9 13:55:46.583: INFO: StatefulSet ss has not reached scale 0, at 2
Jun  9 13:55:47.591: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:47.591: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:47.591: INFO: 
Jun  9 13:55:47.591: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  9 13:55:48.599: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:48.599: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:48.600: INFO: 
Jun  9 13:55:48.600: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  9 13:55:49.607: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jun  9 13:55:49.607: INFO: ss-2  worker-dfhc8-64bc8fc496-xx7cx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-09 13:55:18 +0000 UTC  }]
Jun  9 13:55:49.607: INFO: 
Jun  9 13:55:49.607: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3983
Jun  9 13:55:50.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:55:50.820: INFO: rc: 1
Jun  9 13:55:50.820: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun  9 13:56:00.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:56:00.942: INFO: rc: 1
Jun  9 13:56:00.943: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:56:10.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:56:11.069: INFO: rc: 1
Jun  9 13:56:11.069: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:56:21.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:56:21.206: INFO: rc: 1
Jun  9 13:56:21.206: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:56:31.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:56:31.330: INFO: rc: 1
Jun  9 13:56:31.330: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:56:41.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:56:41.482: INFO: rc: 1
Jun  9 13:56:41.482: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:56:51.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:56:51.611: INFO: rc: 1
Jun  9 13:56:51.611: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:57:01.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:57:02.456: INFO: rc: 1
Jun  9 13:57:02.456: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:57:12.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:57:12.582: INFO: rc: 1
Jun  9 13:57:12.582: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:57:22.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:57:22.710: INFO: rc: 1
Jun  9 13:57:22.710: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:57:32.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:57:32.862: INFO: rc: 1
Jun  9 13:57:32.862: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:57:42.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:57:43.003: INFO: rc: 1
Jun  9 13:57:43.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:57:53.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:57:53.150: INFO: rc: 1
Jun  9 13:57:53.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:58:03.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:58:03.253: INFO: rc: 1
Jun  9 13:58:03.254: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:58:13.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:58:13.372: INFO: rc: 1
Jun  9 13:58:13.373: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:58:23.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:58:23.506: INFO: rc: 1
Jun  9 13:58:23.506: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:58:33.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:58:33.642: INFO: rc: 1
Jun  9 13:58:33.642: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:58:43.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:58:43.763: INFO: rc: 1
Jun  9 13:58:43.763: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:58:53.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:58:53.876: INFO: rc: 1
Jun  9 13:58:53.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:59:03.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:59:03.986: INFO: rc: 1
Jun  9 13:59:03.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:59:13.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:59:14.107: INFO: rc: 1
Jun  9 13:59:14.107: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:59:24.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:59:24.223: INFO: rc: 1
Jun  9 13:59:24.224: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:59:34.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:59:34.344: INFO: rc: 1
Jun  9 13:59:34.344: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:59:44.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:59:44.473: INFO: rc: 1
Jun  9 13:59:44.473: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 13:59:54.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 13:59:54.607: INFO: rc: 1
Jun  9 13:59:54.607: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 14:00:04.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 14:00:04.779: INFO: rc: 1
Jun  9 14:00:04.780: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 14:00:14.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 14:00:14.914: INFO: rc: 1
Jun  9 14:00:14.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 14:00:24.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 14:00:25.061: INFO: rc: 1
Jun  9 14:00:25.061: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 14:00:35.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 14:00:35.203: INFO: rc: 1
Jun  9 14:00:35.203: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 14:00:45.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 14:00:45.348: INFO: rc: 1
Jun  9 14:00:45.348: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  9 14:00:55.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 exec --namespace=statefulset-3983 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 14:00:55.468: INFO: rc: 1
Jun  9 14:00:55.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jun  9 14:00:55.468: INFO: Scaling statefulset ss to 0
Jun  9 14:00:55.482: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun  9 14:00:55.485: INFO: Deleting all statefulset in ns statefulset-3983
Jun  9 14:00:55.489: INFO: Scaling statefulset ss to 0
Jun  9 14:00:55.502: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 14:00:55.505: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:00:55.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3983" for this suite.

• [SLOW TEST:358.439 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":229,"skipped":3822,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:00:55.548: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:00:55.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 version'
Jun  9 14:00:55.881: INFO: stderr: ""
Jun  9 14:00:55.881: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:16:24Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:08:34Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:00:55.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8207" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":230,"skipped":3827,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:00:55.901: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:00:56.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-540" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":231,"skipped":3835,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:00:56.120: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun  9 14:00:56.302: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 14:00:56.323: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 14:00:56.328: INFO: 
Logging pods the kubelet thinks is on node worker-2jqhr-6f5dbbb884-vqc7c before test
Jun  9 14:00:56.353: INFO: kube-proxy-69gzh from kube-system started at 2020-06-09 11:43:27 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.353: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 14:00:56.353: INFO: chart-operator-5b8b4bcc75-wn4xh from giantswarm started at 2020-06-09 11:44:47 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.353: INFO: 	Container chart-operator ready: true, restart count 0
Jun  9 14:00:56.353: INFO: cert-exporter-gkbjk from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.353: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 14:00:56.353: INFO: tiller-deploy-684c6b545b-4w7nq from giantswarm started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.353: INFO: 	Container tiller ready: true, restart count 0
Jun  9 14:00:56.353: INFO: coredns-6d56c484c-7fkzm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.353: INFO: 	Container coredns ready: true, restart count 0
Jun  9 14:00:56.353: INFO: coredns-6d56c484c-txwjd from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.353: INFO: 	Container coredns ready: true, restart count 0
Jun  9 14:00:56.354: INFO: metrics-server-66df9f5b56-bvwdm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.354: INFO: 	Container metrics-server ready: true, restart count 0
Jun  9 14:00:56.354: INFO: net-exporter-4n4fk from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.354: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 14:00:56.354: INFO: docker-mem-limit-startup-script-clstp from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.354: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 14:00:56.355: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-sd6nl from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 14:00:56.355: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  9 14:00:56.355: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 14:00:56.355: INFO: calico-node-vhcbj from kube-system started at 2020-06-09 11:42:16 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.355: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 14:00:56.355: INFO: node-exporter-nsmh9 from kube-system started at 2020-06-09 11:45:08 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.355: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 14:00:56.355: INFO: nginx-ingress-controller-59b5c95c6d-cb4kl from kube-system started at 2020-06-09 11:45:21 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.355: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 14:00:56.355: INFO: 
Logging pods the kubelet thinks is on node worker-dfhc8-64bc8fc496-xx7cx before test
Jun  9 14:00:56.381: INFO: calico-node-7t79n from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 14:00:56.381: INFO: cert-exporter-v64nm from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 14:00:56.381: INFO: node-exporter-k9z45 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 14:00:56.381: INFO: sonobuoy from sonobuoy started at 2020-06-09 12:47:13 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 14:00:56.381: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-p5v44 from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  9 14:00:56.381: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 14:00:56.381: INFO: kube-state-metrics-6d998ffd8b-wtr6p from kube-system started at 2020-06-09 11:44:31 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  9 14:00:56.381: INFO: docker-mem-limit-startup-script-9x4w8 from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 14:00:56.381: INFO: sonobuoy-e2e-job-fb32098c60e64727 from sonobuoy started at 2020-06-09 12:47:22 +0000 UTC (2 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container e2e ready: true, restart count 0
Jun  9 14:00:56.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 14:00:56.381: INFO: kube-proxy-tg6rp from kube-system started at 2020-06-09 11:43:24 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 14:00:56.381: INFO: nginx-ingress-controller-59b5c95c6d-8z4jm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 14:00:56.381: INFO: coredns-6d56c484c-s5fsw from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container coredns ready: true, restart count 0
Jun  9 14:00:56.381: INFO: net-exporter-wvnc9 from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.381: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 14:00:56.381: INFO: 
Logging pods the kubelet thinks is on node worker-k8xcg-8bbfd5b68-w4htb before test
Jun  9 14:00:56.403: INFO: calico-node-zqtv2 from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.403: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 14:00:56.403: INFO: node-exporter-mbg29 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.403: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 14:00:56.403: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-lbr8h from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 14:00:56.403: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  9 14:00:56.403: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 14:00:56.403: INFO: kube-proxy-kfmds from kube-system started at 2020-06-09 11:43:19 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.403: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 14:00:56.403: INFO: docker-mem-limit-startup-script-7t5zg from kube-system started at 2020-06-09 13:32:06 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.403: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 14:00:56.403: INFO: cert-exporter-p2wws from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.403: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 14:00:56.404: INFO: net-exporter-hppdh from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 14:00:56.404: INFO: 	Container net-exporter ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-01a75ca3-f5fe-4bcd-ab34-33b0d9fae146 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-01a75ca3-f5fe-4bcd-ab34-33b0d9fae146 off the node worker-k8xcg-8bbfd5b68-w4htb
STEP: verifying the node doesn't have the label kubernetes.io/e2e-01a75ca3-f5fe-4bcd-ab34-33b0d9fae146
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:01:12.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4639" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:16.498 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":232,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:01:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7363
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:01:12.795: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  9 14:01:17.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7363 create -f -'
Jun  9 14:01:18.974: INFO: stderr: ""
Jun  9 14:01:18.974: INFO: stdout: "e2e-test-crd-publish-openapi-9813-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  9 14:01:18.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7363 delete e2e-test-crd-publish-openapi-9813-crds test-cr'
Jun  9 14:01:19.119: INFO: stderr: ""
Jun  9 14:01:19.119: INFO: stdout: "e2e-test-crd-publish-openapi-9813-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  9 14:01:19.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7363 apply -f -'
Jun  9 14:01:19.505: INFO: stderr: ""
Jun  9 14:01:19.505: INFO: stdout: "e2e-test-crd-publish-openapi-9813-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  9 14:01:19.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 --namespace=crd-publish-openapi-7363 delete e2e-test-crd-publish-openapi-9813-crds test-cr'
Jun  9 14:01:19.700: INFO: stderr: ""
Jun  9 14:01:19.700: INFO: stdout: "e2e-test-crd-publish-openapi-9813-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  9 14:01:19.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 explain e2e-test-crd-publish-openapi-9813-crds'
Jun  9 14:01:19.994: INFO: stderr: ""
Jun  9 14:01:19.994: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9813-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:01:25.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7363" for this suite.

• [SLOW TEST:12.718 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":233,"skipped":3860,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:01:25.338: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-336
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-336
Jun  9 14:01:25.604: INFO: Found 0 stateful pods, waiting for 1
Jun  9 14:01:35.615: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jun  9 14:01:35.650: INFO: Deleting all statefulset in ns statefulset-336
Jun  9 14:01:35.665: INFO: Scaling statefulset ss to 0
Jun  9 14:02:05.722: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 14:02:05.728: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:02:05.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-336" for this suite.

• [SLOW TEST:40.439 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":234,"skipped":3863,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:02:05.778: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8555
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  9 14:02:05.988: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun  9 14:02:30.233: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.24.173.7:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8555 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 14:02:30.233: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 14:02:30.456: INFO: Found all expected endpoints: [netserver-0]
Jun  9 14:02:30.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.24.160.116:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8555 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 14:02:30.462: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 14:02:30.703: INFO: Found all expected endpoints: [netserver-1]
Jun  9 14:02:30.710: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.24.106.39:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8555 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun  9 14:02:30.710: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
Jun  9 14:02:30.967: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:02:30.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8555" for this suite.

• [SLOW TEST:25.207 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":235,"skipped":3874,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:02:30.986: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5581
STEP: Creating secret with name secret-test-3fe72585-35ad-477e-b1d1-cf5f3680e168
STEP: Creating a pod to test consume secrets
Jun  9 14:02:31.425: INFO: Waiting up to 5m0s for pod "pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144" in namespace "secrets-1994" to be "success or failure"
Jun  9 14:02:31.442: INFO: Pod "pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144": Phase="Pending", Reason="", readiness=false. Elapsed: 16.804998ms
Jun  9 14:02:33.450: INFO: Pod "pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025445143s
Jun  9 14:02:35.458: INFO: Pod "pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032964771s
STEP: Saw pod success
Jun  9 14:02:35.458: INFO: Pod "pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144" satisfied condition "success or failure"
Jun  9 14:02:35.462: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144 container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 14:02:35.499: INFO: Waiting for pod pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144 to disappear
Jun  9 14:02:35.503: INFO: Pod pod-secrets-9a06c811-e302-4185-9661-6bbe07c67144 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:02:35.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1994" for this suite.
STEP: Destroying namespace "secret-namespace-5581" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3892,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:02:35.542: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:02:35.885: INFO: Create a RollingUpdate DaemonSet
Jun  9 14:02:35.898: INFO: Check that daemon pods launch on every node of the cluster
Jun  9 14:02:35.911: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:35.922: INFO: Number of nodes with available pods: 0
Jun  9 14:02:35.922: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 14:02:36.935: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:36.944: INFO: Number of nodes with available pods: 0
Jun  9 14:02:36.944: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 14:02:37.934: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:37.940: INFO: Number of nodes with available pods: 0
Jun  9 14:02:37.940: INFO: Node worker-2jqhr-6f5dbbb884-vqc7c is running more than one daemon pod
Jun  9 14:02:38.930: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:38.935: INFO: Number of nodes with available pods: 2
Jun  9 14:02:38.935: INFO: Node worker-dfhc8-64bc8fc496-xx7cx is running more than one daemon pod
Jun  9 14:02:39.931: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:39.938: INFO: Number of nodes with available pods: 3
Jun  9 14:02:39.938: INFO: Number of running nodes: 3, number of available pods: 3
Jun  9 14:02:39.938: INFO: Update the DaemonSet to trigger a rollout
Jun  9 14:02:39.956: INFO: Updating DaemonSet daemon-set
Jun  9 14:02:46.998: INFO: Roll back the DaemonSet before rollout is complete
Jun  9 14:02:47.010: INFO: Updating DaemonSet daemon-set
Jun  9 14:02:47.010: INFO: Make sure DaemonSet rollback is complete
Jun  9 14:02:47.015: INFO: Wrong image for pod: daemon-set-6ms9s. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  9 14:02:47.015: INFO: Pod daemon-set-6ms9s is not available
Jun  9 14:02:47.024: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:48.032: INFO: Wrong image for pod: daemon-set-6ms9s. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  9 14:02:48.032: INFO: Pod daemon-set-6ms9s is not available
Jun  9 14:02:48.040: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:49.032: INFO: Wrong image for pod: daemon-set-6ms9s. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  9 14:02:49.032: INFO: Pod daemon-set-6ms9s is not available
Jun  9 14:02:49.040: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 14:02:50.051: INFO: Pod daemon-set-sb49n is not available
Jun  9 14:02:50.062: INFO: DaemonSet pods can't tolerate node master-ovo8j-6dbbb47d57-c8cf7 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8709, will wait for the garbage collector to delete the pods
Jun  9 14:02:50.168: INFO: Deleting DaemonSet.extensions daemon-set took: 35.385547ms
Jun  9 14:02:50.268: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.370118ms
Jun  9 14:02:57.179: INFO: Number of nodes with available pods: 0
Jun  9 14:02:57.179: INFO: Number of running nodes: 0, number of available pods: 0
Jun  9 14:02:57.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8709/daemonsets","resourceVersion":"41870"},"items":null}

Jun  9 14:02:57.202: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8709/pods","resourceVersion":"41870"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:02:57.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8709" for this suite.

• [SLOW TEST:21.703 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":237,"skipped":3903,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:02:57.244: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 14:02:58.610: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 14:03:00.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308178, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308178, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308178, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308178, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 14:03:03.668: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:03:03.678: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8631-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:04.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-439" for this suite.
STEP: Destroying namespace "webhook-439-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":238,"skipped":3916,"failed":0}
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:05.036: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  9 14:03:05.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-118'
Jun  9 14:03:05.460: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun  9 14:03:05.460: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Jun  9 14:03:05.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 delete jobs e2e-test-httpd-job --namespace=kubectl-118'
Jun  9 14:03:05.733: INFO: stderr: ""
Jun  9 14:03:05.733: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:05.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-118" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":239,"skipped":3916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:05.778: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-07556896-a7d1-4c34-96f6-d521017d169f
STEP: Creating a pod to test consume configMaps
Jun  9 14:03:06.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81" in namespace "configmap-5382" to be "success or failure"
Jun  9 14:03:06.107: INFO: Pod "pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81": Phase="Pending", Reason="", readiness=false. Elapsed: 13.767553ms
Jun  9 14:03:08.112: INFO: Pod "pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01879517s
Jun  9 14:03:10.121: INFO: Pod "pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027639955s
STEP: Saw pod success
Jun  9 14:03:10.121: INFO: Pod "pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81" satisfied condition "success or failure"
Jun  9 14:03:10.140: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 14:03:10.234: INFO: Waiting for pod pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81 to disappear
Jun  9 14:03:10.240: INFO: Pod pod-configmaps-fc9b0ab5-2e7e-4198-8baf-11409520ca81 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:10.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5382" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun  9 14:03:13.150: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-320 pod-service-account-09f23d1a-ebd7-479e-accc-5896f5ee3750 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun  9 14:03:13.485: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-320 pod-service-account-09f23d1a-ebd7-479e-accc-5896f5ee3750 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun  9 14:03:13.817: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-320 pod-service-account-09f23d1a-ebd7-479e-accc-5896f5ee3750 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:14.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-320" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":241,"skipped":3982,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-3353e950-f16e-4a66-89a0-9f4567c1cdbf
STEP: Creating a pod to test consume configMaps
Jun  9 14:03:14.422: INFO: Waiting up to 5m0s for pod "pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326" in namespace "configmap-3995" to be "success or failure"
Jun  9 14:03:14.434: INFO: Pod "pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326": Phase="Pending", Reason="", readiness=false. Elapsed: 12.263501ms
Jun  9 14:03:16.443: INFO: Pod "pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020975109s
STEP: Saw pod success
Jun  9 14:03:16.443: INFO: Pod "pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326" satisfied condition "success or failure"
Jun  9 14:03:16.455: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 14:03:16.493: INFO: Waiting for pod pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326 to disappear
Jun  9 14:03:16.501: INFO: Pod pod-configmaps-ffd09598-9fb2-4ab8-8cf9-b75d09af4326 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:16.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3995" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":3987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:16.531: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:16.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6239" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":4011,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:16.772: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5008.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5008.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5008.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5008.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5008.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5008.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  9 14:03:21.075: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5008/dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3: the server could not find the requested resource (get pods dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3)
Jun  9 14:03:21.080: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5008/dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3: the server could not find the requested resource (get pods dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3)
Jun  9 14:03:21.099: INFO: Unable to read jessie_udp@PodARecord from pod dns-5008/dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3: the server could not find the requested resource (get pods dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3)
Jun  9 14:03:21.104: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5008/dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3: the server could not find the requested resource (get pods dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3)
Jun  9 14:03:21.104: INFO: Lookups using dns-5008/dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun  9 14:03:26.163: INFO: DNS probes using dns-5008/dns-test-ba21f3b5-61fb-4a6a-b969-9ec3b7a6abb3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:26.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5008" for this suite.

• [SLOW TEST:9.430 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":244,"skipped":4022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:26.203: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-140ea80f-c2c6-4bc8-971e-425f2d1fa9f7
STEP: Creating a pod to test consume secrets
Jun  9 14:03:26.444: INFO: Waiting up to 5m0s for pod "pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55" in namespace "secrets-8105" to be "success or failure"
Jun  9 14:03:26.452: INFO: Pod "pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55": Phase="Pending", Reason="", readiness=false. Elapsed: 7.671845ms
Jun  9 14:03:28.459: INFO: Pod "pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015102712s
Jun  9 14:03:30.466: INFO: Pod "pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022321167s
STEP: Saw pod success
Jun  9 14:03:30.467: INFO: Pod "pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55" satisfied condition "success or failure"
Jun  9 14:03:30.471: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55 container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 14:03:30.513: INFO: Waiting for pod pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55 to disappear
Jun  9 14:03:30.517: INFO: Pod pod-secrets-8e666897-d6af-4c7e-a92d-5b0203c47e55 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:30.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8105" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":4045,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:30.536: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  9 14:03:33.764: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:33.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2243" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":4052,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:33.796: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2201
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8129
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:40.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9075" for this suite.
STEP: Destroying namespace "nsdeletetest-2201" for this suite.
Jun  9 14:03:40.491: INFO: Namespace nsdeletetest-2201 was already deleted
STEP: Destroying namespace "nsdeletetest-8129" for this suite.

• [SLOW TEST:6.706 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":247,"skipped":4068,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:40.502: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jun  9 14:03:40.713: INFO: Waiting up to 5m0s for pod "var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0" in namespace "var-expansion-5793" to be "success or failure"
Jun  9 14:03:40.718: INFO: Pod "var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.284286ms
Jun  9 14:03:42.728: INFO: Pod "var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015408574s
Jun  9 14:03:44.735: INFO: Pod "var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022288928s
STEP: Saw pod success
Jun  9 14:03:44.735: INFO: Pod "var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0" satisfied condition "success or failure"
Jun  9 14:03:44.739: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0 container dapi-container: <nil>
STEP: delete the pod
Jun  9 14:03:44.782: INFO: Waiting for pod var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0 to disappear
Jun  9 14:03:44.791: INFO: Pod var-expansion-f9ea8d7a-67da-4736-b642-3b36198e7fd0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:44.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5793" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":248,"skipped":4078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:44.811: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:49.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1298" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":4105,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:49.073: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  9 14:03:49.266: INFO: Waiting up to 5m0s for pod "pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94" in namespace "emptydir-5280" to be "success or failure"
Jun  9 14:03:49.273: INFO: Pod "pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94": Phase="Pending", Reason="", readiness=false. Elapsed: 6.889768ms
Jun  9 14:03:51.279: INFO: Pod "pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013012028s
STEP: Saw pod success
Jun  9 14:03:51.279: INFO: Pod "pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94" satisfied condition "success or failure"
Jun  9 14:03:51.283: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94 container test-container: <nil>
STEP: delete the pod
Jun  9 14:03:51.326: INFO: Waiting for pod pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94 to disappear
Jun  9 14:03:51.332: INFO: Pod pod-e9f5d3c0-53ec-40cd-9a38-19fb0c5ccf94 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:51.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5280" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":250,"skipped":4107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 14:03:51.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e" in namespace "downward-api-8764" to be "success or failure"
Jun  9 14:03:51.584: INFO: Pod "downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.250355ms
Jun  9 14:03:53.601: INFO: Pod "downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027418481s
STEP: Saw pod success
Jun  9 14:03:53.601: INFO: Pod "downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e" satisfied condition "success or failure"
Jun  9 14:03:53.619: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e container client-container: <nil>
STEP: delete the pod
Jun  9 14:03:53.653: INFO: Waiting for pod downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e to disappear
Jun  9 14:03:53.657: INFO: Pod downwardapi-volume-bd158092-b93f-43cc-b86a-3efc35a4528e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:03:53.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8764" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":251,"skipped":4202,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:03:53.673: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 14:03:54.893: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 14:03:56.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308234, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308234, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308235, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308234, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 14:03:59.944: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:04:10.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7123" for this suite.
STEP: Destroying namespace "webhook-7123-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":252,"skipped":4205,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:04:10.393: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  9 14:04:18.749: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 14:04:18.767: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  9 14:04:20.768: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 14:04:20.775: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  9 14:04:22.768: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 14:04:22.775: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  9 14:04:24.768: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 14:04:24.774: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:04:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3831" for this suite.

• [SLOW TEST:14.401 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":253,"skipped":4205,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:04:24.794: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jun  9 14:04:24.991: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-503842985 proxy --unix-socket=/tmp/kubectl-proxy-unix916359080/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:04:25.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-618" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":254,"skipped":4215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:04:25.103: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:04:51.333: INFO: Container started at 2020-06-09 14:04:27 +0000 UTC, pod became ready at 2020-06-09 14:04:49 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:04:51.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9135" for this suite.

• [SLOW TEST:26.250 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":255,"skipped":4238,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:04:51.354: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 14:04:51.568: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213" in namespace "downward-api-5062" to be "success or failure"
Jun  9 14:04:51.573: INFO: Pod "downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518903ms
Jun  9 14:04:53.599: INFO: Pod "downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030715249s
STEP: Saw pod success
Jun  9 14:04:53.599: INFO: Pod "downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213" satisfied condition "success or failure"
Jun  9 14:04:53.610: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213 container client-container: <nil>
STEP: delete the pod
Jun  9 14:04:53.686: INFO: Waiting for pod downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213 to disappear
Jun  9 14:04:53.695: INFO: Pod downwardapi-volume-8ee735e4-efa4-4075-81e1-14ed23dab213 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:04:53.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5062" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4244,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:04:53.760: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-fpzjh in namespace proxy-2741
I0609 14:04:54.030366      24 runners.go:189] Created replication controller with name: proxy-service-fpzjh, namespace: proxy-2741, replica count: 1
I0609 14:04:55.081131      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 14:04:56.081531      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:04:57.082136      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:04:58.082623      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:04:59.083170      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:05:00.083806      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:05:01.084273      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:05:02.084713      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0609 14:05:03.085418      24 runners.go:189] proxy-service-fpzjh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 14:05:03.092: INFO: setup took 9.102103685s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun  9 14:05:03.110: INFO: (0) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 16.338852ms)
Jun  9 14:05:03.120: INFO: (0) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 24.580535ms)
Jun  9 14:05:03.126: INFO: (0) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 31.835711ms)
Jun  9 14:05:03.126: INFO: (0) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 32.093281ms)
Jun  9 14:05:03.134: INFO: (0) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 39.962526ms)
Jun  9 14:05:03.134: INFO: (0) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 39.173618ms)
Jun  9 14:05:03.135: INFO: (0) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 39.998485ms)
Jun  9 14:05:03.136: INFO: (0) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 41.078519ms)
Jun  9 14:05:03.137: INFO: (0) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 42.71063ms)
Jun  9 14:05:03.138: INFO: (0) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 42.854833ms)
Jun  9 14:05:03.138: INFO: (0) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 45.546298ms)
Jun  9 14:05:03.138: INFO: (0) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 45.657606ms)
Jun  9 14:05:03.138: INFO: (0) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 45.725635ms)
Jun  9 14:05:03.139: INFO: (0) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 43.638174ms)
Jun  9 14:05:03.139: INFO: (0) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 45.270275ms)
Jun  9 14:05:03.150: INFO: (0) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 55.002381ms)
Jun  9 14:05:03.163: INFO: (1) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 12.532951ms)
Jun  9 14:05:03.192: INFO: (1) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 41.02321ms)
Jun  9 14:05:03.193: INFO: (1) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 40.703629ms)
Jun  9 14:05:03.193: INFO: (1) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 41.953479ms)
Jun  9 14:05:03.195: INFO: (1) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 43.341318ms)
Jun  9 14:05:03.196: INFO: (1) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 44.159165ms)
Jun  9 14:05:03.196: INFO: (1) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 45.220582ms)
Jun  9 14:05:03.196: INFO: (1) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 44.464111ms)
Jun  9 14:05:03.196: INFO: (1) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 44.357232ms)
Jun  9 14:05:03.198: INFO: (1) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 46.731824ms)
Jun  9 14:05:03.198: INFO: (1) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 46.844669ms)
Jun  9 14:05:03.198: INFO: (1) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 46.081665ms)
Jun  9 14:05:03.198: INFO: (1) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 46.836772ms)
Jun  9 14:05:03.200: INFO: (1) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 48.565146ms)
Jun  9 14:05:03.201: INFO: (1) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 49.690804ms)
Jun  9 14:05:03.201: INFO: (1) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 49.835092ms)
Jun  9 14:05:03.214: INFO: (2) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 13.265076ms)
Jun  9 14:05:03.219: INFO: (2) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 18.012695ms)
Jun  9 14:05:03.221: INFO: (2) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 19.732522ms)
Jun  9 14:05:03.221: INFO: (2) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 19.878638ms)
Jun  9 14:05:03.222: INFO: (2) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 20.535412ms)
Jun  9 14:05:03.223: INFO: (2) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 21.537435ms)
Jun  9 14:05:03.223: INFO: (2) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 21.836181ms)
Jun  9 14:05:03.229: INFO: (2) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 27.362335ms)
Jun  9 14:05:03.230: INFO: (2) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 28.543241ms)
Jun  9 14:05:03.236: INFO: (2) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 34.853367ms)
Jun  9 14:05:03.246: INFO: (2) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 44.536941ms)
Jun  9 14:05:03.246: INFO: (2) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 44.610048ms)
Jun  9 14:05:03.246: INFO: (2) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 44.68431ms)
Jun  9 14:05:03.246: INFO: (2) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 44.751338ms)
Jun  9 14:05:03.248: INFO: (2) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 47.025683ms)
Jun  9 14:05:03.248: INFO: (2) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 46.955965ms)
Jun  9 14:05:03.258: INFO: (3) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 9.318803ms)
Jun  9 14:05:03.258: INFO: (3) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 9.366346ms)
Jun  9 14:05:03.277: INFO: (3) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 28.093188ms)
Jun  9 14:05:03.277: INFO: (3) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 28.267189ms)
Jun  9 14:05:03.277: INFO: (3) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 28.531125ms)
Jun  9 14:05:03.278: INFO: (3) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 28.451077ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 32.004979ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 32.163996ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 32.080271ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 32.20438ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 32.111566ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 32.372612ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 32.996733ms)
Jun  9 14:05:03.281: INFO: (3) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 32.381552ms)
Jun  9 14:05:03.282: INFO: (3) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 32.581195ms)
Jun  9 14:05:03.282: INFO: (3) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 32.602453ms)
Jun  9 14:05:03.345: INFO: (4) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 62.324827ms)
Jun  9 14:05:03.346: INFO: (4) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 63.117767ms)
Jun  9 14:05:03.346: INFO: (4) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 63.403402ms)
Jun  9 14:05:03.354: INFO: (4) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 71.155502ms)
Jun  9 14:05:03.356: INFO: (4) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 72.84665ms)
Jun  9 14:05:03.356: INFO: (4) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 73.124947ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 76.230649ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 75.396637ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 75.565234ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 75.664364ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 75.351023ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 75.421457ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 75.540032ms)
Jun  9 14:05:03.358: INFO: (4) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 75.490686ms)
Jun  9 14:05:03.359: INFO: (4) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 75.888882ms)
Jun  9 14:05:03.364: INFO: (4) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 81.477464ms)
Jun  9 14:05:03.379: INFO: (5) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 15.421826ms)
Jun  9 14:05:03.383: INFO: (5) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 18.935804ms)
Jun  9 14:05:03.383: INFO: (5) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 18.98096ms)
Jun  9 14:05:03.387: INFO: (5) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 22.599924ms)
Jun  9 14:05:03.394: INFO: (5) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 30.158934ms)
Jun  9 14:05:03.397: INFO: (5) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 32.044554ms)
Jun  9 14:05:03.397: INFO: (5) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 32.869ms)
Jun  9 14:05:03.397: INFO: (5) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 32.739231ms)
Jun  9 14:05:03.399: INFO: (5) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 34.268712ms)
Jun  9 14:05:03.403: INFO: (5) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 38.696153ms)
Jun  9 14:05:03.406: INFO: (5) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 41.219515ms)
Jun  9 14:05:03.409: INFO: (5) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 44.68933ms)
Jun  9 14:05:03.411: INFO: (5) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 46.229479ms)
Jun  9 14:05:03.411: INFO: (5) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 46.101613ms)
Jun  9 14:05:03.411: INFO: (5) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 46.503551ms)
Jun  9 14:05:03.412: INFO: (5) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 47.729432ms)
Jun  9 14:05:03.447: INFO: (6) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 34.302766ms)
Jun  9 14:05:03.447: INFO: (6) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 34.743729ms)
Jun  9 14:05:03.472: INFO: (6) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 58.595188ms)
Jun  9 14:05:03.478: INFO: (6) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 64.463487ms)
Jun  9 14:05:03.479: INFO: (6) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 65.450805ms)
Jun  9 14:05:03.480: INFO: (6) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 66.560558ms)
Jun  9 14:05:03.484: INFO: (6) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 70.709949ms)
Jun  9 14:05:03.490: INFO: (6) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 76.930415ms)
Jun  9 14:05:03.490: INFO: (6) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 76.478044ms)
Jun  9 14:05:03.490: INFO: (6) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 76.982524ms)
Jun  9 14:05:03.490: INFO: (6) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 76.786309ms)
Jun  9 14:05:03.492: INFO: (6) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 78.485073ms)
Jun  9 14:05:03.493: INFO: (6) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 79.097819ms)
Jun  9 14:05:03.494: INFO: (6) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 80.900022ms)
Jun  9 14:05:03.494: INFO: (6) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 80.905786ms)
Jun  9 14:05:03.495: INFO: (6) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 81.800453ms)
Jun  9 14:05:03.527: INFO: (7) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 30.829699ms)
Jun  9 14:05:03.575: INFO: (7) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 77.444423ms)
Jun  9 14:05:03.581: INFO: (7) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 83.319546ms)
Jun  9 14:05:03.591: INFO: (7) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 94.467667ms)
Jun  9 14:05:03.643: INFO: (7) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 145.907123ms)
Jun  9 14:05:03.645: INFO: (7) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 149.72656ms)
Jun  9 14:05:03.645: INFO: (7) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 149.477404ms)
Jun  9 14:05:03.646: INFO: (7) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 150.322251ms)
Jun  9 14:05:03.650: INFO: (7) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 153.001124ms)
Jun  9 14:05:03.651: INFO: (7) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 155.121763ms)
Jun  9 14:05:03.652: INFO: (7) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 155.011884ms)
Jun  9 14:05:03.653: INFO: (7) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 155.678983ms)
Jun  9 14:05:03.653: INFO: (7) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 155.425806ms)
Jun  9 14:05:03.655: INFO: (7) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 157.712249ms)
Jun  9 14:05:03.666: INFO: (7) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 170.900831ms)
Jun  9 14:05:03.666: INFO: (7) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 169.315307ms)
Jun  9 14:05:03.698: INFO: (8) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 30.993036ms)
Jun  9 14:05:03.707: INFO: (8) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 39.79215ms)
Jun  9 14:05:03.734: INFO: (8) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 65.921099ms)
Jun  9 14:05:03.738: INFO: (8) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 70.637516ms)
Jun  9 14:05:03.739: INFO: (8) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 71.929956ms)
Jun  9 14:05:03.739: INFO: (8) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 71.61182ms)
Jun  9 14:05:03.739: INFO: (8) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 71.547658ms)
Jun  9 14:05:03.739: INFO: (8) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 71.439804ms)
Jun  9 14:05:03.739: INFO: (8) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 71.719055ms)
Jun  9 14:05:03.740: INFO: (8) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 72.477371ms)
Jun  9 14:05:03.742: INFO: (8) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 74.529646ms)
Jun  9 14:05:03.743: INFO: (8) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 76.147989ms)
Jun  9 14:05:03.744: INFO: (8) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 76.616148ms)
Jun  9 14:05:03.744: INFO: (8) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 76.896329ms)
Jun  9 14:05:03.744: INFO: (8) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 77.173413ms)
Jun  9 14:05:03.745: INFO: (8) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 77.292808ms)
Jun  9 14:05:03.765: INFO: (9) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 19.753553ms)
Jun  9 14:05:03.765: INFO: (9) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 20.737622ms)
Jun  9 14:05:03.765: INFO: (9) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 20.272438ms)
Jun  9 14:05:03.781: INFO: (9) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 34.386245ms)
Jun  9 14:05:03.781: INFO: (9) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 36.075826ms)
Jun  9 14:05:03.781: INFO: (9) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 35.957798ms)
Jun  9 14:05:03.784: INFO: (9) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 38.408334ms)
Jun  9 14:05:03.784: INFO: (9) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 36.992055ms)
Jun  9 14:05:03.787: INFO: (9) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 40.894348ms)
Jun  9 14:05:03.788: INFO: (9) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 41.885339ms)
Jun  9 14:05:03.788: INFO: (9) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 41.789164ms)
Jun  9 14:05:03.790: INFO: (9) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 43.163876ms)
Jun  9 14:05:03.793: INFO: (9) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 46.95134ms)
Jun  9 14:05:03.794: INFO: (9) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 47.657236ms)
Jun  9 14:05:03.794: INFO: (9) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 48.25076ms)
Jun  9 14:05:03.795: INFO: (9) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 48.893566ms)
Jun  9 14:05:03.821: INFO: (10) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 24.908367ms)
Jun  9 14:05:03.821: INFO: (10) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 25.75783ms)
Jun  9 14:05:03.822: INFO: (10) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 26.501398ms)
Jun  9 14:05:03.822: INFO: (10) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 26.440579ms)
Jun  9 14:05:03.822: INFO: (10) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 26.522105ms)
Jun  9 14:05:03.822: INFO: (10) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 27.028252ms)
Jun  9 14:05:03.823: INFO: (10) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 27.611136ms)
Jun  9 14:05:03.823: INFO: (10) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 27.918742ms)
Jun  9 14:05:03.824: INFO: (10) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 27.931744ms)
Jun  9 14:05:03.824: INFO: (10) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 28.804469ms)
Jun  9 14:05:03.824: INFO: (10) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 28.503618ms)
Jun  9 14:05:03.830: INFO: (10) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 34.1815ms)
Jun  9 14:05:03.830: INFO: (10) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 34.553605ms)
Jun  9 14:05:03.831: INFO: (10) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 35.341308ms)
Jun  9 14:05:03.835: INFO: (10) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 39.134455ms)
Jun  9 14:05:03.840: INFO: (10) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 44.053832ms)
Jun  9 14:05:03.864: INFO: (11) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 24.119549ms)
Jun  9 14:05:03.868: INFO: (11) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 28.057531ms)
Jun  9 14:05:03.869: INFO: (11) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 28.804331ms)
Jun  9 14:05:03.869: INFO: (11) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 29.324655ms)
Jun  9 14:05:03.870: INFO: (11) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 29.531532ms)
Jun  9 14:05:03.871: INFO: (11) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 30.860897ms)
Jun  9 14:05:03.871: INFO: (11) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 30.754532ms)
Jun  9 14:05:03.871: INFO: (11) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 30.795468ms)
Jun  9 14:05:03.875: INFO: (11) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 34.807807ms)
Jun  9 14:05:03.875: INFO: (11) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 34.876388ms)
Jun  9 14:05:03.876: INFO: (11) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 36.05143ms)
Jun  9 14:05:03.876: INFO: (11) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 35.949711ms)
Jun  9 14:05:03.876: INFO: (11) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 36.169369ms)
Jun  9 14:05:03.876: INFO: (11) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 36.191891ms)
Jun  9 14:05:03.878: INFO: (11) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 38.429028ms)
Jun  9 14:05:03.880: INFO: (11) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 39.399646ms)
Jun  9 14:05:03.895: INFO: (12) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 14.572764ms)
Jun  9 14:05:03.899: INFO: (12) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 18.470898ms)
Jun  9 14:05:03.899: INFO: (12) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 18.168868ms)
Jun  9 14:05:03.899: INFO: (12) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 18.740883ms)
Jun  9 14:05:03.899: INFO: (12) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 18.177074ms)
Jun  9 14:05:03.899: INFO: (12) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 18.332127ms)
Jun  9 14:05:03.903: INFO: (12) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 23.044911ms)
Jun  9 14:05:03.903: INFO: (12) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 22.522501ms)
Jun  9 14:05:03.908: INFO: (12) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 28.180488ms)
Jun  9 14:05:03.908: INFO: (12) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 27.828585ms)
Jun  9 14:05:03.909: INFO: (12) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 28.720249ms)
Jun  9 14:05:03.909: INFO: (12) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 28.407976ms)
Jun  9 14:05:03.911: INFO: (12) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 30.923335ms)
Jun  9 14:05:03.912: INFO: (12) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 32.649712ms)
Jun  9 14:05:03.913: INFO: (12) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 32.925704ms)
Jun  9 14:05:03.916: INFO: (12) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 35.621086ms)
Jun  9 14:05:03.930: INFO: (13) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 14.309251ms)
Jun  9 14:05:03.931: INFO: (13) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 15.04273ms)
Jun  9 14:05:03.931: INFO: (13) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 15.035537ms)
Jun  9 14:05:03.936: INFO: (13) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 19.51643ms)
Jun  9 14:05:03.937: INFO: (13) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 20.733047ms)
Jun  9 14:05:03.938: INFO: (13) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 21.697857ms)
Jun  9 14:05:03.938: INFO: (13) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 21.709009ms)
Jun  9 14:05:03.938: INFO: (13) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 21.868052ms)
Jun  9 14:05:03.938: INFO: (13) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 21.771426ms)
Jun  9 14:05:03.938: INFO: (13) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 22.071891ms)
Jun  9 14:05:03.939: INFO: (13) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 22.347081ms)
Jun  9 14:05:03.939: INFO: (13) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 22.962814ms)
Jun  9 14:05:03.940: INFO: (13) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 23.537698ms)
Jun  9 14:05:03.943: INFO: (13) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 26.605797ms)
Jun  9 14:05:03.944: INFO: (13) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 27.548426ms)
Jun  9 14:05:03.944: INFO: (13) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 27.973725ms)
Jun  9 14:05:03.962: INFO: (14) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 17.29002ms)
Jun  9 14:05:03.962: INFO: (14) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 16.938816ms)
Jun  9 14:05:03.967: INFO: (14) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 22.431655ms)
Jun  9 14:05:03.968: INFO: (14) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 23.58427ms)
Jun  9 14:05:03.969: INFO: (14) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 24.064187ms)
Jun  9 14:05:03.970: INFO: (14) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 24.7843ms)
Jun  9 14:05:03.971: INFO: (14) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 25.813685ms)
Jun  9 14:05:03.971: INFO: (14) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 26.294513ms)
Jun  9 14:05:03.972: INFO: (14) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 26.915433ms)
Jun  9 14:05:03.972: INFO: (14) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 27.389652ms)
Jun  9 14:05:03.973: INFO: (14) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 27.644906ms)
Jun  9 14:05:03.981: INFO: (14) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 36.126209ms)
Jun  9 14:05:03.983: INFO: (14) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 38.219055ms)
Jun  9 14:05:03.983: INFO: (14) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 38.273162ms)
Jun  9 14:05:03.983: INFO: (14) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 38.392826ms)
Jun  9 14:05:03.983: INFO: (14) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 38.499428ms)
Jun  9 14:05:04.004: INFO: (15) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 20.579074ms)
Jun  9 14:05:04.007: INFO: (15) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 23.320943ms)
Jun  9 14:05:04.010: INFO: (15) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 26.176058ms)
Jun  9 14:05:04.012: INFO: (15) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 28.150978ms)
Jun  9 14:05:04.012: INFO: (15) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 27.985076ms)
Jun  9 14:05:04.012: INFO: (15) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 27.960684ms)
Jun  9 14:05:04.012: INFO: (15) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 28.176182ms)
Jun  9 14:05:04.012: INFO: (15) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 28.226393ms)
Jun  9 14:05:04.014: INFO: (15) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 30.83187ms)
Jun  9 14:05:04.014: INFO: (15) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 30.127039ms)
Jun  9 14:05:04.014: INFO: (15) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 30.114359ms)
Jun  9 14:05:04.015: INFO: (15) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 31.25939ms)
Jun  9 14:05:04.018: INFO: (15) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 34.330439ms)
Jun  9 14:05:04.018: INFO: (15) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 34.561468ms)
Jun  9 14:05:04.019: INFO: (15) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 35.135194ms)
Jun  9 14:05:04.019: INFO: (15) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 35.267839ms)
Jun  9 14:05:04.039: INFO: (16) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 18.295503ms)
Jun  9 14:05:04.039: INFO: (16) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 19.006561ms)
Jun  9 14:05:04.039: INFO: (16) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 18.960388ms)
Jun  9 14:05:04.040: INFO: (16) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 20.100572ms)
Jun  9 14:05:04.042: INFO: (16) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 22.131045ms)
Jun  9 14:05:04.044: INFO: (16) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 23.998237ms)
Jun  9 14:05:04.044: INFO: (16) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 24.479693ms)
Jun  9 14:05:04.044: INFO: (16) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 23.344644ms)
Jun  9 14:05:04.044: INFO: (16) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 23.838027ms)
Jun  9 14:05:04.044: INFO: (16) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 21.895868ms)
Jun  9 14:05:04.045: INFO: (16) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 25.045916ms)
Jun  9 14:05:04.045: INFO: (16) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 25.482698ms)
Jun  9 14:05:04.045: INFO: (16) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 25.298348ms)
Jun  9 14:05:04.046: INFO: (16) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 26.310254ms)
Jun  9 14:05:04.049: INFO: (16) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 29.377549ms)
Jun  9 14:05:04.049: INFO: (16) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 26.447282ms)
Jun  9 14:05:04.065: INFO: (17) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 14.857309ms)
Jun  9 14:05:04.065: INFO: (17) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 15.436825ms)
Jun  9 14:05:04.065: INFO: (17) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 14.704024ms)
Jun  9 14:05:04.065: INFO: (17) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 15.088834ms)
Jun  9 14:05:04.065: INFO: (17) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 15.931866ms)
Jun  9 14:05:04.065: INFO: (17) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 15.280889ms)
Jun  9 14:05:04.071: INFO: (17) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 21.986775ms)
Jun  9 14:05:04.072: INFO: (17) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 21.14364ms)
Jun  9 14:05:04.072: INFO: (17) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 21.527174ms)
Jun  9 14:05:04.072: INFO: (17) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 21.450848ms)
Jun  9 14:05:04.073: INFO: (17) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 23.708236ms)
Jun  9 14:05:04.084: INFO: (17) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 33.771202ms)
Jun  9 14:05:04.085: INFO: (17) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 33.92562ms)
Jun  9 14:05:04.085: INFO: (17) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 35.321531ms)
Jun  9 14:05:04.086: INFO: (17) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 35.425414ms)
Jun  9 14:05:04.087: INFO: (17) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 36.633123ms)
Jun  9 14:05:04.114: INFO: (18) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 25.986449ms)
Jun  9 14:05:04.115: INFO: (18) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 27.271363ms)
Jun  9 14:05:04.115: INFO: (18) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 27.861676ms)
Jun  9 14:05:04.117: INFO: (18) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 29.447549ms)
Jun  9 14:05:04.119: INFO: (18) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 31.196801ms)
Jun  9 14:05:04.119: INFO: (18) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 31.58664ms)
Jun  9 14:05:04.126: INFO: (18) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 37.983302ms)
Jun  9 14:05:04.127: INFO: (18) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 38.974261ms)
Jun  9 14:05:04.127: INFO: (18) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 39.907527ms)
Jun  9 14:05:04.127: INFO: (18) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 39.304172ms)
Jun  9 14:05:04.127: INFO: (18) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 39.407632ms)
Jun  9 14:05:04.128: INFO: (18) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 40.162526ms)
Jun  9 14:05:04.128: INFO: (18) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 40.04928ms)
Jun  9 14:05:04.128: INFO: (18) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 40.783045ms)
Jun  9 14:05:04.129: INFO: (18) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 41.118389ms)
Jun  9 14:05:04.129: INFO: (18) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 41.829187ms)
Jun  9 14:05:04.151: INFO: (19) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname2/proxy/: bar (200; 19.082719ms)
Jun  9 14:05:04.155: INFO: (19) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname1/proxy/: foo (200; 26.017426ms)
Jun  9 14:05:04.155: INFO: (19) /api/v1/namespaces/proxy-2741/services/proxy-service-fpzjh:portname2/proxy/: bar (200; 25.999866ms)
Jun  9 14:05:04.156: INFO: (19) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h/proxy/rewriteme">test</a> (200; 24.316932ms)
Jun  9 14:05:04.158: INFO: (19) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:443/proxy/tlsrewritem... (200; 28.219848ms)
Jun  9 14:05:04.161: INFO: (19) /api/v1/namespaces/proxy-2741/services/http:proxy-service-fpzjh:portname1/proxy/: foo (200; 30.010014ms)
Jun  9 14:05:04.161: INFO: (19) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 29.887638ms)
Jun  9 14:05:04.162: INFO: (19) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 30.970876ms)
Jun  9 14:05:04.162: INFO: (19) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:160/proxy/: foo (200; 32.262465ms)
Jun  9 14:05:04.163: INFO: (19) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:162/proxy/: bar (200; 32.986591ms)
Jun  9 14:05:04.163: INFO: (19) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:462/proxy/: tls qux (200; 31.72904ms)
Jun  9 14:05:04.164: INFO: (19) /api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/http:proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">... (200; 33.213021ms)
Jun  9 14:05:04.164: INFO: (19) /api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/: <a href="/api/v1/namespaces/proxy-2741/pods/proxy-service-fpzjh-5c46h:1080/proxy/rewriteme">test<... (200; 34.316183ms)
Jun  9 14:05:04.165: INFO: (19) /api/v1/namespaces/proxy-2741/pods/https:proxy-service-fpzjh-5c46h:460/proxy/: tls baz (200; 34.895829ms)
Jun  9 14:05:04.166: INFO: (19) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname2/proxy/: tls qux (200; 34.44501ms)
Jun  9 14:05:04.168: INFO: (19) /api/v1/namespaces/proxy-2741/services/https:proxy-service-fpzjh:tlsportname1/proxy/: tls baz (200; 36.695449ms)
STEP: deleting ReplicationController proxy-service-fpzjh in namespace proxy-2741, will wait for the garbage collector to delete the pods
Jun  9 14:05:04.234: INFO: Deleting ReplicationController proxy-service-fpzjh took: 10.403486ms
Jun  9 14:05:04.334: INFO: Terminating ReplicationController proxy-service-fpzjh pods took: 100.44528ms
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:05:16.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2741" for this suite.

• [SLOW TEST:22.791 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":257,"skipped":4259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:05:16.552: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  9 14:05:16.808: INFO: Waiting up to 5m0s for pod "pod-40044944-44a0-46f9-9c36-fea62825fc1b" in namespace "emptydir-4636" to be "success or failure"
Jun  9 14:05:16.813: INFO: Pod "pod-40044944-44a0-46f9-9c36-fea62825fc1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300361ms
Jun  9 14:05:18.823: INFO: Pod "pod-40044944-44a0-46f9-9c36-fea62825fc1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014554733s
STEP: Saw pod success
Jun  9 14:05:18.823: INFO: Pod "pod-40044944-44a0-46f9-9c36-fea62825fc1b" satisfied condition "success or failure"
Jun  9 14:05:18.828: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-40044944-44a0-46f9-9c36-fea62825fc1b container test-container: <nil>
STEP: delete the pod
Jun  9 14:05:18.855: INFO: Waiting for pod pod-40044944-44a0-46f9-9c36-fea62825fc1b to disappear
Jun  9 14:05:18.861: INFO: Pod pod-40044944-44a0-46f9-9c36-fea62825fc1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:05:18.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4636" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4297,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:05:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 14:05:19.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed" in namespace "projected-6496" to be "success or failure"
Jun  9 14:05:19.108: INFO: Pod "downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098382ms
Jun  9 14:05:21.115: INFO: Pod "downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012500096s
STEP: Saw pod success
Jun  9 14:05:21.115: INFO: Pod "downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed" satisfied condition "success or failure"
Jun  9 14:05:21.119: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed container client-container: <nil>
STEP: delete the pod
Jun  9 14:05:21.158: INFO: Waiting for pod downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed to disappear
Jun  9 14:05:21.166: INFO: Pod downwardapi-volume-3dbfde7d-73ac-4a3c-a510-a6cbdb1f1bed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:05:21.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6496" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4311,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:05:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-e6d7b7a3-a4c3-4590-aa84-d4be741fc577
STEP: Creating a pod to test consume configMaps
Jun  9 14:05:21.420: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4" in namespace "configmap-3808" to be "success or failure"
Jun  9 14:05:21.434: INFO: Pod "pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.978385ms
Jun  9 14:05:23.441: INFO: Pod "pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020246218s
STEP: Saw pod success
Jun  9 14:05:23.441: INFO: Pod "pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4" satisfied condition "success or failure"
Jun  9 14:05:23.446: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 14:05:23.575: INFO: Waiting for pod pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4 to disappear
Jun  9 14:05:23.617: INFO: Pod pod-configmaps-c0402f78-9f01-420b-ae4e-0dfecdd329f4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:05:23.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3808" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4317,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:05:23.649: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3841
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:05:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Creating first CR 
Jun  9 14:05:24.541: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-09T14:05:24Z generation:1 name:name1 resourceVersion:43296 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:147c5068-210b-4456-a1fc-929a387546ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun  9 14:05:34.549: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-09T14:05:34Z generation:1 name:name2 resourceVersion:43353 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:008fae99-cf14-4763-b1ea-6b742839945e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun  9 14:05:44.565: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-09T14:05:24Z generation:2 name:name1 resourceVersion:43378 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:147c5068-210b-4456-a1fc-929a387546ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun  9 14:05:54.575: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-09T14:05:34Z generation:2 name:name2 resourceVersion:43406 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:008fae99-cf14-4763-b1ea-6b742839945e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun  9 14:06:04.588: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-09T14:05:24Z generation:2 name:name1 resourceVersion:43433 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:147c5068-210b-4456-a1fc-929a387546ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun  9 14:06:14.600: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-09T14:05:34Z generation:2 name:name2 resourceVersion:43461 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:008fae99-cf14-4763-b1ea-6b742839945e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:06:25.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3841" for this suite.

• [SLOW TEST:61.501 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":261,"skipped":4323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:06:25.161: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5599
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:06:25.361: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:06:31.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5599" for this suite.

• [SLOW TEST:6.693 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":262,"skipped":4352,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:06:31.855: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  9 14:06:34.661: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:06:34.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2769" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":263,"skipped":4369,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:06:34.714: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-00d0ee6c-bdc2-4baf-b62e-68e4f793700e
STEP: Creating a pod to test consume secrets
Jun  9 14:06:34.942: INFO: Waiting up to 5m0s for pod "pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52" in namespace "secrets-7252" to be "success or failure"
Jun  9 14:06:34.954: INFO: Pod "pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52": Phase="Pending", Reason="", readiness=false. Elapsed: 11.183177ms
Jun  9 14:06:36.963: INFO: Pod "pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020550247s
Jun  9 14:06:38.979: INFO: Pod "pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035942748s
STEP: Saw pod success
Jun  9 14:06:38.979: INFO: Pod "pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52" satisfied condition "success or failure"
Jun  9 14:06:38.987: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52 container secret-volume-test: <nil>
STEP: delete the pod
Jun  9 14:06:39.025: INFO: Waiting for pod pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52 to disappear
Jun  9 14:06:39.030: INFO: Pod pod-secrets-e779c129-f908-4006-8b64-2031e0b65d52 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:06:39.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7252" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":264,"skipped":4381,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:06:39.054: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jun  9 14:06:43.871: INFO: Successfully updated pod "labelsupdateb67fc0a2-4411-4972-9451-9c3f5f8926cb"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:06:45.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3616" for this suite.

• [SLOW TEST:6.888 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4386,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:06:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jun  9 14:06:46.142: INFO: namespace kubectl-8028
Jun  9 14:06:46.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 create -f - --namespace=kubectl-8028'
Jun  9 14:06:46.543: INFO: stderr: ""
Jun  9 14:06:46.543: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jun  9 14:06:47.550: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 14:06:47.550: INFO: Found 0 / 1
Jun  9 14:06:48.554: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 14:06:48.555: INFO: Found 0 / 1
Jun  9 14:06:49.553: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 14:06:49.553: INFO: Found 1 / 1
Jun  9 14:06:49.553: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  9 14:06:49.557: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 14:06:49.557: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  9 14:06:49.557: INFO: wait on agnhost-master startup in kubectl-8028 
Jun  9 14:06:49.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 logs agnhost-master-rtqlx agnhost-master --namespace=kubectl-8028'
Jun  9 14:06:49.691: INFO: stderr: ""
Jun  9 14:06:49.691: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun  9 14:06:49.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8028'
Jun  9 14:06:49.854: INFO: stderr: ""
Jun  9 14:06:49.854: INFO: stdout: "service/rm2 exposed\n"
Jun  9 14:06:49.873: INFO: Service rm2 in namespace kubectl-8028 found.
STEP: exposing service
Jun  9 14:06:51.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-503842985 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8028'
Jun  9 14:06:52.051: INFO: stderr: ""
Jun  9 14:06:52.051: INFO: stdout: "service/rm3 exposed\n"
Jun  9 14:06:52.063: INFO: Service rm3 in namespace kubectl-8028 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:06:54.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8028" for this suite.

• [SLOW TEST:8.150 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":266,"skipped":4398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:06:54.093: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 14:06:55.298: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 14:06:57.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308415, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308415, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308415, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308415, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 14:07:00.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:12.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1619" for this suite.
STEP: Destroying namespace "webhook-1619-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":267,"skipped":4423,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:12.773: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-10ce68a9-2a80-412f-8f6c-ded350097ba9
STEP: Creating a pod to test consume secrets
Jun  9 14:07:13.014: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d" in namespace "projected-1052" to be "success or failure"
Jun  9 14:07:13.020: INFO: Pod "pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.468687ms
Jun  9 14:07:15.027: INFO: Pod "pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012827949s
Jun  9 14:07:17.036: INFO: Pod "pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021962501s
STEP: Saw pod success
Jun  9 14:07:17.037: INFO: Pod "pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d" satisfied condition "success or failure"
Jun  9 14:07:17.044: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  9 14:07:17.114: INFO: Waiting for pod pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d to disappear
Jun  9 14:07:17.118: INFO: Pod pod-projected-secrets-8b8387a1-e143-48b6-a3ca-7e14f1df076d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:17.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1052" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":268,"skipped":4439,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:17.143: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:07:17.347: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ae66e002-c4f7-4bb3-9d69-82a2092119fe" in namespace "security-context-test-9959" to be "success or failure"
Jun  9 14:07:17.352: INFO: Pod "busybox-user-65534-ae66e002-c4f7-4bb3-9d69-82a2092119fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516999ms
Jun  9 14:07:19.357: INFO: Pod "busybox-user-65534-ae66e002-c4f7-4bb3-9d69-82a2092119fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00992124s
Jun  9 14:07:21.366: INFO: Pod "busybox-user-65534-ae66e002-c4f7-4bb3-9d69-82a2092119fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019120539s
Jun  9 14:07:21.367: INFO: Pod "busybox-user-65534-ae66e002-c4f7-4bb3-9d69-82a2092119fe" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:21.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9959" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":269,"skipped":4446,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:21.389: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-844d14d5-2a4a-4481-9db1-87356d48bca8
STEP: Creating a pod to test consume configMaps
Jun  9 14:07:21.626: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef" in namespace "configmap-8751" to be "success or failure"
Jun  9 14:07:21.643: INFO: Pod "pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef": Phase="Pending", Reason="", readiness=false. Elapsed: 16.278385ms
Jun  9 14:07:23.651: INFO: Pod "pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02455687s
Jun  9 14:07:25.661: INFO: Pod "pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034871346s
STEP: Saw pod success
Jun  9 14:07:25.661: INFO: Pod "pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef" satisfied condition "success or failure"
Jun  9 14:07:25.666: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 14:07:25.704: INFO: Waiting for pod pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef to disappear
Jun  9 14:07:25.709: INFO: Pod pod-configmaps-4f021412-7ff4-4141-9e17-12e1f46e32ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8751" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":270,"skipped":4453,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:25.723: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jun  9 14:07:25.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4" in namespace "projected-5878" to be "success or failure"
Jun  9 14:07:25.947: INFO: Pod "downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.810779ms
Jun  9 14:07:27.952: INFO: Pod "downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019223109s
STEP: Saw pod success
Jun  9 14:07:27.952: INFO: Pod "downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4" satisfied condition "success or failure"
Jun  9 14:07:27.956: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4 container client-container: <nil>
STEP: delete the pod
Jun  9 14:07:27.982: INFO: Waiting for pod downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4 to disappear
Jun  9 14:07:27.987: INFO: Pod downwardapi-volume-31123734-d7e1-41a0-b3ae-04ac010fa8b4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:27.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5878" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":271,"skipped":4453,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:28.006: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6835
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jun  9 14:07:28.197: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 14:07:28.241: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 14:07:28.246: INFO: 
Logging pods the kubelet thinks is on node worker-2jqhr-6f5dbbb884-vqc7c before test
Jun  9 14:07:28.269: INFO: calico-node-vhcbj from kube-system started at 2020-06-09 11:42:16 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.269: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 14:07:28.269: INFO: node-exporter-nsmh9 from kube-system started at 2020-06-09 11:45:08 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.269: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 14:07:28.269: INFO: nginx-ingress-controller-59b5c95c6d-cb4kl from kube-system started at 2020-06-09 11:45:21 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 14:07:28.270: INFO: kube-proxy-69gzh from kube-system started at 2020-06-09 11:43:27 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 14:07:28.270: INFO: chart-operator-5b8b4bcc75-wn4xh from giantswarm started at 2020-06-09 11:44:47 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container chart-operator ready: true, restart count 0
Jun  9 14:07:28.270: INFO: cert-exporter-gkbjk from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 14:07:28.270: INFO: tiller-deploy-684c6b545b-4w7nq from giantswarm started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container tiller ready: true, restart count 0
Jun  9 14:07:28.270: INFO: coredns-6d56c484c-7fkzm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container coredns ready: true, restart count 0
Jun  9 14:07:28.270: INFO: coredns-6d56c484c-txwjd from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container coredns ready: true, restart count 0
Jun  9 14:07:28.270: INFO: metrics-server-66df9f5b56-bvwdm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container metrics-server ready: true, restart count 0
Jun  9 14:07:28.270: INFO: net-exporter-4n4fk from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 14:07:28.270: INFO: docker-mem-limit-startup-script-clstp from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 14:07:28.270: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-sd6nl from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 14:07:28.270: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  9 14:07:28.270: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 14:07:28.270: INFO: 
Logging pods the kubelet thinks is on node worker-dfhc8-64bc8fc496-xx7cx before test
Jun  9 14:07:28.293: INFO: kube-proxy-tg6rp from kube-system started at 2020-06-09 11:43:24 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.293: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 14:07:28.293: INFO: nginx-ingress-controller-59b5c95c6d-8z4jm from kube-system started at 2020-06-09 12:54:57 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.293: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun  9 14:07:28.293: INFO: coredns-6d56c484c-s5fsw from kube-system started at 2020-06-09 11:44:27 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container coredns ready: true, restart count 0
Jun  9 14:07:28.294: INFO: net-exporter-wvnc9 from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 14:07:28.294: INFO: calico-node-7t79n from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 14:07:28.294: INFO: cert-exporter-v64nm from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 14:07:28.294: INFO: node-exporter-k9z45 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 14:07:28.294: INFO: sonobuoy from sonobuoy started at 2020-06-09 12:47:13 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 14:07:28.294: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-p5v44 from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  9 14:07:28.294: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 14:07:28.294: INFO: kube-state-metrics-6d998ffd8b-wtr6p from kube-system started at 2020-06-09 11:44:31 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  9 14:07:28.294: INFO: docker-mem-limit-startup-script-9x4w8 from kube-system started at 2020-06-09 12:01:31 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 14:07:28.294: INFO: sonobuoy-e2e-job-fb32098c60e64727 from sonobuoy started at 2020-06-09 12:47:22 +0000 UTC (2 container statuses recorded)
Jun  9 14:07:28.294: INFO: 	Container e2e ready: true, restart count 0
Jun  9 14:07:28.294: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 14:07:28.294: INFO: 
Logging pods the kubelet thinks is on node worker-k8xcg-8bbfd5b68-w4htb before test
Jun  9 14:07:28.307: INFO: docker-mem-limit-startup-script-7t5zg from kube-system started at 2020-06-09 13:32:06 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.307: INFO: 	Container startup-script ready: true, restart count 0
Jun  9 14:07:28.307: INFO: cert-exporter-p2wws from kube-system started at 2020-06-09 11:44:28 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.307: INFO: 	Container cert-exporter ready: true, restart count 0
Jun  9 14:07:28.307: INFO: net-exporter-hppdh from kube-system started at 2020-06-09 11:45:04 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.307: INFO: 	Container net-exporter ready: true, restart count 0
Jun  9 14:07:28.307: INFO: calico-node-zqtv2 from kube-system started at 2020-06-09 11:42:17 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.307: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 14:07:28.307: INFO: node-exporter-mbg29 from kube-system started at 2020-06-09 11:45:07 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.307: INFO: 	Container node-exporter ready: true, restart count 0
Jun  9 14:07:28.307: INFO: sonobuoy-systemd-logs-daemon-set-b6f581f679a54102-lbr8h from sonobuoy started at 2020-06-09 12:47:23 +0000 UTC (2 container statuses recorded)
Jun  9 14:07:28.307: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun  9 14:07:28.307: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 14:07:28.307: INFO: kube-proxy-kfmds from kube-system started at 2020-06-09 11:43:19 +0000 UTC (1 container statuses recorded)
Jun  9 14:07:28.308: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1616e54f9901c3c4], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:29.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6835" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":272,"skipped":4469,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:29.389: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jun  9 14:07:29.627: INFO: Waiting up to 5m0s for pod "client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf" in namespace "containers-6508" to be "success or failure"
Jun  9 14:07:29.634: INFO: Pod "client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.332186ms
Jun  9 14:07:31.650: INFO: Pod "client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022361574s
Jun  9 14:07:33.658: INFO: Pod "client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030700291s
STEP: Saw pod success
Jun  9 14:07:33.659: INFO: Pod "client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf" satisfied condition "success or failure"
Jun  9 14:07:33.665: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf container test-container: <nil>
STEP: delete the pod
Jun  9 14:07:33.736: INFO: Waiting for pod client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf to disappear
Jun  9 14:07:33.743: INFO: Pod client-containers-d7ecb941-8b29-41d3-8e9c-e89400e4ddbf no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:33.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6508" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4474,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:33.772: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-87d0cc12-fab0-49c2-9ace-673d35f99060
STEP: Creating a pod to test consume configMaps
Jun  9 14:07:34.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1" in namespace "configmap-4462" to be "success or failure"
Jun  9 14:07:34.023: INFO: Pod "pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.290691ms
Jun  9 14:07:36.029: INFO: Pod "pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009679588s
Jun  9 14:07:38.044: INFO: Pod "pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024440419s
STEP: Saw pod success
Jun  9 14:07:38.044: INFO: Pod "pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1" satisfied condition "success or failure"
Jun  9 14:07:38.048: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 14:07:38.103: INFO: Waiting for pod pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1 to disappear
Jun  9 14:07:38.111: INFO: Pod pod-configmaps-84961c42-4e30-4686-9fc1-52127334b1c1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:38.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4462" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4480,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:38.130: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2221
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-a88b2285-cbfe-486f-8ec9-5ed47ab7ef80
STEP: Creating secret with name s-test-opt-upd-4039c202-a0c6-4a53-a585-e365348a54e5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a88b2285-cbfe-486f-8ec9-5ed47ab7ef80
STEP: Updating secret s-test-opt-upd-4039c202-a0c6-4a53-a585-e365348a54e5
STEP: Creating secret with name s-test-opt-create-fd710192-fde6-4d10-84f6-4738dd68c316
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:44.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2221" for this suite.

• [SLOW TEST:6.390 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4484,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  9 14:07:45.259: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun  9 14:07:47.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308465, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308465, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308465, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308465, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 14:07:50.309: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jun  9 14:07:50.322: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:51.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4326" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.469 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":276,"skipped":4490,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:51.992: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:07:57.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3096" for this suite.

• [SLOW TEST:5.150 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":277,"skipped":4508,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:07:57.142: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun  9 14:07:57.407: INFO: Pod name pod-release: Found 0 pods out of 1
Jun  9 14:08:02.413: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:08:03.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-836" for this suite.

• [SLOW TEST:6.336 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":278,"skipped":4518,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:08:03.479: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1397
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-8edf2b2b-d41e-4761-b636-1c13034d16e1
STEP: Creating a pod to test consume configMaps
Jun  9 14:08:03.782: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8" in namespace "projected-1397" to be "success or failure"
Jun  9 14:08:03.793: INFO: Pod "pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.038734ms
Jun  9 14:08:05.802: INFO: Pod "pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02044302s
Jun  9 14:08:07.815: INFO: Pod "pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032713064s
STEP: Saw pod success
Jun  9 14:08:07.815: INFO: Pod "pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8" satisfied condition "success or failure"
Jun  9 14:08:07.819: INFO: Trying to get logs from node worker-k8xcg-8bbfd5b68-w4htb pod pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  9 14:08:07.874: INFO: Waiting for pod pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8 to disappear
Jun  9 14:08:07.888: INFO: Pod pod-projected-configmaps-0e9a72d9-3370-4fc4-ae3d-a4f3e15ad6a8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:08:07.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1397" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4523,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun  9 14:08:07.923: INFO: >>> kubeConfig: /tmp/kubeconfig-503842985
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  9 14:08:08.971: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 14:08:10.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308489, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308489, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308489, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308488, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 14:08:12.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308489, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308489, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308489, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727308488, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  9 14:08:16.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun  9 14:08:16.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7940" for this suite.
STEP: Destroying namespace "webhook-7940-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.276 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":280,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSJun  9 14:08:16.201: INFO: Running AfterSuite actions on all nodes
Jun  9 14:08:16.202: INFO: Running AfterSuite actions on node 1
Jun  9 14:08:16.202: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4821.660 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h20m23.918842769s
Test Suite Passed
