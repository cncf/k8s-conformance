I0818 17:39:44.386397      26 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-222924798
I0818 17:39:44.386425      26 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0818 17:39:44.386531      26 e2e.go:109] Starting e2e run "d6a52bd9-248c-4f24-bf94-bf6d9af69632" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1597772382 - Will randomize all specs
Will run 280 of 4844 specs

Aug 18 17:39:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 17:39:44.403: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 18 17:39:44.504: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 18 17:39:44.830: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 18 17:39:44.831: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Aug 18 17:39:44.831: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 18 17:39:44.883: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 18 17:39:44.884: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 18 17:39:44.884: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Aug 18 17:39:44.884: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Aug 18 17:39:44.884: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Aug 18 17:39:44.884: INFO: e2e test version: v1.17.11
Aug 18 17:39:44.886: INFO: kube-apiserver version: v1.17.11+IKS
Aug 18 17:39:44.886: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 17:39:44.902: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:39:44.903: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
Aug 18 17:39:45.038: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 18 17:39:45.324: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 18 17:39:52.151: INFO: Successfully updated pod "pod-update-627d85c7-cb57-43ac-ad66-617e09051c18"
STEP: verifying the updated pod is in kubernetes
Aug 18 17:39:52.180: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:39:52.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4410" for this suite.

• [SLOW TEST:7.354 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":1,"skipped":14,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:39:52.261: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1587
[It] should support rolling-update to same image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 17:39:52.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7920'
Aug 18 17:39:53.035: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 18 17:39:53.035: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Aug 18 17:39:53.052: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Aug 18 17:39:53.095: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 18 17:39:53.124: INFO: scanned /root for discovery docs: <nil>
Aug 18 17:39:53.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7920'
Aug 18 17:40:14.807: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 18 17:40:14.807: INFO: stdout: "Created e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3\nScaling up e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Aug 18 17:40:14.807: INFO: stdout: "Created e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3\nScaling up e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Aug 18 17:40:14.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7920'
Aug 18 17:40:15.663: INFO: stderr: ""
Aug 18 17:40:15.663: INFO: stdout: "e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3-k6wrv "
Aug 18 17:40:15.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3-k6wrv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7920'
Aug 18 17:40:15.777: INFO: stderr: ""
Aug 18 17:40:15.777: INFO: stdout: "true"
Aug 18 17:40:15.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3-k6wrv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7920'
Aug 18 17:40:15.877: INFO: stderr: ""
Aug 18 17:40:15.877: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Aug 18 17:40:15.877: INFO: e2e-test-httpd-rc-8cb6018aaeb3a264b93b6bc1c66275c3-k6wrv is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1593
Aug 18 17:40:15.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete rc e2e-test-httpd-rc --namespace=kubectl-7920'
Aug 18 17:40:16.011: INFO: stderr: ""
Aug 18 17:40:16.011: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:40:16.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7920" for this suite.

• [SLOW TEST:23.816 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1582
    should support rolling-update to same image [Deprecated] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image [Deprecated] [Conformance]","total":280,"completed":2,"skipped":21,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:40:16.078: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:40:17.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369216, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:40:19.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369216, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:40:21.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369217, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369216, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:40:24.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 18 17:40:24.232: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:40:24.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1855" for this suite.
STEP: Destroying namespace "webhook-1855-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":3,"skipped":31,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:40:24.742: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-659
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:40:25.017: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:40:30.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-659" for this suite.

• [SLOW TEST:5.483 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":4,"skipped":40,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:40:30.225: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 18 17:40:30.816: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 18 17:40:30.956: INFO: Waiting for terminating namespaces to be deleted...
Aug 18 17:40:30.973: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.114 before test
Aug 18 17:40:31.069: INFO: vpn-f66c45467-kh4hm from kube-system started at 2020-08-18 16:16:37 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.069: INFO: 	Container vpn ready: true, restart count 0
Aug 18 17:40:31.069: INFO: calico-node-2ngjr from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.069: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 17:40:31.069: INFO: ibm-keepalived-watcher-q59v8 from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.069: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 17:40:31.069: INFO: metrics-server-797d668946-ltqcd from kube-system started at 2020-08-18 15:52:24 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.069: INFO: 	Container metrics-server ready: true, restart count 0
Aug 18 17:40:31.069: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 18 17:40:31.069: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 17:40:31.069: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 17:40:31.069: INFO: ibm-master-proxy-static-10.13.3.114 from kube-system started at 2020-08-18 15:51:09 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.069: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 17:40:31.070: INFO: 	Container pause ready: true, restart count 0
Aug 18 17:40:31.070: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb from kube-system started at 2020-08-18 15:56:04 +0000 UTC (4 container statuses recorded)
Aug 18 17:40:31.070: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 17:40:31.070: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 17:40:31.070: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 17:40:31.070: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 17:40:31.070: INFO: calico-kube-controllers-5754cfb59d-8hh6k from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.070: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 18 17:40:31.070: INFO: coredns-6567db4fff-6tjgz from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.070: INFO: 	Container coredns ready: true, restart count 0
Aug 18 17:40:31.070: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.115 before test
Aug 18 17:40:31.172: INFO: ibm-file-plugin-ff7c989f9-fsrmg from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 18 17:40:31.172: INFO: sonobuoy-e2e-job-715a84e433104f63 from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container e2e ready: true, restart count 0
Aug 18 17:40:31.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 17:40:31.172: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-g6ck9 from ibm-system started at 2020-08-18 15:52:58 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 17:40:31.172: INFO: coredns-6567db4fff-wxj8p from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container coredns ready: true, restart count 0
Aug 18 17:40:31.172: INFO: ibm-master-proxy-static-10.13.3.115 from kube-system started at 2020-08-18 15:51:10 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 17:40:31.172: INFO: 	Container pause ready: true, restart count 0
Aug 18 17:40:31.172: INFO: calico-node-j6sx8 from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 17:40:31.172: INFO: dashboard-metrics-scraper-5789d44f58-nsshl from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 18 17:40:31.172: INFO: catalog-operator-67646bfcdb-77lhs from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 18 17:40:31.172: INFO: olm-operator-787498c9b7-9cmpc from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container olm-operator ready: true, restart count 0
Aug 18 17:40:31.172: INFO: ibm-keepalived-watcher-zktgc from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 17:40:31.172: INFO: kubernetes-dashboard-984c5c57-bl4dx from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 18 17:40:31.172: INFO: coredns-autoscaler-649976fbf4-q69hh from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container autoscaler ready: true, restart count 0
Aug 18 17:40:31.172: INFO: sonobuoy from sonobuoy started at 2020-08-18 17:39:12 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 18 17:40:31.172: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-cpwhn from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 17:40:31.172: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 17:40:31.172: INFO: ibm-storage-watcher-56b6fd445c-dqt8p from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.172: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 18 17:40:31.172: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.84 before test
Aug 18 17:40:31.245: INFO: ibm-keepalived-watcher-228kf from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 17:40:31.245: INFO: calico-node-b68l4 from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 17:40:31.245: INFO: addon-catalog-source-62ppj from ibm-system started at 2020-08-18 15:52:32 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container configmap-registry-server ready: true, restart count 0
Aug 18 17:40:31.245: INFO: coredns-6567db4fff-4lb9m from kube-system started at 2020-08-18 16:16:59 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container coredns ready: true, restart count 0
Aug 18 17:40:31.245: INFO: ibm-master-proxy-static-10.13.3.84 from kube-system started at 2020-08-18 15:51:51 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 17:40:31.245: INFO: 	Container pause ready: true, restart count 0
Aug 18 17:40:31.245: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-vs6hk from ibm-system started at 2020-08-18 15:52:58 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 17:40:31.245: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-08-18 15:55:58 +0000 UTC (1 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 18 17:40:31.245: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-56hk8 from kube-system started at 2020-08-18 15:56:04 +0000 UTC (4 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 17:40:31.245: INFO: 	Container ingress-auth-2 ready: true, restart count 1
Aug 18 17:40:31.245: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 17:40:31.245: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 17:40:31.245: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-kdbbz from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 17:40:31.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 17:40:31.245: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node 10.13.3.114
STEP: verifying the node has the label node 10.13.3.115
STEP: verifying the node has the label node 10.13.3.84
Aug 18 17:40:31.573: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.13.3.84
Aug 18 17:40:31.573: INFO: Pod addon-catalog-source-62ppj requesting resource cpu=10m on Node 10.13.3.84
Aug 18 17:40:31.573: INFO: Pod catalog-operator-67646bfcdb-77lhs requesting resource cpu=10m on Node 10.13.3.115
Aug 18 17:40:31.573: INFO: Pod ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-g6ck9 requesting resource cpu=5m on Node 10.13.3.115
Aug 18 17:40:31.573: INFO: Pod ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-vs6hk requesting resource cpu=5m on Node 10.13.3.84
Aug 18 17:40:31.574: INFO: Pod olm-operator-787498c9b7-9cmpc requesting resource cpu=10m on Node 10.13.3.115
Aug 18 17:40:31.574: INFO: Pod calico-kube-controllers-5754cfb59d-8hh6k requesting resource cpu=10m on Node 10.13.3.114
Aug 18 17:40:31.574: INFO: Pod calico-node-2ngjr requesting resource cpu=250m on Node 10.13.3.114
Aug 18 17:40:31.574: INFO: Pod calico-node-b68l4 requesting resource cpu=250m on Node 10.13.3.84
Aug 18 17:40:31.574: INFO: Pod calico-node-j6sx8 requesting resource cpu=250m on Node 10.13.3.115
Aug 18 17:40:31.574: INFO: Pod coredns-6567db4fff-4lb9m requesting resource cpu=100m on Node 10.13.3.84
Aug 18 17:40:31.574: INFO: Pod coredns-6567db4fff-6tjgz requesting resource cpu=100m on Node 10.13.3.114
Aug 18 17:40:31.574: INFO: Pod coredns-6567db4fff-wxj8p requesting resource cpu=100m on Node 10.13.3.115
Aug 18 17:40:31.574: INFO: Pod coredns-autoscaler-649976fbf4-q69hh requesting resource cpu=20m on Node 10.13.3.115
Aug 18 17:40:31.574: INFO: Pod dashboard-metrics-scraper-5789d44f58-nsshl requesting resource cpu=1m on Node 10.13.3.115
Aug 18 17:40:31.575: INFO: Pod ibm-file-plugin-ff7c989f9-fsrmg requesting resource cpu=50m on Node 10.13.3.115
Aug 18 17:40:31.575: INFO: Pod ibm-keepalived-watcher-228kf requesting resource cpu=5m on Node 10.13.3.84
Aug 18 17:40:31.575: INFO: Pod ibm-keepalived-watcher-q59v8 requesting resource cpu=5m on Node 10.13.3.114
Aug 18 17:40:31.575: INFO: Pod ibm-keepalived-watcher-zktgc requesting resource cpu=5m on Node 10.13.3.115
Aug 18 17:40:31.575: INFO: Pod ibm-master-proxy-static-10.13.3.114 requesting resource cpu=25m on Node 10.13.3.114
Aug 18 17:40:31.575: INFO: Pod ibm-master-proxy-static-10.13.3.115 requesting resource cpu=25m on Node 10.13.3.115
Aug 18 17:40:31.575: INFO: Pod ibm-master-proxy-static-10.13.3.84 requesting resource cpu=25m on Node 10.13.3.84
Aug 18 17:40:31.575: INFO: Pod ibm-storage-watcher-56b6fd445c-dqt8p requesting resource cpu=50m on Node 10.13.3.115
Aug 18 17:40:31.575: INFO: Pod kubernetes-dashboard-984c5c57-bl4dx requesting resource cpu=50m on Node 10.13.3.115
Aug 18 17:40:31.575: INFO: Pod metrics-server-797d668946-ltqcd requesting resource cpu=121m on Node 10.13.3.114
Aug 18 17:40:31.575: INFO: Pod public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-56hk8 requesting resource cpu=10m on Node 10.13.3.84
Aug 18 17:40:31.576: INFO: Pod public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb requesting resource cpu=10m on Node 10.13.3.114
Aug 18 17:40:31.576: INFO: Pod vpn-f66c45467-kh4hm requesting resource cpu=5m on Node 10.13.3.114
Aug 18 17:40:31.576: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.13.3.115
Aug 18 17:40:31.576: INFO: Pod sonobuoy-e2e-job-715a84e433104f63 requesting resource cpu=0m on Node 10.13.3.115
Aug 18 17:40:31.576: INFO: Pod sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l requesting resource cpu=0m on Node 10.13.3.114
Aug 18 17:40:31.576: INFO: Pod sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-cpwhn requesting resource cpu=0m on Node 10.13.3.115
Aug 18 17:40:31.576: INFO: Pod sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-kdbbz requesting resource cpu=0m on Node 10.13.3.84
STEP: Starting Pods to consume most of the cluster CPU.
Aug 18 17:40:31.576: INFO: Creating a pod which consumes cpu=2368m on Node 10.13.3.114
Aug 18 17:40:31.629: INFO: Creating a pod which consumes cpu=2333m on Node 10.13.3.115
Aug 18 17:40:31.646: INFO: Creating a pod which consumes cpu=2453m on Node 10.13.3.84
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b.162c6d8fae99443a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5239/filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b to 10.13.3.115]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b.162c6d90192b322d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b.162c6d904453a9f1], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b.162c6d9047d32c23], Reason = [Created], Message = [Created container filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b.162c6d9050541a83], Reason = [Started], Message = [Started container filler-pod-0e3e0fe8-53bb-4fbf-a17a-a3eabb0d5b6b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45.162c6d8fad7af73f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5239/filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45 to 10.13.3.114]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45.162c6d8ff2221093], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45.162c6d90179dc93f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45.162c6d901b51877c], Reason = [Created], Message = [Created container filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45.162c6d90231939f8], Reason = [Started], Message = [Started container filler-pod-bb0c4948-1b56-45a5-8187-6023c3887d45]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369.162c6d8fafa54142], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5239/filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369 to 10.13.3.84]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369.162c6d900c16c242], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369.162c6d9037beff6b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369.162c6d903b8ffeda], Reason = [Created], Message = [Created container filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369.162c6d90446e47d7], Reason = [Started], Message = [Started container filler-pod-ecc421b6-6216-4e38-a4e2-b6188a1eb369]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162c6d90a411cf63], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.13.3.114
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.13.3.115
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.13.3.84
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:40:37.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5239" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:7.605 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":5,"skipped":50,"failed":0}
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:40:37.831: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-5965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Aug 18 17:40:38.345: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 18 17:41:38.400: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:41:38.417: INFO: Starting informer...
STEP: Starting pod...
Aug 18 17:41:38.748: INFO: Pod is running on 10.13.3.84. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 18 17:41:38.856: INFO: Pod wasn't evicted. Proceeding
Aug 18 17:41:38.856: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 18 17:42:54.065: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:42:54.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5965" for this suite.

• [SLOW TEST:136.309 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":6,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:42:54.141: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-cce4e97d-914b-4e9d-9e58-3660d6e01394
STEP: Creating a pod to test consume secrets
Aug 18 17:42:54.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b" in namespace "projected-9293" to be "success or failure"
Aug 18 17:42:54.555: INFO: Pod "pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.275474ms
Aug 18 17:42:56.569: INFO: Pod "pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028535971s
Aug 18 17:42:58.586: INFO: Pod "pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045659492s
Aug 18 17:43:00.601: INFO: Pod "pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060943326s
STEP: Saw pod success
Aug 18 17:43:00.601: INFO: Pod "pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b" satisfied condition "success or failure"
Aug 18 17:43:00.617: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 18 17:43:00.751: INFO: Waiting for pod pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b to disappear
Aug 18 17:43:00.768: INFO: Pod pod-projected-secrets-d69d5860-678b-4aaf-a865-b03da18fb80b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9293" for this suite.

• [SLOW TEST:6.712 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":151,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:00.854: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:43:02.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:43:04.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369381, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:43:07.244: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:18.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3273" for this suite.
STEP: Destroying namespace "webhook-3273-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.144 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":8,"skipped":159,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:18.998: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:30.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5656" for this suite.

• [SLOW TEST:11.974 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":9,"skipped":170,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:30.974: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9151
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:43:31.270: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:32.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9151" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":10,"skipped":185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:32.414: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 18 17:43:33.230: INFO: Waiting up to 5m0s for pod "pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f" in namespace "emptydir-3159" to be "success or failure"
Aug 18 17:43:33.243: INFO: Pod "pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.145689ms
Aug 18 17:43:35.256: INFO: Pod "pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f": Phase="Running", Reason="", readiness=true. Elapsed: 2.025501928s
Aug 18 17:43:37.271: INFO: Pod "pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04122074s
STEP: Saw pod success
Aug 18 17:43:37.271: INFO: Pod "pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f" satisfied condition "success or failure"
Aug 18 17:43:37.285: INFO: Trying to get logs from node 10.13.3.84 pod pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f container test-container: <nil>
STEP: delete the pod
Aug 18 17:43:37.438: INFO: Waiting for pod pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f to disappear
Aug 18 17:43:37.466: INFO: Pod pod-4aea81dc-69f7-4465-ba05-8b64e5aec83f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:37.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3159" for this suite.

• [SLOW TEST:5.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":244,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:37.554: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 17:43:38.176: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20" in namespace "downward-api-875" to be "success or failure"
Aug 18 17:43:38.203: INFO: Pod "downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20": Phase="Pending", Reason="", readiness=false. Elapsed: 26.94849ms
Aug 18 17:43:40.218: INFO: Pod "downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041614974s
Aug 18 17:43:42.239: INFO: Pod "downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063526476s
STEP: Saw pod success
Aug 18 17:43:42.239: INFO: Pod "downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20" satisfied condition "success or failure"
Aug 18 17:43:42.261: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20 container client-container: <nil>
STEP: delete the pod
Aug 18 17:43:42.395: INFO: Waiting for pod downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20 to disappear
Aug 18 17:43:42.419: INFO: Pod downwardapi-volume-acdd777a-0743-44b4-999f-c632b5ae8c20 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:42.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-875" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":12,"skipped":245,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 18 17:43:42.821: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:49.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7347" for this suite.

• [SLOW TEST:7.158 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":13,"skipped":256,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:49.678: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-0dabe161-3b82-4203-883e-a869cd91e487
STEP: Creating a pod to test consume secrets
Aug 18 17:43:50.014: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488" in namespace "projected-5946" to be "success or failure"
Aug 18 17:43:50.028: INFO: Pod "pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488": Phase="Pending", Reason="", readiness=false. Elapsed: 14.741316ms
Aug 18 17:43:52.045: INFO: Pod "pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030791139s
STEP: Saw pod success
Aug 18 17:43:52.045: INFO: Pod "pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488" satisfied condition "success or failure"
Aug 18 17:43:52.063: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 18 17:43:52.158: INFO: Waiting for pod pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488 to disappear
Aug 18 17:43:52.171: INFO: Pod pod-projected-secrets-4cde78af-670a-4c0a-948a-605e9fec5488 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:43:52.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5946" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":267,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:43:52.266: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 18 17:44:01.712: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:44:01.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6029" for this suite.

• [SLOW TEST:9.599 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":15,"skipped":277,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:44:01.866: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:44:02.238: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b38f5291-db76-4a66-a934-0995742e47f2" in namespace "security-context-test-2302" to be "success or failure"
Aug 18 17:44:02.256: INFO: Pod "busybox-user-65534-b38f5291-db76-4a66-a934-0995742e47f2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.420752ms
Aug 18 17:44:04.273: INFO: Pod "busybox-user-65534-b38f5291-db76-4a66-a934-0995742e47f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034388655s
Aug 18 17:44:06.287: INFO: Pod "busybox-user-65534-b38f5291-db76-4a66-a934-0995742e47f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048395843s
Aug 18 17:44:06.287: INFO: Pod "busybox-user-65534-b38f5291-db76-4a66-a934-0995742e47f2" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:44:06.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2302" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":16,"skipped":289,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:44:06.367: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:325
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Aug 18 17:44:06.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-7421'
Aug 18 17:44:06.977: INFO: stderr: ""
Aug 18 17:44:06.977: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 18 17:44:06.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7421'
Aug 18 17:44:07.095: INFO: stderr: ""
Aug 18 17:44:07.095: INFO: stdout: "update-demo-nautilus-j56f7 update-demo-nautilus-vdxwb "
Aug 18 17:44:07.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-j56f7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:07.225: INFO: stderr: ""
Aug 18 17:44:07.225: INFO: stdout: ""
Aug 18 17:44:07.225: INFO: update-demo-nautilus-j56f7 is created but not running
Aug 18 17:44:12.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7421'
Aug 18 17:44:12.331: INFO: stderr: ""
Aug 18 17:44:12.331: INFO: stdout: "update-demo-nautilus-j56f7 update-demo-nautilus-vdxwb "
Aug 18 17:44:12.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-j56f7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:12.430: INFO: stderr: ""
Aug 18 17:44:12.430: INFO: stdout: "true"
Aug 18 17:44:12.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-j56f7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:12.541: INFO: stderr: ""
Aug 18 17:44:12.541: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 17:44:12.541: INFO: validating pod update-demo-nautilus-j56f7
Aug 18 17:44:12.566: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 17:44:12.566: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 17:44:12.566: INFO: update-demo-nautilus-j56f7 is verified up and running
Aug 18 17:44:12.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-vdxwb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:12.665: INFO: stderr: ""
Aug 18 17:44:12.665: INFO: stdout: "true"
Aug 18 17:44:12.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-vdxwb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:12.798: INFO: stderr: ""
Aug 18 17:44:12.798: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 17:44:12.798: INFO: validating pod update-demo-nautilus-vdxwb
Aug 18 17:44:12.824: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 17:44:12.824: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 17:44:12.824: INFO: update-demo-nautilus-vdxwb is verified up and running
STEP: rolling-update to new replication controller
Aug 18 17:44:12.826: INFO: scanned /root for discovery docs: <nil>
Aug 18 17:44:12.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7421'
Aug 18 17:44:36.217: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 18 17:44:36.217: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 18 17:44:36.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7421'
Aug 18 17:44:36.317: INFO: stderr: ""
Aug 18 17:44:36.317: INFO: stdout: "update-demo-kitten-228d9 update-demo-kitten-pn49d "
Aug 18 17:44:36.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-kitten-228d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:36.437: INFO: stderr: ""
Aug 18 17:44:36.437: INFO: stdout: "true"
Aug 18 17:44:36.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-kitten-228d9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:36.546: INFO: stderr: ""
Aug 18 17:44:36.546: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 18 17:44:36.546: INFO: validating pod update-demo-kitten-228d9
Aug 18 17:44:36.567: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 18 17:44:36.568: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 18 17:44:36.568: INFO: update-demo-kitten-228d9 is verified up and running
Aug 18 17:44:36.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-kitten-pn49d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:36.673: INFO: stderr: ""
Aug 18 17:44:36.673: INFO: stdout: "true"
Aug 18 17:44:36.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-kitten-pn49d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7421'
Aug 18 17:44:36.768: INFO: stderr: ""
Aug 18 17:44:36.768: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 18 17:44:36.768: INFO: validating pod update-demo-kitten-pn49d
Aug 18 17:44:36.802: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 18 17:44:36.802: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 18 17:44:36.802: INFO: update-demo-kitten-pn49d is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:44:36.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7421" for this suite.

• [SLOW TEST:30.543 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:323
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":17,"skipped":304,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:44:36.911: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Aug 18 17:44:37.185: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 18 17:45:37.251: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:45:37.277: INFO: Starting informer...
STEP: Starting pods...
Aug 18 17:45:37.581: INFO: Pod1 is running on 10.13.3.84. Tainting Node
Aug 18 17:45:40.061: INFO: Pod2 is running on 10.13.3.84. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 18 17:45:51.446: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 18 17:46:11.422: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:12.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4583" for this suite.

• [SLOW TEST:95.519 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":18,"skipped":314,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8127
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Aug 18 17:46:12.732: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:32.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8127" for this suite.

• [SLOW TEST:20.781 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":19,"skipped":333,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:33.214: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5529
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6445
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:41.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2290" for this suite.
STEP: Destroying namespace "nsdeletetest-5529" for this suite.
Aug 18 17:46:41.357: INFO: Namespace nsdeletetest-5529 was already deleted
STEP: Destroying namespace "nsdeletetest-6445" for this suite.

• [SLOW TEST:8.174 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":20,"skipped":338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:41.388: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
[It] should create an rc from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 17:46:41.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-343'
Aug 18 17:46:41.791: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 18 17:46:41.791: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Aug 18 17:46:41.823: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-p9vdm]
Aug 18 17:46:41.823: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-p9vdm" in namespace "kubectl-343" to be "running and ready"
Aug 18 17:46:41.839: INFO: Pod "e2e-test-httpd-rc-p9vdm": Phase="Pending", Reason="", readiness=false. Elapsed: 16.808728ms
Aug 18 17:46:43.855: INFO: Pod "e2e-test-httpd-rc-p9vdm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032747508s
Aug 18 17:46:45.872: INFO: Pod "e2e-test-httpd-rc-p9vdm": Phase="Running", Reason="", readiness=true. Elapsed: 4.049453081s
Aug 18 17:46:45.872: INFO: Pod "e2e-test-httpd-rc-p9vdm" satisfied condition "running and ready"
Aug 18 17:46:45.872: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-p9vdm]
Aug 18 17:46:45.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs rc/e2e-test-httpd-rc --namespace=kubectl-343'
Aug 18 17:46:46.098: INFO: stderr: ""
Aug 18 17:46:46.098: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.14.204. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.14.204. Set the 'ServerName' directive globally to suppress this message\n[Tue Aug 18 17:46:44.393723 2020] [mpm_event:notice] [pid 1:tid 139812500577128] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue Aug 18 17:46:44.393788 2020] [core:notice] [pid 1:tid 139812500577128] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1531
Aug 18 17:46:46.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete rc e2e-test-httpd-rc --namespace=kubectl-343'
Aug 18 17:46:46.229: INFO: stderr: ""
Aug 18 17:46:46.229: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:46.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-343" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image [Deprecated] [Conformance]","total":280,"completed":21,"skipped":365,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:46.304: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:46:47.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369607, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369607, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369607, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369607, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:46:50.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:50.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7314" for this suite.
STEP: Destroying namespace "webhook-7314-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":22,"skipped":376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:50.811: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-835
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 18 17:46:51.350: INFO: Waiting up to 5m0s for pod "pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653" in namespace "emptydir-835" to be "success or failure"
Aug 18 17:46:51.363: INFO: Pod "pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653": Phase="Pending", Reason="", readiness=false. Elapsed: 12.644501ms
Aug 18 17:46:53.376: INFO: Pod "pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025778089s
STEP: Saw pod success
Aug 18 17:46:53.376: INFO: Pod "pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653" satisfied condition "success or failure"
Aug 18 17:46:53.389: INFO: Trying to get logs from node 10.13.3.84 pod pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653 container test-container: <nil>
STEP: delete the pod
Aug 18 17:46:53.495: INFO: Waiting for pod pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653 to disappear
Aug 18 17:46:53.508: INFO: Pod pod-62b99eb4-9d6e-4fa3-9858-e949d42c2653 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:53.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-835" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":23,"skipped":425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:53.585: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 18 17:46:54.115: INFO: Waiting up to 5m0s for pod "downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210" in namespace "downward-api-2971" to be "success or failure"
Aug 18 17:46:54.129: INFO: Pod "downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210": Phase="Pending", Reason="", readiness=false. Elapsed: 14.122123ms
Aug 18 17:46:56.143: INFO: Pod "downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027771911s
STEP: Saw pod success
Aug 18 17:46:56.143: INFO: Pod "downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210" satisfied condition "success or failure"
Aug 18 17:46:56.155: INFO: Trying to get logs from node 10.13.3.84 pod downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210 container dapi-container: <nil>
STEP: delete the pod
Aug 18 17:46:56.289: INFO: Waiting for pod downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210 to disappear
Aug 18 17:46:56.303: INFO: Pod downward-api-28073bd9-8038-4c1a-a441-36b5c3c54210 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:56.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2971" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:56.424: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:46:56.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9388" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":25,"skipped":506,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:46:56.761: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1746
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1746
I0818 17:46:57.771028      26 runners.go:189] Created replication controller with name: externalname-service, namespace: services-1746, replica count: 2
I0818 17:47:00.821630      26 runners.go:189] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0818 17:47:03.821898      26 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 18 17:47:03.821: INFO: Creating new exec pod
Aug 18 17:47:09.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-1746 execpod85mpm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 18 17:47:09.598: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 18 17:47:09.599: INFO: stdout: ""
Aug 18 17:47:09.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-1746 execpod85mpm -- /bin/sh -x -c nc -zv -t -w 2 172.21.106.16 80'
Aug 18 17:47:09.883: INFO: stderr: "+ nc -zv -t -w 2 172.21.106.16 80\nConnection to 172.21.106.16 80 port [tcp/http] succeeded!\n"
Aug 18 17:47:09.883: INFO: stdout: ""
Aug 18 17:47:09.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-1746 execpod85mpm -- /bin/sh -x -c nc -zv -t -w 2 10.13.3.84 31782'
Aug 18 17:47:10.161: INFO: stderr: "+ nc -zv -t -w 2 10.13.3.84 31782\nConnection to 10.13.3.84 31782 port [tcp/31782] succeeded!\n"
Aug 18 17:47:10.161: INFO: stdout: ""
Aug 18 17:47:10.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-1746 execpod85mpm -- /bin/sh -x -c nc -zv -t -w 2 10.13.3.115 31782'
Aug 18 17:47:10.446: INFO: stderr: "+ nc -zv -t -w 2 10.13.3.115 31782\nConnection to 10.13.3.115 31782 port [tcp/31782] succeeded!\n"
Aug 18 17:47:10.446: INFO: stdout: ""
Aug 18 17:47:10.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-1746 execpod85mpm -- /bin/sh -x -c nc -zv -t -w 2 149.81.149.61 31782'
Aug 18 17:47:10.735: INFO: stderr: "+ nc -zv -t -w 2 149.81.149.61 31782\nConnection to 149.81.149.61 31782 port [tcp/31782] succeeded!\n"
Aug 18 17:47:10.735: INFO: stdout: ""
Aug 18 17:47:10.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-1746 execpod85mpm -- /bin/sh -x -c nc -zv -t -w 2 149.81.149.62 31782'
Aug 18 17:47:11.047: INFO: stderr: "+ nc -zv -t -w 2 149.81.149.62 31782\nConnection to 149.81.149.62 31782 port [tcp/31782] succeeded!\n"
Aug 18 17:47:11.047: INFO: stdout: ""
Aug 18 17:47:11.047: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:47:11.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1746" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:14.519 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":26,"skipped":509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:47:11.280: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:47:12.363: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 17:47:14.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369632, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369632, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369632, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369632, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:47:17.517: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:47:17.845: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Aug 18 17:47:28.487: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:47:30.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3368" for this suite.
STEP: Destroying namespace "webhook-3368-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.989 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":27,"skipped":536,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:47:31.270: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2234
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:47:31.602: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:47:31.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2234" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":28,"skipped":549,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:47:31.847: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-254c8df6-b5aa-49b0-8761-b5ad5969af27 in namespace container-probe-6402
Aug 18 17:47:36.219: INFO: Started pod liveness-254c8df6-b5aa-49b0-8761-b5ad5969af27 in namespace container-probe-6402
STEP: checking the pod's current state and verifying that restartCount is present
Aug 18 17:47:36.231: INFO: Initial restart count of pod liveness-254c8df6-b5aa-49b0-8761-b5ad5969af27 is 0
Aug 18 17:47:58.407: INFO: Restart count of pod container-probe-6402/liveness-254c8df6-b5aa-49b0-8761-b5ad5969af27 is now 1 (22.175529114s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:47:58.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6402" for this suite.

• [SLOW TEST:26.770 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:47:58.618: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:48:04.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8091" for this suite.

• [SLOW TEST:5.511 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":30,"skipped":584,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:48:04.130: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 17:48:04.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b" in namespace "projected-2062" to be "success or failure"
Aug 18 17:48:04.460: INFO: Pod "downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.667324ms
Aug 18 17:48:06.474: INFO: Pod "downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02754187s
Aug 18 17:48:08.490: INFO: Pod "downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043443486s
STEP: Saw pod success
Aug 18 17:48:08.490: INFO: Pod "downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b" satisfied condition "success or failure"
Aug 18 17:48:08.504: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b container client-container: <nil>
STEP: delete the pod
Aug 18 17:48:08.971: INFO: Waiting for pod downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b to disappear
Aug 18 17:48:08.989: INFO: Pod downwardapi-volume-aa437155-2e03-4b09-8507-6fd17640b94b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:48:08.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2062" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":31,"skipped":600,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:48:09.079: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:48:10.364: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 17:48:12.433: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369690, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369690, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369690, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733369690, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:48:15.517: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:48:15.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3566" for this suite.
STEP: Destroying namespace "webhook-3566-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.097 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":32,"skipped":606,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:48:16.176: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-70b66d07-ec6c-4b15-98a8-f4d5e92e8417
STEP: Creating a pod to test consume secrets
Aug 18 17:48:16.521: INFO: Waiting up to 5m0s for pod "pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045" in namespace "secrets-3700" to be "success or failure"
Aug 18 17:48:16.534: INFO: Pod "pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045": Phase="Pending", Reason="", readiness=false. Elapsed: 13.002176ms
Aug 18 17:48:18.547: INFO: Pod "pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0257379s
STEP: Saw pod success
Aug 18 17:48:18.547: INFO: Pod "pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045" satisfied condition "success or failure"
Aug 18 17:48:18.563: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045 container secret-env-test: <nil>
STEP: delete the pod
Aug 18 17:48:18.869: INFO: Waiting for pod pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045 to disappear
Aug 18 17:48:18.882: INFO: Pod pod-secrets-a96d9562-8e0e-4874-a783-715ec2bef045 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:48:18.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3700" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:48:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-ed7bc5e0-a73c-4aa4-9f08-b68c06843aa5 in namespace container-probe-1767
Aug 18 17:48:23.547: INFO: Started pod test-webserver-ed7bc5e0-a73c-4aa4-9f08-b68c06843aa5 in namespace container-probe-1767
STEP: checking the pod's current state and verifying that restartCount is present
Aug 18 17:48:23.560: INFO: Initial restart count of pod test-webserver-ed7bc5e0-a73c-4aa4-9f08-b68c06843aa5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:52:24.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1767" for this suite.

• [SLOW TEST:246.002 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":34,"skipped":671,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:52:24.976: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2x7tj in namespace proxy-9241
I0818 17:52:25.359710      26 runners.go:189] Created replication controller with name: proxy-service-2x7tj, namespace: proxy-9241, replica count: 1
I0818 17:52:26.410187      26 runners.go:189] proxy-service-2x7tj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0818 17:52:27.410459      26 runners.go:189] proxy-service-2x7tj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0818 17:52:28.410685      26 runners.go:189] proxy-service-2x7tj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0818 17:52:29.410911      26 runners.go:189] proxy-service-2x7tj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 18 17:52:29.434: INFO: setup took 4.177062215s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 18 17:52:29.465: INFO: (0) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 30.816886ms)
Aug 18 17:52:29.465: INFO: (0) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 30.641732ms)
Aug 18 17:52:29.467: INFO: (0) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 32.566169ms)
Aug 18 17:52:29.477: INFO: (0) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 42.729304ms)
Aug 18 17:52:29.477: INFO: (0) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 43.124124ms)
Aug 18 17:52:29.477: INFO: (0) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 42.866624ms)
Aug 18 17:52:29.478: INFO: (0) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 43.016182ms)
Aug 18 17:52:29.478: INFO: (0) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 43.661666ms)
Aug 18 17:52:29.483: INFO: (0) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 48.066705ms)
Aug 18 17:52:29.487: INFO: (0) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 52.455423ms)
Aug 18 17:52:29.488: INFO: (0) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 53.01278ms)
Aug 18 17:52:29.487: INFO: (0) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 53.024395ms)
Aug 18 17:52:29.488: INFO: (0) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 53.427336ms)
Aug 18 17:52:29.488: INFO: (0) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 53.439623ms)
Aug 18 17:52:29.490: INFO: (0) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 55.734903ms)
Aug 18 17:52:29.490: INFO: (0) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 55.966051ms)
Aug 18 17:52:29.511: INFO: (1) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 20.518461ms)
Aug 18 17:52:29.512: INFO: (1) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 21.630759ms)
Aug 18 17:52:29.514: INFO: (1) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 23.016515ms)
Aug 18 17:52:29.520: INFO: (1) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 28.992977ms)
Aug 18 17:52:29.524: INFO: (1) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 32.850051ms)
Aug 18 17:52:29.524: INFO: (1) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 33.129927ms)
Aug 18 17:52:29.524: INFO: (1) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 32.824974ms)
Aug 18 17:52:29.528: INFO: (1) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 36.893873ms)
Aug 18 17:52:29.528: INFO: (1) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 36.43071ms)
Aug 18 17:52:29.528: INFO: (1) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 36.829797ms)
Aug 18 17:52:29.529: INFO: (1) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 38.664412ms)
Aug 18 17:52:29.540: INFO: (1) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 48.590713ms)
Aug 18 17:52:29.540: INFO: (1) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 49.387373ms)
Aug 18 17:52:29.540: INFO: (1) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 48.731152ms)
Aug 18 17:52:29.548: INFO: (1) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 56.58253ms)
Aug 18 17:52:29.548: INFO: (1) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 56.780674ms)
Aug 18 17:52:29.565: INFO: (2) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 17.338506ms)
Aug 18 17:52:29.566: INFO: (2) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 18.199356ms)
Aug 18 17:52:29.567: INFO: (2) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 18.579567ms)
Aug 18 17:52:29.571: INFO: (2) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 22.815298ms)
Aug 18 17:52:29.573: INFO: (2) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 24.528131ms)
Aug 18 17:52:29.573: INFO: (2) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 24.516089ms)
Aug 18 17:52:29.573: INFO: (2) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 24.943995ms)
Aug 18 17:52:29.573: INFO: (2) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 24.58793ms)
Aug 18 17:52:29.574: INFO: (2) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 26.349893ms)
Aug 18 17:52:29.575: INFO: (2) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 26.223559ms)
Aug 18 17:52:29.588: INFO: (2) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 40.185889ms)
Aug 18 17:52:29.590: INFO: (2) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 41.681372ms)
Aug 18 17:52:29.594: INFO: (2) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 46.100618ms)
Aug 18 17:52:29.600: INFO: (2) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 51.866982ms)
Aug 18 17:52:29.600: INFO: (2) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 51.486834ms)
Aug 18 17:52:29.601: INFO: (2) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 52.470466ms)
Aug 18 17:52:29.618: INFO: (3) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 16.794743ms)
Aug 18 17:52:29.626: INFO: (3) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 24.301177ms)
Aug 18 17:52:29.626: INFO: (3) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 24.65494ms)
Aug 18 17:52:29.626: INFO: (3) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 24.969045ms)
Aug 18 17:52:29.626: INFO: (3) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 24.991318ms)
Aug 18 17:52:29.626: INFO: (3) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 25.1057ms)
Aug 18 17:52:29.627: INFO: (3) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 25.515441ms)
Aug 18 17:52:29.627: INFO: (3) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.386401ms)
Aug 18 17:52:29.627: INFO: (3) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 25.605701ms)
Aug 18 17:52:29.628: INFO: (3) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 26.32777ms)
Aug 18 17:52:29.645: INFO: (3) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 44.174134ms)
Aug 18 17:52:29.647: INFO: (3) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 46.146806ms)
Aug 18 17:52:29.647: INFO: (3) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 46.323512ms)
Aug 18 17:52:29.649: INFO: (3) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 47.052848ms)
Aug 18 17:52:29.653: INFO: (3) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 51.88819ms)
Aug 18 17:52:29.660: INFO: (3) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 58.645994ms)
Aug 18 17:52:29.686: INFO: (4) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 26.146606ms)
Aug 18 17:52:29.686: INFO: (4) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 26.421007ms)
Aug 18 17:52:29.687: INFO: (4) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 26.583205ms)
Aug 18 17:52:29.687: INFO: (4) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 27.057137ms)
Aug 18 17:52:29.687: INFO: (4) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 27.012671ms)
Aug 18 17:52:29.687: INFO: (4) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 27.122874ms)
Aug 18 17:52:29.688: INFO: (4) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 28.125848ms)
Aug 18 17:52:29.688: INFO: (4) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 28.206426ms)
Aug 18 17:52:29.693: INFO: (4) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 32.788701ms)
Aug 18 17:52:29.693: INFO: (4) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 33.392603ms)
Aug 18 17:52:29.710: INFO: (4) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 50.441133ms)
Aug 18 17:52:29.712: INFO: (4) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 52.546445ms)
Aug 18 17:52:29.716: INFO: (4) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 56.081331ms)
Aug 18 17:52:29.717: INFO: (4) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 56.751615ms)
Aug 18 17:52:29.717: INFO: (4) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 57.347987ms)
Aug 18 17:52:29.717: INFO: (4) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 57.340336ms)
Aug 18 17:52:29.734: INFO: (5) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 16.543604ms)
Aug 18 17:52:29.738: INFO: (5) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 19.402328ms)
Aug 18 17:52:29.738: INFO: (5) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 19.448209ms)
Aug 18 17:52:29.738: INFO: (5) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 19.942344ms)
Aug 18 17:52:29.739: INFO: (5) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 21.076304ms)
Aug 18 17:52:29.739: INFO: (5) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 20.752301ms)
Aug 18 17:52:29.741: INFO: (5) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 22.463455ms)
Aug 18 17:52:29.741: INFO: (5) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 22.704702ms)
Aug 18 17:52:29.741: INFO: (5) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 23.218295ms)
Aug 18 17:52:29.741: INFO: (5) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 23.44083ms)
Aug 18 17:52:29.756: INFO: (5) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 38.088914ms)
Aug 18 17:52:29.764: INFO: (5) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 46.442756ms)
Aug 18 17:52:29.771: INFO: (5) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 53.107811ms)
Aug 18 17:52:29.772: INFO: (5) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 54.130306ms)
Aug 18 17:52:29.773: INFO: (5) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 54.683852ms)
Aug 18 17:52:29.773: INFO: (5) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 55.086815ms)
Aug 18 17:52:29.795: INFO: (6) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 21.442916ms)
Aug 18 17:52:29.798: INFO: (6) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 24.659971ms)
Aug 18 17:52:29.799: INFO: (6) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 26.160338ms)
Aug 18 17:52:29.799: INFO: (6) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 26.049947ms)
Aug 18 17:52:29.799: INFO: (6) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 25.874399ms)
Aug 18 17:52:29.800: INFO: (6) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 26.514287ms)
Aug 18 17:52:29.800: INFO: (6) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 26.41085ms)
Aug 18 17:52:29.800: INFO: (6) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 26.265209ms)
Aug 18 17:52:29.800: INFO: (6) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 26.753512ms)
Aug 18 17:52:29.803: INFO: (6) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 29.289497ms)
Aug 18 17:52:29.820: INFO: (6) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 46.225881ms)
Aug 18 17:52:29.820: INFO: (6) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 46.39415ms)
Aug 18 17:52:29.823: INFO: (6) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 49.768232ms)
Aug 18 17:52:29.831: INFO: (6) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 57.465156ms)
Aug 18 17:52:29.831: INFO: (6) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 57.544719ms)
Aug 18 17:52:29.831: INFO: (6) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 57.753257ms)
Aug 18 17:52:29.849: INFO: (7) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 17.864706ms)
Aug 18 17:52:29.851: INFO: (7) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 19.885263ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 20.43122ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 20.340214ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 20.707525ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 20.556329ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 20.6747ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 20.635388ms)
Aug 18 17:52:29.852: INFO: (7) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 20.96225ms)
Aug 18 17:52:29.857: INFO: (7) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.622265ms)
Aug 18 17:52:29.871: INFO: (7) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 39.392638ms)
Aug 18 17:52:29.881: INFO: (7) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 49.886833ms)
Aug 18 17:52:29.882: INFO: (7) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 50.440928ms)
Aug 18 17:52:29.882: INFO: (7) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 50.53581ms)
Aug 18 17:52:29.882: INFO: (7) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 50.545571ms)
Aug 18 17:52:29.882: INFO: (7) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 50.561855ms)
Aug 18 17:52:29.907: INFO: (8) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 24.223169ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 29.575371ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 29.324292ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 29.612672ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 29.588716ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 30.010152ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 29.775458ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 30.010709ms)
Aug 18 17:52:29.912: INFO: (8) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 30.125837ms)
Aug 18 17:52:29.913: INFO: (8) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 30.129196ms)
Aug 18 17:52:29.922: INFO: (8) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 39.92614ms)
Aug 18 17:52:29.935: INFO: (8) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 52.046862ms)
Aug 18 17:52:29.935: INFO: (8) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 52.023695ms)
Aug 18 17:52:29.935: INFO: (8) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 52.048009ms)
Aug 18 17:52:29.935: INFO: (8) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 52.003708ms)
Aug 18 17:52:29.935: INFO: (8) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 52.188811ms)
Aug 18 17:52:29.960: INFO: (9) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 24.373834ms)
Aug 18 17:52:29.960: INFO: (9) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 24.915977ms)
Aug 18 17:52:29.961: INFO: (9) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 25.413273ms)
Aug 18 17:52:29.961: INFO: (9) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 25.369969ms)
Aug 18 17:52:29.961: INFO: (9) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.373562ms)
Aug 18 17:52:29.961: INFO: (9) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 25.573657ms)
Aug 18 17:52:29.961: INFO: (9) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 25.509022ms)
Aug 18 17:52:29.961: INFO: (9) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.479691ms)
Aug 18 17:52:29.962: INFO: (9) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 26.398742ms)
Aug 18 17:52:29.962: INFO: (9) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 26.633634ms)
Aug 18 17:52:29.972: INFO: (9) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 36.423098ms)
Aug 18 17:52:29.981: INFO: (9) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 45.4349ms)
Aug 18 17:52:29.981: INFO: (9) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 45.381403ms)
Aug 18 17:52:29.981: INFO: (9) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 45.456711ms)
Aug 18 17:52:29.982: INFO: (9) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 46.794999ms)
Aug 18 17:52:29.986: INFO: (9) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 50.978022ms)
Aug 18 17:52:30.006: INFO: (10) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 19.782791ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 23.581075ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 23.350509ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 23.678292ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 23.463613ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 23.710166ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 24.083115ms)
Aug 18 17:52:30.011: INFO: (10) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 23.846373ms)
Aug 18 17:52:30.016: INFO: (10) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 28.947577ms)
Aug 18 17:52:30.017: INFO: (10) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 30.129213ms)
Aug 18 17:52:30.025: INFO: (10) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 37.835214ms)
Aug 18 17:52:30.034: INFO: (10) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 47.301177ms)
Aug 18 17:52:30.034: INFO: (10) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 47.291053ms)
Aug 18 17:52:30.034: INFO: (10) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 47.389446ms)
Aug 18 17:52:30.034: INFO: (10) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 47.348949ms)
Aug 18 17:52:30.034: INFO: (10) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 47.779764ms)
Aug 18 17:52:30.054: INFO: (11) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 19.498638ms)
Aug 18 17:52:30.055: INFO: (11) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 19.742966ms)
Aug 18 17:52:30.056: INFO: (11) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 20.843882ms)
Aug 18 17:52:30.056: INFO: (11) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 21.917304ms)
Aug 18 17:52:30.058: INFO: (11) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 22.325736ms)
Aug 18 17:52:30.058: INFO: (11) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 22.388605ms)
Aug 18 17:52:30.058: INFO: (11) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 23.205619ms)
Aug 18 17:52:30.059: INFO: (11) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 24.30732ms)
Aug 18 17:52:30.059: INFO: (11) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 23.894355ms)
Aug 18 17:52:30.059: INFO: (11) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 23.792987ms)
Aug 18 17:52:30.082: INFO: (11) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 46.390799ms)
Aug 18 17:52:30.084: INFO: (11) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 49.12242ms)
Aug 18 17:52:30.098: INFO: (11) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 62.917312ms)
Aug 18 17:52:30.103: INFO: (11) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 68.74942ms)
Aug 18 17:52:30.103: INFO: (11) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 68.313738ms)
Aug 18 17:52:30.104: INFO: (11) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 68.430283ms)
Aug 18 17:52:30.125: INFO: (12) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 20.390042ms)
Aug 18 17:52:30.125: INFO: (12) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 21.45304ms)
Aug 18 17:52:30.125: INFO: (12) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 21.383831ms)
Aug 18 17:52:30.125: INFO: (12) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 21.123316ms)
Aug 18 17:52:30.135: INFO: (12) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 31.318138ms)
Aug 18 17:52:30.135: INFO: (12) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 31.265086ms)
Aug 18 17:52:30.135: INFO: (12) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 31.112542ms)
Aug 18 17:52:30.136: INFO: (12) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 31.466228ms)
Aug 18 17:52:30.136: INFO: (12) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 31.728584ms)
Aug 18 17:52:30.136: INFO: (12) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 32.242649ms)
Aug 18 17:52:30.154: INFO: (12) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 50.379772ms)
Aug 18 17:52:30.154: INFO: (12) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 50.23685ms)
Aug 18 17:52:30.156: INFO: (12) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 52.055866ms)
Aug 18 17:52:30.157: INFO: (12) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 53.112488ms)
Aug 18 17:52:30.157: INFO: (12) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 53.215911ms)
Aug 18 17:52:30.163: INFO: (12) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 58.792077ms)
Aug 18 17:52:30.182: INFO: (13) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 18.60798ms)
Aug 18 17:52:30.182: INFO: (13) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 19.130585ms)
Aug 18 17:52:30.182: INFO: (13) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 19.233395ms)
Aug 18 17:52:30.182: INFO: (13) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 19.162391ms)
Aug 18 17:52:30.182: INFO: (13) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 19.497009ms)
Aug 18 17:52:30.182: INFO: (13) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 19.44323ms)
Aug 18 17:52:30.188: INFO: (13) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 24.788424ms)
Aug 18 17:52:30.188: INFO: (13) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 25.144714ms)
Aug 18 17:52:30.188: INFO: (13) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 25.241929ms)
Aug 18 17:52:30.189: INFO: (13) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 25.732039ms)
Aug 18 17:52:30.213: INFO: (13) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 49.694586ms)
Aug 18 17:52:30.218: INFO: (13) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 55.042656ms)
Aug 18 17:52:30.220: INFO: (13) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 56.650182ms)
Aug 18 17:52:30.220: INFO: (13) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 57.079325ms)
Aug 18 17:52:30.220: INFO: (13) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 57.440357ms)
Aug 18 17:52:30.221: INFO: (13) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 57.091859ms)
Aug 18 17:52:30.239: INFO: (14) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 18.375861ms)
Aug 18 17:52:30.240: INFO: (14) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 19.61646ms)
Aug 18 17:52:30.241: INFO: (14) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 20.109236ms)
Aug 18 17:52:30.241: INFO: (14) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 20.511222ms)
Aug 18 17:52:30.241: INFO: (14) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 20.836494ms)
Aug 18 17:52:30.242: INFO: (14) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 21.592412ms)
Aug 18 17:52:30.246: INFO: (14) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.111176ms)
Aug 18 17:52:30.247: INFO: (14) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 26.301472ms)
Aug 18 17:52:30.247: INFO: (14) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 26.234544ms)
Aug 18 17:52:30.247: INFO: (14) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 26.425952ms)
Aug 18 17:52:30.261: INFO: (14) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 39.507448ms)
Aug 18 17:52:30.269: INFO: (14) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 48.709572ms)
Aug 18 17:52:30.270: INFO: (14) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 48.603071ms)
Aug 18 17:52:30.272: INFO: (14) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 50.719659ms)
Aug 18 17:52:30.272: INFO: (14) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 50.670065ms)
Aug 18 17:52:30.272: INFO: (14) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 51.576687ms)
Aug 18 17:52:30.294: INFO: (15) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 21.589437ms)
Aug 18 17:52:30.297: INFO: (15) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 24.269465ms)
Aug 18 17:52:30.297: INFO: (15) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 24.135183ms)
Aug 18 17:52:30.297: INFO: (15) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 24.532755ms)
Aug 18 17:52:30.297: INFO: (15) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 24.656271ms)
Aug 18 17:52:30.297: INFO: (15) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 24.625899ms)
Aug 18 17:52:30.297: INFO: (15) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 24.651273ms)
Aug 18 17:52:30.298: INFO: (15) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 24.799302ms)
Aug 18 17:52:30.298: INFO: (15) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 25.065203ms)
Aug 18 17:52:30.298: INFO: (15) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 25.517618ms)
Aug 18 17:52:30.312: INFO: (15) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 39.450541ms)
Aug 18 17:52:30.321: INFO: (15) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 48.27171ms)
Aug 18 17:52:30.322: INFO: (15) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 49.137996ms)
Aug 18 17:52:30.323: INFO: (15) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 50.271407ms)
Aug 18 17:52:30.326: INFO: (15) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 52.74781ms)
Aug 18 17:52:30.326: INFO: (15) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 52.781596ms)
Aug 18 17:52:30.344: INFO: (16) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 17.739536ms)
Aug 18 17:52:30.345: INFO: (16) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 19.135809ms)
Aug 18 17:52:30.346: INFO: (16) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 19.877247ms)
Aug 18 17:52:30.346: INFO: (16) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 19.662723ms)
Aug 18 17:52:30.346: INFO: (16) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 19.806699ms)
Aug 18 17:52:30.348: INFO: (16) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 21.891819ms)
Aug 18 17:52:30.350: INFO: (16) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 23.809493ms)
Aug 18 17:52:30.350: INFO: (16) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 23.887524ms)
Aug 18 17:52:30.350: INFO: (16) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 23.797136ms)
Aug 18 17:52:30.350: INFO: (16) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 23.815823ms)
Aug 18 17:52:30.391: INFO: (16) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 64.330048ms)
Aug 18 17:52:30.391: INFO: (16) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 64.56845ms)
Aug 18 17:52:30.391: INFO: (16) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 64.652578ms)
Aug 18 17:52:30.391: INFO: (16) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 64.954862ms)
Aug 18 17:52:30.391: INFO: (16) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 65.374811ms)
Aug 18 17:52:30.391: INFO: (16) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 65.464314ms)
Aug 18 17:52:30.415: INFO: (17) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 22.736011ms)
Aug 18 17:52:30.415: INFO: (17) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 22.97512ms)
Aug 18 17:52:30.415: INFO: (17) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 23.153956ms)
Aug 18 17:52:30.415: INFO: (17) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 23.252435ms)
Aug 18 17:52:30.415: INFO: (17) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 23.535681ms)
Aug 18 17:52:30.415: INFO: (17) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 23.764473ms)
Aug 18 17:52:30.417: INFO: (17) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 25.132947ms)
Aug 18 17:52:30.417: INFO: (17) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.373571ms)
Aug 18 17:52:30.417: INFO: (17) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 25.404177ms)
Aug 18 17:52:30.417: INFO: (17) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 25.521649ms)
Aug 18 17:52:30.425: INFO: (17) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 33.145772ms)
Aug 18 17:52:30.435: INFO: (17) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 43.375103ms)
Aug 18 17:52:30.437: INFO: (17) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 45.57212ms)
Aug 18 17:52:30.437: INFO: (17) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 45.761896ms)
Aug 18 17:52:30.437: INFO: (17) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 45.878825ms)
Aug 18 17:52:30.438: INFO: (17) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 45.707604ms)
Aug 18 17:52:30.458: INFO: (18) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 20.027878ms)
Aug 18 17:52:30.459: INFO: (18) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 21.143788ms)
Aug 18 17:52:30.459: INFO: (18) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 21.598686ms)
Aug 18 17:52:30.459: INFO: (18) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 21.580295ms)
Aug 18 17:52:30.459: INFO: (18) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 21.350907ms)
Aug 18 17:52:30.460: INFO: (18) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 21.987587ms)
Aug 18 17:52:30.460: INFO: (18) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 21.954834ms)
Aug 18 17:52:30.460: INFO: (18) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 21.94504ms)
Aug 18 17:52:30.464: INFO: (18) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 25.887749ms)
Aug 18 17:52:30.464: INFO: (18) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 26.349257ms)
Aug 18 17:52:30.476: INFO: (18) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 38.05608ms)
Aug 18 17:52:30.485: INFO: (18) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 47.098034ms)
Aug 18 17:52:30.486: INFO: (18) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 47.848167ms)
Aug 18 17:52:30.486: INFO: (18) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 48.43333ms)
Aug 18 17:52:30.487: INFO: (18) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 48.305242ms)
Aug 18 17:52:30.487: INFO: (18) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 48.646799ms)
Aug 18 17:52:30.505: INFO: (19) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 17.382942ms)
Aug 18 17:52:30.508: INFO: (19) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:460/proxy/: tls baz (200; 20.237158ms)
Aug 18 17:52:30.508: INFO: (19) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">... (200; 20.224279ms)
Aug 18 17:52:30.508: INFO: (19) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:1080/proxy/rewriteme">test<... (200; 20.922298ms)
Aug 18 17:52:30.508: INFO: (19) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 21.279137ms)
Aug 18 17:52:30.509: INFO: (19) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:462/proxy/: tls qux (200; 22.312577ms)
Aug 18 17:52:30.510: INFO: (19) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n/proxy/rewriteme">test</a> (200; 22.833528ms)
Aug 18 17:52:30.513: INFO: (19) /api/v1/namespaces/proxy-9241/pods/proxy-service-2x7tj-vpm5n:160/proxy/: foo (200; 24.891567ms)
Aug 18 17:52:30.513: INFO: (19) /api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/: <a href="/api/v1/namespaces/proxy-9241/pods/https:proxy-service-2x7tj-vpm5n:443/proxy/tlsrewritem... (200; 25.281264ms)
Aug 18 17:52:30.513: INFO: (19) /api/v1/namespaces/proxy-9241/pods/http:proxy-service-2x7tj-vpm5n:162/proxy/: bar (200; 25.798624ms)
Aug 18 17:52:30.525: INFO: (19) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname2/proxy/: bar (200; 37.92793ms)
Aug 18 17:52:30.537: INFO: (19) /api/v1/namespaces/proxy-9241/services/proxy-service-2x7tj:portname1/proxy/: foo (200; 49.759017ms)
Aug 18 17:52:30.537: INFO: (19) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname1/proxy/: foo (200; 50.498264ms)
Aug 18 17:52:30.538: INFO: (19) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname2/proxy/: tls qux (200; 50.781801ms)
Aug 18 17:52:30.538: INFO: (19) /api/v1/namespaces/proxy-9241/services/https:proxy-service-2x7tj:tlsportname1/proxy/: tls baz (200; 51.14903ms)
Aug 18 17:52:30.539: INFO: (19) /api/v1/namespaces/proxy-9241/services/http:proxy-service-2x7tj:portname2/proxy/: bar (200; 50.941961ms)
STEP: deleting ReplicationController proxy-service-2x7tj in namespace proxy-9241, will wait for the garbage collector to delete the pods
Aug 18 17:52:30.634: INFO: Deleting ReplicationController proxy-service-2x7tj took: 32.491047ms
Aug 18 17:52:30.935: INFO: Terminating ReplicationController proxy-service-2x7tj pods took: 300.416773ms
[AfterEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:52:41.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9241" for this suite.

• [SLOW TEST:16.520 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":35,"skipped":672,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:52:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-a33598fc-0365-4f44-968d-64a4e13455e5
STEP: Creating a pod to test consume configMaps
Aug 18 17:52:41.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07" in namespace "projected-3687" to be "success or failure"
Aug 18 17:52:41.831: INFO: Pod "pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.778443ms
Aug 18 17:52:43.843: INFO: Pod "pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02570771s
STEP: Saw pod success
Aug 18 17:52:43.843: INFO: Pod "pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07" satisfied condition "success or failure"
Aug 18 17:52:43.857: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 17:52:43.960: INFO: Waiting for pod pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07 to disappear
Aug 18 17:52:43.973: INFO: Pod pod-projected-configmaps-67b4788f-ecbc-4775-814a-15fc14850a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:52:43.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3687" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":674,"failed":0}
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:52:44.036: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6423, will wait for the garbage collector to delete the pods
Aug 18 17:52:48.457: INFO: Deleting Job.batch foo took: 51.928521ms
Aug 18 17:52:48.658: INFO: Terminating Job.batch foo pods took: 200.249769ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:53:31.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6423" for this suite.

• [SLOW TEST:48.004 seconds]
[sig-apps] Job
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":37,"skipped":675,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:53:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6160
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:53:32.317: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 18 17:53:36.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 create -f -'
Aug 18 17:53:36.798: INFO: stderr: ""
Aug 18 17:53:36.799: INFO: stdout: "e2e-test-crd-publish-openapi-6788-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 18 17:53:36.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 delete e2e-test-crd-publish-openapi-6788-crds test-foo'
Aug 18 17:53:37.129: INFO: stderr: ""
Aug 18 17:53:37.129: INFO: stdout: "e2e-test-crd-publish-openapi-6788-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 18 17:53:37.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 apply -f -'
Aug 18 17:53:37.951: INFO: stderr: ""
Aug 18 17:53:37.951: INFO: stdout: "e2e-test-crd-publish-openapi-6788-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 18 17:53:37.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 delete e2e-test-crd-publish-openapi-6788-crds test-foo'
Aug 18 17:53:38.130: INFO: stderr: ""
Aug 18 17:53:38.130: INFO: stdout: "e2e-test-crd-publish-openapi-6788-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 18 17:53:38.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 create -f -'
Aug 18 17:53:38.299: INFO: rc: 1
Aug 18 17:53:38.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 apply -f -'
Aug 18 17:53:38.648: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 18 17:53:38.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 create -f -'
Aug 18 17:53:38.988: INFO: rc: 1
Aug 18 17:53:38.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6160 apply -f -'
Aug 18 17:53:39.341: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 18 17:53:39.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-6788-crds'
Aug 18 17:53:39.655: INFO: stderr: ""
Aug 18 17:53:39.655: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6788-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 18 17:53:39.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-6788-crds.metadata'
Aug 18 17:53:39.841: INFO: stderr: ""
Aug 18 17:53:39.841: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6788-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 18 17:53:39.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-6788-crds.spec'
Aug 18 17:53:40.134: INFO: stderr: ""
Aug 18 17:53:40.134: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6788-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 18 17:53:40.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-6788-crds.spec.bars'
Aug 18 17:53:40.454: INFO: stderr: ""
Aug 18 17:53:40.454: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6788-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 18 17:53:40.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-6788-crds.spec.bars2'
Aug 18 17:53:40.831: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:53:44.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6160" for this suite.

• [SLOW TEST:12.649 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":38,"skipped":696,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:53:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-fecccefa-7e1f-4e81-bfc4-b6e269b8edf0
STEP: Creating secret with name secret-projected-all-test-volume-9c7c7389-8045-4aea-9a58-352f3552dfa1
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 18 17:53:45.069: INFO: Waiting up to 5m0s for pod "projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70" in namespace "projected-6539" to be "success or failure"
Aug 18 17:53:45.082: INFO: Pod "projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70": Phase="Pending", Reason="", readiness=false. Elapsed: 13.095544ms
Aug 18 17:53:47.095: INFO: Pod "projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026784818s
STEP: Saw pod success
Aug 18 17:53:47.096: INFO: Pod "projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70" satisfied condition "success or failure"
Aug 18 17:53:47.109: INFO: Trying to get logs from node 10.13.3.84 pod projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 18 17:53:47.209: INFO: Waiting for pod projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70 to disappear
Aug 18 17:53:47.222: INFO: Pod projected-volume-48d50f11-46e3-4152-ab68-f5786b5f6c70 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:53:47.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6539" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":39,"skipped":699,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:53:47.293: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 18 17:53:50.448: INFO: Successfully updated pod "annotationupdate2889f1dd-e572-43c9-b79a-2e1bff8fafb9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:53:52.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7276" for this suite.

• [SLOW TEST:5.587 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:53:52.882: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7860
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 18 17:54:03.452: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:54:03.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0818 17:54:03.452902      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7860" for this suite.

• [SLOW TEST:10.629 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":41,"skipped":771,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:54:03.511: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 17:54:03.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5" in namespace "projected-6811" to be "success or failure"
Aug 18 17:54:03.868: INFO: Pod "downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.110324ms
Aug 18 17:54:05.904: INFO: Pod "downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05146396s
Aug 18 17:54:07.921: INFO: Pod "downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068181658s
STEP: Saw pod success
Aug 18 17:54:07.921: INFO: Pod "downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5" satisfied condition "success or failure"
Aug 18 17:54:07.937: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5 container client-container: <nil>
STEP: delete the pod
Aug 18 17:54:08.048: INFO: Waiting for pod downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5 to disappear
Aug 18 17:54:08.062: INFO: Pod downwardapi-volume-20944105-77ba-4fdb-a811-f5d3766333a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:54:08.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6811" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":776,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:54:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:54:08.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1667" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":43,"skipped":789,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:54:08.877: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 18 17:54:09.169: INFO: PodSpec: initContainers in spec.initContainers
Aug 18 17:54:55.000: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-75a7dbdc-6588-4886-a7a9-c0f2970ae06a", GenerateName:"", Namespace:"init-container-1559", SelfLink:"/api/v1/namespaces/init-container-1559/pods/pod-init-75a7dbdc-6588-4886-a7a9-c0f2970ae06a", UID:"3395c8c3-9028-44f4-bdb6-1bc00d7b783e", ResourceVersion:"33286", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63733370049, loc:(*time.Location)(0x7931640)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"169616024"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7crss", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003c9f3c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7crss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7crss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7crss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003157f58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.13.3.84", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003b2e180), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003157ff0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c167d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004c167d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004c167dc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370049, loc:(*time.Location)(0x7931640)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370049, loc:(*time.Location)(0x7931640)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370049, loc:(*time.Location)(0x7931640)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370049, loc:(*time.Location)(0x7931640)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.13.3.84", PodIP:"172.30.14.229", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.14.229"}}, StartTime:(*v1.Time)(0xc003b97b80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00312cfc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00312d030)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://6c0428dafcb0fe934d49ac7e4716682858b00d1eac7f70ffff83839ccea8f849", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b97bc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b97ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004c1691f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:54:55.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1559" for this suite.

• [SLOW TEST:46.197 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":44,"skipped":790,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:54:55.076: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:54:56.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370096, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370096, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370096, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370096, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:54:59.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:54:59.840: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9048-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:06.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5096" for this suite.
STEP: Destroying namespace "webhook-5096-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.198 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":45,"skipped":805,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:07.274: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-0af2ea55-2fb3-451e-a613-843af3bd1d65
STEP: Creating a pod to test consume configMaps
Aug 18 17:55:07.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482" in namespace "configmap-1515" to be "success or failure"
Aug 18 17:55:07.622: INFO: Pod "pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482": Phase="Pending", Reason="", readiness=false. Elapsed: 11.34143ms
Aug 18 17:55:09.637: INFO: Pod "pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026316038s
Aug 18 17:55:11.656: INFO: Pod "pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045364397s
STEP: Saw pod success
Aug 18 17:55:11.656: INFO: Pod "pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482" satisfied condition "success or failure"
Aug 18 17:55:11.673: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 17:55:11.766: INFO: Waiting for pod pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482 to disappear
Aug 18 17:55:11.780: INFO: Pod pod-configmaps-119eacd8-4a1e-4c1f-b24f-756340d7c482 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:11.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1515" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":805,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:11.848: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 17:55:12.161: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 18 17:55:17.177: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 18 17:55:17.177: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 18 17:55:19.193: INFO: Creating deployment "test-rollover-deployment"
Aug 18 17:55:19.246: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 18 17:55:21.309: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 18 17:55:21.449: INFO: Ensure that both replica sets have 1 created replica
Aug 18 17:55:21.480: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 18 17:55:21.565: INFO: Updating deployment test-rollover-deployment
Aug 18 17:55:21.565: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 18 17:55:23.615: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 18 17:55:23.679: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 18 17:55:23.729: INFO: all replica sets need to contain the pod-template-hash label
Aug 18 17:55:23.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370123, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:55:25.991: INFO: all replica sets need to contain the pod-template-hash label
Aug 18 17:55:25.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370123, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:55:27.779: INFO: all replica sets need to contain the pod-template-hash label
Aug 18 17:55:27.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370123, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:55:29.774: INFO: all replica sets need to contain the pod-template-hash label
Aug 18 17:55:29.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370123, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:55:31.774: INFO: all replica sets need to contain the pod-template-hash label
Aug 18 17:55:31.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370123, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370119, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:55:33.771: INFO: 
Aug 18 17:55:33.771: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 18 17:55:33.832: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2307 /apis/apps/v1/namespaces/deployment-2307/deployments/test-rollover-deployment 66536619-d835-42bb-9989-df3c475d4198 33627 2 2020-08-18 17:55:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d0b1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-18 17:55:19 +0000 UTC,LastTransitionTime:2020-08-18 17:55:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-08-18 17:55:33 +0000 UTC,LastTransitionTime:2020-08-18 17:55:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 18 17:55:33.849: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-2307 /apis/apps/v1/namespaces/deployment-2307/replicasets/test-rollover-deployment-574d6dfbff 05c1e0e5-71a4-4d32-84b6-ef901367b41c 33617 2 2020-08-18 17:55:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 66536619-d835-42bb-9989-df3c475d4198 0xc002d0ba77 0xc002d0ba78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d0bb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 18 17:55:33.849: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 18 17:55:33.849: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2307 /apis/apps/v1/namespaces/deployment-2307/replicasets/test-rollover-controller 8971f486-cc67-4543-bed1-3807bfd6d276 33626 2 2020-08-18 17:55:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 66536619-d835-42bb-9989-df3c475d4198 0xc002d0b937 0xc002d0b938}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d0b9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 18 17:55:33.849: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-2307 /apis/apps/v1/namespaces/deployment-2307/replicasets/test-rollover-deployment-f6c94f66c 6e27d60e-50a3-4cfa-a96f-f167e66fbe7f 33571 2 2020-08-18 17:55:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 66536619-d835-42bb-9989-df3c475d4198 0xc002d0bc30 0xc002d0bc31}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d0bd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 18 17:55:33.864: INFO: Pod "test-rollover-deployment-574d6dfbff-jvzjd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-jvzjd test-rollover-deployment-574d6dfbff- deployment-2307 /api/v1/namespaces/deployment-2307/pods/test-rollover-deployment-574d6dfbff-jvzjd ba88a008-808e-46c6-b2e7-9d8537303afa 33587 0 2020-08-18 17:55:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 05c1e0e5-71a4-4d32-84b6-ef901367b41c 0xc002d965c7 0xc002d965c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4ghxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4ghxw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4ghxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 17:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 17:55:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 17:55:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 17:55:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.234,StartTime:2020-08-18 17:55:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 17:55:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://ccaa5a427426e3fe2f1f587d09dcc583b6c833c4595b13d5f2083f204c991120,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:33.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2307" for this suite.

• [SLOW TEST:22.087 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":47,"skipped":816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:33.936: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Aug 18 17:55:34.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 cluster-info'
Aug 18 17:55:34.346: INFO: stderr: ""
Aug 18 17:55:34.346: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mNodeLocalDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/node-local-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:34.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-297" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":48,"skipped":857,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:34.409: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 17:55:34.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68" in namespace "downward-api-7026" to be "success or failure"
Aug 18 17:55:34.784: INFO: Pod "downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68": Phase="Pending", Reason="", readiness=false. Elapsed: 15.231478ms
Aug 18 17:55:36.801: INFO: Pod "downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68": Phase="Running", Reason="", readiness=true. Elapsed: 2.031390302s
Aug 18 17:55:38.816: INFO: Pod "downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047182115s
STEP: Saw pod success
Aug 18 17:55:38.816: INFO: Pod "downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68" satisfied condition "success or failure"
Aug 18 17:55:38.830: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68 container client-container: <nil>
STEP: delete the pod
Aug 18 17:55:38.932: INFO: Waiting for pod downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68 to disappear
Aug 18 17:55:38.947: INFO: Pod downwardapi-volume-37662872-6cbe-4400-b0eb-8f1f3bf98f68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:38.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7026" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":49,"skipped":871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:39.020: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:39.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9108" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":50,"skipped":945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:39.413: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-1a93369d-78a3-4301-bfc8-28c076b35836
STEP: Creating a pod to test consume configMaps
Aug 18 17:55:40.010: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338" in namespace "projected-6600" to be "success or failure"
Aug 18 17:55:40.025: INFO: Pod "pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338": Phase="Pending", Reason="", readiness=false. Elapsed: 14.418559ms
Aug 18 17:55:42.040: INFO: Pod "pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029462713s
Aug 18 17:55:44.053: INFO: Pod "pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042547009s
STEP: Saw pod success
Aug 18 17:55:44.053: INFO: Pod "pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338" satisfied condition "success or failure"
Aug 18 17:55:44.066: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 17:55:44.148: INFO: Waiting for pod pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338 to disappear
Aug 18 17:55:44.164: INFO: Pod pod-projected-configmaps-6b31e441-9714-45b6-9935-a11b9a522338 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:44.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6600" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":51,"skipped":978,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:44.237: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1490
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 17:55:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1564'
Aug 18 17:55:44.675: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 18 17:55:44.675: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1496
Aug 18 17:55:46.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1564'
Aug 18 17:55:46.828: INFO: stderr: ""
Aug 18 17:55:46.829: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:55:46.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1564" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":52,"skipped":991,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:55:46.918: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1760
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 17:55:47.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-617'
Aug 18 17:55:47.365: INFO: stderr: ""
Aug 18 17:55:47.365: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1765
Aug 18 17:55:47.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete pods e2e-test-httpd-pod --namespace=kubectl-617'
Aug 18 17:56:01.708: INFO: stderr: ""
Aug 18 17:56:01.708: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:56:01.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-617" for this suite.

• [SLOW TEST:14.824 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1756
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":53,"skipped":1002,"failed":0}
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:56:01.742: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 18 17:56:02.103: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8389 /api/v1/namespaces/watch-8389/configmaps/e2e-watch-test-label-changed 9e864b2d-8f4d-4f58-9f61-2a519581d551 33943 0 2020-08-18 17:56:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 18 17:56:02.103: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8389 /api/v1/namespaces/watch-8389/configmaps/e2e-watch-test-label-changed 9e864b2d-8f4d-4f58-9f61-2a519581d551 33944 0 2020-08-18 17:56:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 18 17:56:02.104: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8389 /api/v1/namespaces/watch-8389/configmaps/e2e-watch-test-label-changed 9e864b2d-8f4d-4f58-9f61-2a519581d551 33945 0 2020-08-18 17:56:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 18 17:56:12.457: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8389 /api/v1/namespaces/watch-8389/configmaps/e2e-watch-test-label-changed 9e864b2d-8f4d-4f58-9f61-2a519581d551 33989 0 2020-08-18 17:56:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 18 17:56:12.458: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8389 /api/v1/namespaces/watch-8389/configmaps/e2e-watch-test-label-changed 9e864b2d-8f4d-4f58-9f61-2a519581d551 33990 0 2020-08-18 17:56:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 18 17:56:12.458: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8389 /api/v1/namespaces/watch-8389/configmaps/e2e-watch-test-label-changed 9e864b2d-8f4d-4f58-9f61-2a519581d551 33991 0 2020-08-18 17:56:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:56:12.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8389" for this suite.

• [SLOW TEST:10.790 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":54,"skipped":1002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:56:12.533: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 18 17:56:12.865: INFO: Waiting up to 5m0s for pod "pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1" in namespace "emptydir-2656" to be "success or failure"
Aug 18 17:56:12.882: INFO: Pod "pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.398113ms
Aug 18 17:56:14.898: INFO: Pod "pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033399091s
Aug 18 17:56:16.914: INFO: Pod "pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048719298s
STEP: Saw pod success
Aug 18 17:56:16.914: INFO: Pod "pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1" satisfied condition "success or failure"
Aug 18 17:56:16.929: INFO: Trying to get logs from node 10.13.3.84 pod pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1 container test-container: <nil>
STEP: delete the pod
Aug 18 17:56:17.019: INFO: Waiting for pod pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1 to disappear
Aug 18 17:56:17.035: INFO: Pod pod-8b224cf8-ab1a-419b-bdea-cfc218e5fbf1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:56:17.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2656" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":1041,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:56:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 17:56:18.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 17:56:20.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733370178, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 17:56:23.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:56:24.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5928" for this suite.
STEP: Destroying namespace "webhook-5928-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":56,"skipped":1051,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:56:25.180: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-2215
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 17:56:26.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2215" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":57,"skipped":1061,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 17:56:26.241: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6096
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-6096
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6096
Aug 18 17:56:26.580: INFO: Found 0 stateful pods, waiting for 1
Aug 18 17:56:36.595: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 18 17:56:36.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 17:56:36.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 17:56:36.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 17:56:36.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 17:56:36.955: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 18 17:56:46.967: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 17:56:46.967: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 17:56:47.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998299s
Aug 18 17:56:48.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.951259508s
Aug 18 17:56:49.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.93657788s
Aug 18 17:56:50.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.915884216s
Aug 18 17:56:51.134: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.897975391s
Aug 18 17:56:52.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.883341095s
Aug 18 17:56:53.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.86847732s
Aug 18 17:56:54.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.854036555s
Aug 18 17:56:55.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.838623452s
Aug 18 17:56:56.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 822.048657ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6096
Aug 18 17:56:57.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:56:57.594: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 17:56:57.594: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 17:56:57.594: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 17:56:57.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:56:57.875: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 18 17:56:57.875: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 17:56:57.875: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 17:56:57.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:56:58.220: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 18 17:56:58.220: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 17:56:58.220: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 17:56:58.235: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 17:56:58.235: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 17:56:58.235: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 18 17:56:58.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 17:56:58.490: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 17:56:58.490: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 17:56:58.490: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 17:56:58.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 17:56:58.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 17:56:58.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 17:56:58.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 17:56:58.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 17:56:59.184: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 17:56:59.184: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 17:56:59.184: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 17:56:59.184: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 17:56:59.200: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 18 17:57:09.237: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 17:57:09.237: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 17:57:09.237: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 17:57:09.291: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:09.291: INFO: ss-0  10.13.3.84   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:26 +0000 UTC  }]
Aug 18 17:57:09.291: INFO: ss-1  10.13.3.114  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:09.291: INFO: ss-2  10.13.3.115  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:09.291: INFO: 
Aug 18 17:57:09.291: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 18 17:57:10.317: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:10.317: INFO: ss-0  10.13.3.84   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:26 +0000 UTC  }]
Aug 18 17:57:10.317: INFO: ss-1  10.13.3.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:10.317: INFO: ss-2  10.13.3.115  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:10.317: INFO: 
Aug 18 17:57:10.317: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 18 17:57:11.334: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:11.334: INFO: ss-0  10.13.3.84   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:26 +0000 UTC  }]
Aug 18 17:57:11.334: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:11.334: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:11.334: INFO: 
Aug 18 17:57:11.334: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 18 17:57:12.353: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:12.353: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:12.353: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:12.353: INFO: 
Aug 18 17:57:12.353: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 18 17:57:13.367: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:13.367: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:13.367: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:13.367: INFO: 
Aug 18 17:57:13.367: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 18 17:57:14.382: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:14.382: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:14.382: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:14.382: INFO: 
Aug 18 17:57:14.382: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 18 17:57:15.398: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:15.398: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:15.398: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:15.398: INFO: 
Aug 18 17:57:15.398: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 18 17:57:16.413: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:16.413: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:16.413: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:16.413: INFO: 
Aug 18 17:57:16.413: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 18 17:57:17.427: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:17.427: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:17.427: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:17.428: INFO: 
Aug 18 17:57:17.428: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 18 17:57:18.867: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Aug 18 17:57:18.867: INFO: ss-1  10.13.3.114  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:18.867: INFO: ss-2  10.13.3.115  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-18 17:56:47 +0000 UTC  }]
Aug 18 17:57:18.867: INFO: 
Aug 18 17:57:18.867: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6096
Aug 18 17:57:19.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:57:20.033: INFO: rc: 1
Aug 18 17:57:20.033: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:57:30.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:57:30.200: INFO: rc: 1
Aug 18 17:57:30.201: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:57:40.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:57:40.341: INFO: rc: 1
Aug 18 17:57:40.341: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:57:50.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:57:50.461: INFO: rc: 1
Aug 18 17:57:50.461: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:58:00.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:58:00.595: INFO: rc: 1
Aug 18 17:58:00.596: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:58:10.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:58:10.730: INFO: rc: 1
Aug 18 17:58:10.730: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:58:20.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:58:20.850: INFO: rc: 1
Aug 18 17:58:20.851: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:58:30.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:58:30.966: INFO: rc: 1
Aug 18 17:58:30.966: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:58:40.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:58:41.101: INFO: rc: 1
Aug 18 17:58:41.101: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:58:51.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:58:51.230: INFO: rc: 1
Aug 18 17:58:51.230: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:59:01.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:59:01.345: INFO: rc: 1
Aug 18 17:59:01.345: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:59:11.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:59:11.487: INFO: rc: 1
Aug 18 17:59:11.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:59:21.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:59:21.680: INFO: rc: 1
Aug 18 17:59:21.681: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:59:31.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:59:31.784: INFO: rc: 1
Aug 18 17:59:31.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:59:41.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:59:41.906: INFO: rc: 1
Aug 18 17:59:41.906: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 17:59:51.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 17:59:52.017: INFO: rc: 1
Aug 18 17:59:52.017: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:00:02.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:00:02.155: INFO: rc: 1
Aug 18 18:00:02.155: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:00:12.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:00:12.320: INFO: rc: 1
Aug 18 18:00:12.320: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:00:22.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:00:22.437: INFO: rc: 1
Aug 18 18:00:22.437: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:00:32.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:00:32.573: INFO: rc: 1
Aug 18 18:00:32.573: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:00:42.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:00:42.723: INFO: rc: 1
Aug 18 18:00:42.723: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:00:52.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:00:52.852: INFO: rc: 1
Aug 18 18:00:52.852: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:01:02.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:01:02.971: INFO: rc: 1
Aug 18 18:01:02.971: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:01:12.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:01:13.100: INFO: rc: 1
Aug 18 18:01:13.100: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:01:23.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:01:23.234: INFO: rc: 1
Aug 18 18:01:23.234: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:01:33.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:01:33.368: INFO: rc: 1
Aug 18 18:01:33.368: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:01:43.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:01:43.486: INFO: rc: 1
Aug 18 18:01:43.486: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:01:53.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:01:53.631: INFO: rc: 1
Aug 18 18:01:53.631: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:02:03.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:02:03.767: INFO: rc: 1
Aug 18 18:02:03.767: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:02:13.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:02:13.914: INFO: rc: 1
Aug 18 18:02:13.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 18 18:02:23.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6096 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:02:24.045: INFO: rc: 1
Aug 18 18:02:24.046: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Aug 18 18:02:24.046: INFO: Scaling statefulset ss to 0
Aug 18 18:02:24.112: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 18 18:02:24.125: INFO: Deleting all statefulset in ns statefulset-6096
Aug 18 18:02:24.141: INFO: Scaling statefulset ss to 0
Aug 18 18:02:24.195: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:02:24.210: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:02:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6096" for this suite.

• [SLOW TEST:358.106 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":58,"skipped":1067,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:02:24.348: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:02:41.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4507" for this suite.

• [SLOW TEST:17.042 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":59,"skipped":1076,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:02:41.390: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5292.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5292.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:02:53.888: INFO: DNS probes using dns-test-8c6f42e5-6b8a-4bb8-a766-82b9a23506f7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5292.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5292.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:02:56.067: INFO: File wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:02:56.094: INFO: File jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:02:56.094: INFO: Lookups using dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e failed for: [wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local]

Aug 18 18:03:01.116: INFO: File wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:01.134: INFO: File jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:01.134: INFO: Lookups using dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e failed for: [wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local]

Aug 18 18:03:06.147: INFO: File wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:06.168: INFO: File jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:06.168: INFO: Lookups using dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e failed for: [wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local]

Aug 18 18:03:11.116: INFO: File wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:11.137: INFO: File jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:11.137: INFO: Lookups using dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e failed for: [wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local]

Aug 18 18:03:16.114: INFO: File wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:16.496: INFO: File jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local from pod  dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 18 18:03:16.496: INFO: Lookups using dns-5292/dns-test-4edabdab-ac46-4a62-a224-0434791caa0e failed for: [wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local]

Aug 18 18:03:21.144: INFO: DNS probes using dns-test-4edabdab-ac46-4a62-a224-0434791caa0e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5292.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5292.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5292.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5292.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:03:25.454: INFO: DNS probes using dns-test-f2f0ae02-1500-4120-b559-17cc42af1cbc succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:03:25.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5292" for this suite.

• [SLOW TEST:44.383 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":60,"skipped":1080,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:03:25.774: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:325
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Aug 18 18:03:26.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-6464'
Aug 18 18:03:26.459: INFO: stderr: ""
Aug 18 18:03:26.459: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 18 18:03:26.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6464'
Aug 18 18:03:26.566: INFO: stderr: ""
Aug 18 18:03:26.566: INFO: stdout: "update-demo-nautilus-72pnm update-demo-nautilus-89psf "
Aug 18 18:03:26.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-72pnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6464'
Aug 18 18:03:26.681: INFO: stderr: ""
Aug 18 18:03:26.681: INFO: stdout: ""
Aug 18 18:03:26.681: INFO: update-demo-nautilus-72pnm is created but not running
Aug 18 18:03:31.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6464'
Aug 18 18:03:31.800: INFO: stderr: ""
Aug 18 18:03:31.800: INFO: stdout: "update-demo-nautilus-72pnm update-demo-nautilus-89psf "
Aug 18 18:03:31.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-72pnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6464'
Aug 18 18:03:31.895: INFO: stderr: ""
Aug 18 18:03:31.895: INFO: stdout: "true"
Aug 18 18:03:31.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-72pnm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6464'
Aug 18 18:03:32.012: INFO: stderr: ""
Aug 18 18:03:32.013: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:03:32.013: INFO: validating pod update-demo-nautilus-72pnm
Aug 18 18:03:32.047: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:03:32.047: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:03:32.047: INFO: update-demo-nautilus-72pnm is verified up and running
Aug 18 18:03:32.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-89psf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6464'
Aug 18 18:03:32.164: INFO: stderr: ""
Aug 18 18:03:32.164: INFO: stdout: "true"
Aug 18 18:03:32.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-89psf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6464'
Aug 18 18:03:32.261: INFO: stderr: ""
Aug 18 18:03:32.261: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:03:32.261: INFO: validating pod update-demo-nautilus-89psf
Aug 18 18:03:32.287: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:03:32.287: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:03:32.287: INFO: update-demo-nautilus-89psf is verified up and running
STEP: using delete to clean up resources
Aug 18 18:03:32.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-6464'
Aug 18 18:03:32.428: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:03:32.428: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 18 18:03:32.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6464'
Aug 18 18:03:32.568: INFO: stderr: "No resources found in kubectl-6464 namespace.\n"
Aug 18 18:03:32.568: INFO: stdout: ""
Aug 18 18:03:32.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -l name=update-demo --namespace=kubectl-6464 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 18 18:03:32.856: INFO: stderr: ""
Aug 18 18:03:32.856: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:03:32.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6464" for this suite.

• [SLOW TEST:7.293 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:323
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":61,"skipped":1090,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:03:33.068: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Aug 18 18:03:33.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-5994'
Aug 18 18:03:34.149: INFO: stderr: ""
Aug 18 18:03:34.149: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 18 18:03:35.164: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:03:35.164: INFO: Found 0 / 1
Aug 18 18:03:36.165: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:03:36.165: INFO: Found 1 / 1
Aug 18 18:03:36.165: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 18 18:03:36.184: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:03:36.184: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 18 18:03:36.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 patch pod agnhost-master-k974c --namespace=kubectl-5994 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 18 18:03:36.324: INFO: stderr: ""
Aug 18 18:03:36.324: INFO: stdout: "pod/agnhost-master-k974c patched\n"
STEP: checking annotations
Aug 18 18:03:36.336: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:03:36.336: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:03:36.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5994" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":62,"skipped":1095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:03:36.437: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 18 18:03:44.889: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:03:44.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8374" for this suite.

• [SLOW TEST:21.374 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":1133,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:03:57.812: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Aug 18 18:04:05.511: INFO: Waiting up to 5m0s for pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6" in namespace "containers-6238" to be "success or failure"
Aug 18 18:04:05.524: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.532098ms
Aug 18 18:04:07.541: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030434587s
Aug 18 18:04:09.558: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046734819s
Aug 18 18:04:11.572: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061198202s
Aug 18 18:04:13.586: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074838288s
Aug 18 18:04:15.600: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089359723s
Aug 18 18:04:17.615: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10363011s
Aug 18 18:04:19.628: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.117525057s
Aug 18 18:04:21.649: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.137927903s
Aug 18 18:04:23.664: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.153423069s
STEP: Saw pod success
Aug 18 18:04:23.664: INFO: Pod "client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6" satisfied condition "success or failure"
Aug 18 18:04:23.679: INFO: Trying to get logs from node 10.13.3.84 pod client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6 container test-container: <nil>
STEP: delete the pod
Aug 18 18:04:23.843: INFO: Waiting for pod client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6 to disappear
Aug 18 18:04:23.856: INFO: Pod client-containers-292d9694-a1cf-4721-aa21-1263f8f03ec6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:04:23.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6238" for this suite.

• [SLOW TEST:26.097 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":64,"skipped":1142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:04:23.916: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create a job from an image, then delete the job [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Aug 18 18:04:24.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=kubectl-8770 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 18 18:04:27.368: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 18 18:04:27.368: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:04:29.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8770" for this suite.

• [SLOW TEST:5.547 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1843
    should create a job from an image, then delete the job [Deprecated] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job [Deprecated] [Conformance]","total":280,"completed":65,"skipped":1181,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:04:29.464: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:05:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4977" for this suite.

• [SLOW TEST:47.245 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":1198,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:05:16.708: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:05:17.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e" in namespace "downward-api-5714" to be "success or failure"
Aug 18 18:05:17.042: INFO: Pod "downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.562176ms
Aug 18 18:05:19.058: INFO: Pod "downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029401985s
Aug 18 18:05:21.073: INFO: Pod "downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044483977s
STEP: Saw pod success
Aug 18 18:05:21.073: INFO: Pod "downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e" satisfied condition "success or failure"
Aug 18 18:05:21.089: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e container client-container: <nil>
STEP: delete the pod
Aug 18 18:05:21.172: INFO: Waiting for pod downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e to disappear
Aug 18 18:05:21.187: INFO: Pod downwardapi-volume-84edd4aa-d14b-4513-9176-9a9cf24f6f4e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:05:21.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5714" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1210,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:05:21.242: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d in namespace container-probe-9851
Aug 18 18:05:23.573: INFO: Started pod liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d in namespace container-probe-9851
STEP: checking the pod's current state and verifying that restartCount is present
Aug 18 18:05:23.590: INFO: Initial restart count of pod liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d is 0
Aug 18 18:05:35.696: INFO: Restart count of pod container-probe-9851/liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d is now 1 (12.105041033s elapsed)
Aug 18 18:05:55.843: INFO: Restart count of pod container-probe-9851/liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d is now 2 (32.25272758s elapsed)
Aug 18 18:06:16.205: INFO: Restart count of pod container-probe-9851/liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d is now 3 (52.614757585s elapsed)
Aug 18 18:06:36.399: INFO: Restart count of pod container-probe-9851/liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d is now 4 (1m12.808747262s elapsed)
Aug 18 18:07:47.357: INFO: Restart count of pod container-probe-9851/liveness-56fa5f8a-acdc-4082-af9b-49cda18cb32d is now 5 (2m23.766745168s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:07:47.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9851" for this suite.

• [SLOW TEST:146.217 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":1219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:07:47.470: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a job from an image when restart is OnFailure [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 18:07:47.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-860'
Aug 18 18:07:47.862: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 18 18:07:47.862: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Aug 18 18:07:47.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete jobs e2e-test-httpd-job --namespace=kubectl-860'
Aug 18 18:07:47.991: INFO: stderr: ""
Aug 18 18:07:47.991: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:07:47.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-860" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure [Deprecated] [Conformance]","total":280,"completed":69,"skipped":1317,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:07:48.053: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0818 18:08:28.463769      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 18 18:08:28.463: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:08:28.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-716" for this suite.

• [SLOW TEST:40.460 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":70,"skipped":1329,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:08:28.514: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6217
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4ee57a65-2ff5-4a09-9a33-b5f4e6032f58
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4ee57a65-2ff5-4a09-9a33-b5f4e6032f58
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:08:33.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6217" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":71,"skipped":1339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:08:33.086: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Aug 18 18:08:33.350: INFO: namespace kubectl-1314
Aug 18 18:08:33.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-1314'
Aug 18 18:08:33.704: INFO: stderr: ""
Aug 18 18:08:33.704: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 18 18:08:34.722: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:08:34.722: INFO: Found 0 / 1
Aug 18 18:08:35.726: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:08:35.726: INFO: Found 1 / 1
Aug 18 18:08:35.726: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 18 18:08:35.743: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:08:35.743: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 18 18:08:35.743: INFO: wait on agnhost-master startup in kubectl-1314 
Aug 18 18:08:35.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs agnhost-master-jblkj agnhost-master --namespace=kubectl-1314'
Aug 18 18:08:35.900: INFO: stderr: ""
Aug 18 18:08:35.900: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 18 18:08:35.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1314'
Aug 18 18:08:36.074: INFO: stderr: ""
Aug 18 18:08:36.074: INFO: stdout: "service/rm2 exposed\n"
Aug 18 18:08:36.088: INFO: Service rm2 in namespace kubectl-1314 found.
STEP: exposing service
Aug 18 18:08:38.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1314'
Aug 18 18:08:38.295: INFO: stderr: ""
Aug 18 18:08:38.295: INFO: stdout: "service/rm3 exposed\n"
Aug 18 18:08:38.311: INFO: Service rm3 in namespace kubectl-1314 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:08:40.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1314" for this suite.

• [SLOW TEST:7.316 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":72,"skipped":1407,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:08:40.403: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:182
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:08:40.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9320" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":73,"skipped":1421,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:08:40.993: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6714
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:08:41.282: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 18 18:08:45.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6714 create -f -'
Aug 18 18:08:45.583: INFO: stderr: ""
Aug 18 18:08:45.583: INFO: stdout: "e2e-test-crd-publish-openapi-9312-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 18 18:08:45.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6714 delete e2e-test-crd-publish-openapi-9312-crds test-cr'
Aug 18 18:08:45.738: INFO: stderr: ""
Aug 18 18:08:45.738: INFO: stdout: "e2e-test-crd-publish-openapi-9312-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 18 18:08:45.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6714 apply -f -'
Aug 18 18:08:45.972: INFO: stderr: ""
Aug 18 18:08:45.972: INFO: stdout: "e2e-test-crd-publish-openapi-9312-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 18 18:08:45.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6714 delete e2e-test-crd-publish-openapi-9312-crds test-cr'
Aug 18 18:08:46.148: INFO: stderr: ""
Aug 18 18:08:46.148: INFO: stdout: "e2e-test-crd-publish-openapi-9312-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 18 18:08:46.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-9312-crds'
Aug 18 18:08:46.515: INFO: stderr: ""
Aug 18 18:08:46.515: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9312-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:08:50.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6714" for this suite.

• [SLOW TEST:9.342 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":74,"skipped":1427,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:08:50.335: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4642
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4642
STEP: Creating statefulset with conflicting port in namespace statefulset-4642
STEP: Waiting until pod test-pod will start running in namespace statefulset-4642
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4642
Aug 18 18:08:54.919: INFO: Observed stateful pod in namespace: statefulset-4642, name: ss-0, uid: 43961ac6-c5ec-4f14-bfac-5f860c63d32d, status phase: Pending. Waiting for statefulset controller to delete.
Aug 18 18:08:54.942: INFO: Observed stateful pod in namespace: statefulset-4642, name: ss-0, uid: 43961ac6-c5ec-4f14-bfac-5f860c63d32d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 18 18:08:54.969: INFO: Observed stateful pod in namespace: statefulset-4642, name: ss-0, uid: 43961ac6-c5ec-4f14-bfac-5f860c63d32d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 18 18:08:54.991: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4642
STEP: Removing pod with conflicting port in namespace statefulset-4642
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4642 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 18 18:08:59.114: INFO: Deleting all statefulset in ns statefulset-4642
Aug 18 18:08:59.131: INFO: Scaling statefulset ss to 0
Aug 18 18:09:19.208: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:09:19.225: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:09:19.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4642" for this suite.

• [SLOW TEST:29.049 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":75,"skipped":1436,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:09:19.385: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:09:23.922: INFO: DNS probes using dns-7248/dns-test-3b88c75d-7ecf-439e-b0b5-c6f6f52c94cf succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:09:23.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7248" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":76,"skipped":1439,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:09:24.055: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-v8kb
STEP: Creating a pod to test atomic-volume-subpath
Aug 18 18:09:24.860: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v8kb" in namespace "subpath-5567" to be "success or failure"
Aug 18 18:09:24.875: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.25189ms
Aug 18 18:09:26.889: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 2.028627364s
Aug 18 18:09:28.905: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 4.044350892s
Aug 18 18:09:30.920: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 6.059529714s
Aug 18 18:09:32.934: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 8.073916645s
Aug 18 18:09:34.949: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 10.088019592s
Aug 18 18:09:36.968: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 12.107925752s
Aug 18 18:09:38.983: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 14.122566425s
Aug 18 18:09:40.997: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 16.136499372s
Aug 18 18:09:43.013: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 18.152258662s
Aug 18 18:09:45.029: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 20.168004782s
Aug 18 18:09:47.251: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 22.390379004s
Aug 18 18:09:49.497: INFO: Pod "pod-subpath-test-projected-v8kb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.636441451s
STEP: Saw pod success
Aug 18 18:09:49.497: INFO: Pod "pod-subpath-test-projected-v8kb" satisfied condition "success or failure"
Aug 18 18:09:49.509: INFO: Trying to get logs from node 10.13.3.84 pod pod-subpath-test-projected-v8kb container test-container-subpath-projected-v8kb: <nil>
STEP: delete the pod
Aug 18 18:09:49.605: INFO: Waiting for pod pod-subpath-test-projected-v8kb to disappear
Aug 18 18:09:49.618: INFO: Pod pod-subpath-test-projected-v8kb no longer exists
STEP: Deleting pod pod-subpath-test-projected-v8kb
Aug 18 18:09:49.619: INFO: Deleting pod "pod-subpath-test-projected-v8kb" in namespace "subpath-5567"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:09:49.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5567" for this suite.

• [SLOW TEST:25.636 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":77,"skipped":1451,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:09:49.694: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:10:12.042: INFO: Container started at 2020-08-18 18:09:51 +0000 UTC, pod became ready at 2020-08-18 18:10:11 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:10:12.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5843" for this suite.

• [SLOW TEST:22.403 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":78,"skipped":1468,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:10:12.097: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-c3173e59-fb91-46d6-920b-81ab4480a7de in namespace container-probe-5696
Aug 18 18:10:16.861: INFO: Started pod busybox-c3173e59-fb91-46d6-920b-81ab4480a7de in namespace container-probe-5696
STEP: checking the pod's current state and verifying that restartCount is present
Aug 18 18:10:16.874: INFO: Initial restart count of pod busybox-c3173e59-fb91-46d6-920b-81ab4480a7de is 0
Aug 18 18:11:09.287: INFO: Restart count of pod container-probe-5696/busybox-c3173e59-fb91-46d6-920b-81ab4480a7de is now 1 (52.413052858s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:09.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5696" for this suite.

• [SLOW TEST:57.322 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1470,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:09.419: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-jntj
STEP: Creating a pod to test atomic-volume-subpath
Aug 18 18:11:09.738: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jntj" in namespace "subpath-3664" to be "success or failure"
Aug 18 18:11:09.752: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Pending", Reason="", readiness=false. Elapsed: 13.931032ms
Aug 18 18:11:11.762: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 2.024309607s
Aug 18 18:11:13.775: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 4.037731589s
Aug 18 18:11:15.788: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 6.050506358s
Aug 18 18:11:17.803: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 8.065768418s
Aug 18 18:11:19.818: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 10.080106123s
Aug 18 18:11:21.831: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 12.093465329s
Aug 18 18:11:23.844: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 14.106859619s
Aug 18 18:11:25.860: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 16.12221524s
Aug 18 18:11:27.874: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 18.13663511s
Aug 18 18:11:29.887: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 20.149423478s
Aug 18 18:11:31.901: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Running", Reason="", readiness=true. Elapsed: 22.16303356s
Aug 18 18:11:33.916: INFO: Pod "pod-subpath-test-configmap-jntj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.178375672s
STEP: Saw pod success
Aug 18 18:11:33.916: INFO: Pod "pod-subpath-test-configmap-jntj" satisfied condition "success or failure"
Aug 18 18:11:33.932: INFO: Trying to get logs from node 10.13.3.84 pod pod-subpath-test-configmap-jntj container test-container-subpath-configmap-jntj: <nil>
STEP: delete the pod
Aug 18 18:11:34.477: INFO: Waiting for pod pod-subpath-test-configmap-jntj to disappear
Aug 18 18:11:34.498: INFO: Pod pod-subpath-test-configmap-jntj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jntj
Aug 18 18:11:34.498: INFO: Deleting pod "pod-subpath-test-configmap-jntj" in namespace "subpath-3664"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:34.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3664" for this suite.

• [SLOW TEST:25.158 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":80,"skipped":1471,"failed":0}
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:34.577: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-1006
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1006 to expose endpoints map[]
Aug 18 18:11:34.941: INFO: Get endpoints failed (16.490328ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 18 18:11:35.958: INFO: successfully validated that service multi-endpoint-test in namespace services-1006 exposes endpoints map[] (1.033370851s elapsed)
STEP: Creating pod pod1 in namespace services-1006
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1006 to expose endpoints map[pod1:[100]]
Aug 18 18:11:38.097: INFO: successfully validated that service multi-endpoint-test in namespace services-1006 exposes endpoints map[pod1:[100]] (2.107450077s elapsed)
STEP: Creating pod pod2 in namespace services-1006
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1006 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 18 18:11:41.302: INFO: successfully validated that service multi-endpoint-test in namespace services-1006 exposes endpoints map[pod1:[100] pod2:[101]] (3.184199761s elapsed)
STEP: Deleting pod pod1 in namespace services-1006
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1006 to expose endpoints map[pod2:[101]]
Aug 18 18:11:42.390: INFO: successfully validated that service multi-endpoint-test in namespace services-1006 exposes endpoints map[pod2:[101]] (1.065140918s elapsed)
STEP: Deleting pod pod2 in namespace services-1006
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1006 to expose endpoints map[]
Aug 18 18:11:42.435: INFO: successfully validated that service multi-endpoint-test in namespace services-1006 exposes endpoints map[] (20.991763ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:42.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1006" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.019 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":81,"skipped":1471,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:42.596: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1629
[It] should create a deployment from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 18:11:42.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6478'
Aug 18 18:11:42.986: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 18 18:11:42.986: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1634
Aug 18 18:11:47.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6478'
Aug 18 18:11:47.148: INFO: stderr: ""
Aug 18 18:11:47.148: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:47.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6478" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image [Deprecated] [Conformance]","total":280,"completed":82,"skipped":1473,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:47.210: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-85939a84-d0ca-4ec6-9a5a-3f83b9550dce
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:47.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1740" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":83,"skipped":1484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:47.529: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:11:47.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73" in namespace "projected-3840" to be "success or failure"
Aug 18 18:11:47.831: INFO: Pod "downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73": Phase="Pending", Reason="", readiness=false. Elapsed: 15.03556ms
Aug 18 18:11:49.846: INFO: Pod "downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029389583s
Aug 18 18:11:51.866: INFO: Pod "downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049649788s
STEP: Saw pod success
Aug 18 18:11:51.866: INFO: Pod "downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73" satisfied condition "success or failure"
Aug 18 18:11:51.881: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73 container client-container: <nil>
STEP: delete the pod
Aug 18 18:11:51.971: INFO: Waiting for pod downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73 to disappear
Aug 18 18:11:51.987: INFO: Pod downwardapi-volume-c87eac6a-b9bc-43f1-958f-bb463944ab73 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:51.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3840" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1514,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:11:56.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9058" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":85,"skipped":1521,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:11:56.702: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-dc06333f-9f8f-40f2-a2d7-b8a071332077
STEP: Creating a pod to test consume secrets
Aug 18 18:11:57.013: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7" in namespace "projected-1843" to be "success or failure"
Aug 18 18:11:57.025: INFO: Pod "pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.513412ms
Aug 18 18:11:59.040: INFO: Pod "pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026843669s
Aug 18 18:12:01.059: INFO: Pod "pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04603926s
STEP: Saw pod success
Aug 18 18:12:01.059: INFO: Pod "pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7" satisfied condition "success or failure"
Aug 18 18:12:01.079: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:12:01.166: INFO: Waiting for pod pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7 to disappear
Aug 18 18:12:01.179: INFO: Pod pod-projected-secrets-008d777b-c466-4998-9a65-886d3d3bddf7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:12:01.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1843" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":86,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:12:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:50
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 18 18:12:03.601: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-222924798 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 18 18:12:13.744: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:12:13.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5607" for this suite.

• [SLOW TEST:12.573 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":87,"skipped":1585,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:12:13.809: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9161
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5309
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:12:50.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2700" for this suite.
STEP: Destroying namespace "nsdeletetest-9161" for this suite.
Aug 18 18:12:50.231: INFO: Namespace nsdeletetest-9161 was already deleted
STEP: Destroying namespace "nsdeletetest-5309" for this suite.

• [SLOW TEST:36.449 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":88,"skipped":1599,"failed":0}
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:12:50.258: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:12:50.605: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-584b9eaa-3ad1-47d2-b671-3720418565f9" in namespace "security-context-test-1516" to be "success or failure"
Aug 18 18:12:50.618: INFO: Pod "alpine-nnp-false-584b9eaa-3ad1-47d2-b671-3720418565f9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.459707ms
Aug 18 18:12:52.634: INFO: Pod "alpine-nnp-false-584b9eaa-3ad1-47d2-b671-3720418565f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028538997s
Aug 18 18:12:54.647: INFO: Pod "alpine-nnp-false-584b9eaa-3ad1-47d2-b671-3720418565f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042447486s
Aug 18 18:12:54.648: INFO: Pod "alpine-nnp-false-584b9eaa-3ad1-47d2-b671-3720418565f9" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:12:54.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1516" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1602,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:12:54.731: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 18 18:12:55.022: INFO: Waiting up to 5m0s for pod "pod-45d3b668-7ad1-4e83-8199-c0da22812ac1" in namespace "emptydir-6652" to be "success or failure"
Aug 18 18:12:55.038: INFO: Pod "pod-45d3b668-7ad1-4e83-8199-c0da22812ac1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.843643ms
Aug 18 18:12:57.051: INFO: Pod "pod-45d3b668-7ad1-4e83-8199-c0da22812ac1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028804784s
Aug 18 18:12:59.066: INFO: Pod "pod-45d3b668-7ad1-4e83-8199-c0da22812ac1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04394085s
STEP: Saw pod success
Aug 18 18:12:59.066: INFO: Pod "pod-45d3b668-7ad1-4e83-8199-c0da22812ac1" satisfied condition "success or failure"
Aug 18 18:12:59.081: INFO: Trying to get logs from node 10.13.3.84 pod pod-45d3b668-7ad1-4e83-8199-c0da22812ac1 container test-container: <nil>
STEP: delete the pod
Aug 18 18:12:59.174: INFO: Waiting for pod pod-45d3b668-7ad1-4e83-8199-c0da22812ac1 to disappear
Aug 18 18:12:59.188: INFO: Pod pod-45d3b668-7ad1-4e83-8199-c0da22812ac1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:12:59.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6652" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":90,"skipped":1610,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:12:59.257: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:12:59.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6" in namespace "projected-8077" to be "success or failure"
Aug 18 18:12:59.802: INFO: Pod "downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6": Phase="Pending", Reason="", readiness=false. Elapsed: 43.268353ms
Aug 18 18:13:02.260: INFO: Pod "downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.500845159s
STEP: Saw pod success
Aug 18 18:13:02.260: INFO: Pod "downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6" satisfied condition "success or failure"
Aug 18 18:13:02.274: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6 container client-container: <nil>
STEP: delete the pod
Aug 18 18:13:02.365: INFO: Waiting for pod downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6 to disappear
Aug 18 18:13:02.396: INFO: Pod downwardapi-volume-ede76a5e-fa78-4a48-92ce-184d44c457b6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:13:02.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8077" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":91,"skipped":1623,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:13:02.454: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9987
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 18 18:13:02.715: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 18 18:13:19.228: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:13:23.007: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:13:37.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9987" for this suite.

• [SLOW TEST:35.610 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":92,"skipped":1639,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:13:38.065: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:13:55.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4975" for this suite.

• [SLOW TEST:16.991 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":93,"skipped":1654,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:13:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1860
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 18 18:13:55.329: INFO: Waiting up to 5m0s for pod "pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b" in namespace "emptydir-1860" to be "success or failure"
Aug 18 18:13:55.343: INFO: Pod "pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.136237ms
Aug 18 18:13:57.362: INFO: Pod "pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032487803s
Aug 18 18:13:59.375: INFO: Pod "pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045404081s
STEP: Saw pod success
Aug 18 18:13:59.375: INFO: Pod "pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b" satisfied condition "success or failure"
Aug 18 18:13:59.390: INFO: Trying to get logs from node 10.13.3.84 pod pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b container test-container: <nil>
STEP: delete the pod
Aug 18 18:13:59.474: INFO: Waiting for pod pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b to disappear
Aug 18 18:13:59.490: INFO: Pod pod-7a623f55-4c09-4c05-b87f-69b5a6777c4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:13:59.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1860" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":94,"skipped":1658,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:13:59.547: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-lzjq
STEP: Creating a pod to test atomic-volume-subpath
Aug 18 18:13:59.900: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lzjq" in namespace "subpath-6872" to be "success or failure"
Aug 18 18:13:59.913: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Pending", Reason="", readiness=false. Elapsed: 13.176076ms
Aug 18 18:14:01.930: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 2.029811769s
Aug 18 18:14:03.943: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 4.042990226s
Aug 18 18:14:05.955: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 6.054793454s
Aug 18 18:14:07.970: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 8.070533389s
Aug 18 18:14:09.986: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 10.086193198s
Aug 18 18:14:12.002: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 12.102768508s
Aug 18 18:14:14.021: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 14.121458197s
Aug 18 18:14:16.037: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 16.137665831s
Aug 18 18:14:18.055: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 18.155298843s
Aug 18 18:14:20.069: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Running", Reason="", readiness=true. Elapsed: 20.169577405s
Aug 18 18:14:22.083: INFO: Pod "pod-subpath-test-configmap-lzjq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.183663728s
STEP: Saw pod success
Aug 18 18:14:22.083: INFO: Pod "pod-subpath-test-configmap-lzjq" satisfied condition "success or failure"
Aug 18 18:14:22.102: INFO: Trying to get logs from node 10.13.3.84 pod pod-subpath-test-configmap-lzjq container test-container-subpath-configmap-lzjq: <nil>
STEP: delete the pod
Aug 18 18:14:22.179: INFO: Waiting for pod pod-subpath-test-configmap-lzjq to disappear
Aug 18 18:14:22.191: INFO: Pod pod-subpath-test-configmap-lzjq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lzjq
Aug 18 18:14:22.191: INFO: Deleting pod "pod-subpath-test-configmap-lzjq" in namespace "subpath-6872"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:14:22.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6872" for this suite.

• [SLOW TEST:22.705 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":95,"skipped":1658,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:14:22.252: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 18 18:14:22.520: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Aug 18 18:14:23.137: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 18 18:14:25.316: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:14:27.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:14:29.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:14:31.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:14:33.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371263, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:14:36.929: INFO: Waited 1.579796124s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:14:37.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-981" for this suite.

• [SLOW TEST:15.520 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":96,"skipped":1663,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:14:37.772: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8041
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8041
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8041
Aug 18 18:14:38.083: INFO: Found 0 stateful pods, waiting for 1
Aug 18 18:14:48.097: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 18 18:14:48.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 18:14:48.374: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 18:14:48.374: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 18:14:48.374: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 18:14:48.388: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 18 18:14:58.414: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 18:14:58.414: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:14:58.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998371s
Aug 18 18:14:59.499: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987158994s
Aug 18 18:15:00.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974562173s
Aug 18 18:15:01.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.958011958s
Aug 18 18:15:02.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.943438247s
Aug 18 18:15:03.642: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.929251032s
Aug 18 18:15:04.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.831601638s
Aug 18 18:15:05.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.81546325s
Aug 18 18:15:06.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.801872948s
Aug 18 18:15:07.709: INFO: Verifying statefulset ss doesn't scale past 1 for another 788.044824ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8041
Aug 18 18:15:08.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:15:08.981: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 18:15:08.981: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 18:15:08.981: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 18:15:08.998: INFO: Found 1 stateful pods, waiting for 3
Aug 18 18:15:19.012: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:15:19.012: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:15:19.012: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 18 18:15:19.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 18:15:19.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 18:15:19.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 18:15:19.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 18:15:19.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 18:15:19.560: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 18:15:19.560: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 18:15:19.560: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 18:15:19.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 18:15:19.855: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 18:15:19.855: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 18:15:19.855: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 18:15:19.855: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:15:19.873: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 18 18:15:29.906: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 18:15:29.906: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 18:15:29.906: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 18 18:15:29.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999775s
Aug 18 18:15:30.972: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984228917s
Aug 18 18:15:31.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969397648s
Aug 18 18:15:33.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.954530198s
Aug 18 18:15:34.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940301868s
Aug 18 18:15:35.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.920284867s
Aug 18 18:15:36.055: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.904555416s
Aug 18 18:15:37.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.886682981s
Aug 18 18:15:38.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.864788195s
Aug 18 18:15:39.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 847.662748ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8041
Aug 18 18:15:40.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:15:40.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 18:15:40.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 18:15:40.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 18:15:40.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:15:40.589: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 18:15:40.589: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 18:15:40.589: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 18:15:40.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-8041 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:15:40.853: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 18:15:40.853: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 18:15:40.853: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 18:15:40.853: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 18 18:16:10.916: INFO: Deleting all statefulset in ns statefulset-8041
Aug 18 18:16:10.932: INFO: Scaling statefulset ss to 0
Aug 18 18:16:10.983: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:16:10.998: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:16:11.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8041" for this suite.

• [SLOW TEST:93.357 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":97,"skipped":1670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:16:11.130: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-110e4132-235f-4751-ba48-e3561552b305
STEP: Creating a pod to test consume configMaps
Aug 18 18:16:11.653: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043" in namespace "projected-4414" to be "success or failure"
Aug 18 18:16:11.662: INFO: Pod "pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043": Phase="Pending", Reason="", readiness=false. Elapsed: 8.669338ms
Aug 18 18:16:13.675: INFO: Pod "pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022078677s
STEP: Saw pod success
Aug 18 18:16:13.675: INFO: Pod "pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043" satisfied condition "success or failure"
Aug 18 18:16:13.689: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:16:13.809: INFO: Waiting for pod pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043 to disappear
Aug 18 18:16:13.823: INFO: Pod pod-projected-configmaps-be778416-dfa9-43e6-917d-f0d623b4b043 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:16:13.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4414" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1713,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:16:13.942: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:16:14.214: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:16:18.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1302" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1719,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:16:18.403: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:16:35.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2847" for this suite.

• [SLOW TEST:16.870 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":100,"skipped":1733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:16:35.273: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9491
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 18 18:16:35.543: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:16:39.323: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:16:54.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9491" for this suite.

• [SLOW TEST:18.868 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":101,"skipped":1760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:16:54.142: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:16:54.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4" in namespace "downward-api-1683" to be "success or failure"
Aug 18 18:16:54.460: INFO: Pod "downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.657462ms
Aug 18 18:16:56.475: INFO: Pod "downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028328562s
Aug 18 18:16:58.703: INFO: Pod "downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.256685071s
STEP: Saw pod success
Aug 18 18:16:58.703: INFO: Pod "downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4" satisfied condition "success or failure"
Aug 18 18:16:58.716: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4 container client-container: <nil>
STEP: delete the pod
Aug 18 18:16:59.240: INFO: Waiting for pod downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4 to disappear
Aug 18 18:16:59.254: INFO: Pod downwardapi-volume-23435100-f7d1-4795-bb01-06f3b0591ad4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:16:59.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1683" for this suite.

• [SLOW TEST:5.157 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1773,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:16:59.299: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5404
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Aug 18 18:16:59.583: INFO: Found 0 stateful pods, waiting for 3
Aug 18 18:17:09.598: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:17:09.598: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:17:09.598: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 18 18:17:09.700: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 18 18:17:19.809: INFO: Updating stateful set ss2
Aug 18 18:17:19.839: INFO: Waiting for Pod statefulset-5404/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:17:29.872: INFO: Waiting for Pod statefulset-5404/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 18 18:17:40.531: INFO: Found 2 stateful pods, waiting for 3
Aug 18 18:17:50.547: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:17:50.547: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:17:50.547: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 18 18:17:50.624: INFO: Updating stateful set ss2
Aug 18 18:17:50.659: INFO: Waiting for Pod statefulset-5404/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:18:00.740: INFO: Updating stateful set ss2
Aug 18 18:18:00.768: INFO: Waiting for StatefulSet statefulset-5404/ss2 to complete update
Aug 18 18:18:00.768: INFO: Waiting for Pod statefulset-5404/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:18:10.801: INFO: Waiting for StatefulSet statefulset-5404/ss2 to complete update
Aug 18 18:18:10.801: INFO: Waiting for Pod statefulset-5404/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:18:20.800: INFO: Waiting for StatefulSet statefulset-5404/ss2 to complete update
Aug 18 18:18:20.800: INFO: Waiting for Pod statefulset-5404/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 18 18:18:30.800: INFO: Deleting all statefulset in ns statefulset-5404
Aug 18 18:18:30.815: INFO: Scaling statefulset ss2 to 0
Aug 18 18:18:51.089: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:18:51.104: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:18:51.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5404" for this suite.

• [SLOW TEST:111.941 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":103,"skipped":1775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:18:51.243: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-838b0d63-add6-401f-9ac0-3afdc518bee2
STEP: Creating a pod to test consume configMaps
Aug 18 18:18:51.605: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de" in namespace "configmap-2354" to be "success or failure"
Aug 18 18:18:51.638: INFO: Pod "pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de": Phase="Pending", Reason="", readiness=false. Elapsed: 32.516824ms
Aug 18 18:18:53.652: INFO: Pod "pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047075479s
Aug 18 18:18:55.666: INFO: Pod "pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061085315s
STEP: Saw pod success
Aug 18 18:18:55.666: INFO: Pod "pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de" satisfied condition "success or failure"
Aug 18 18:18:55.679: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:18:55.778: INFO: Waiting for pod pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de to disappear
Aug 18 18:18:55.790: INFO: Pod pod-configmaps-ba2e6f09-0129-49b4-85df-a42f7027b0de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:18:55.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2354" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1807,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:18:55.832: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 18 18:19:00.258: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:00.271: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 18 18:19:02.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:02.286: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 18 18:19:04.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:04.287: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 18 18:19:06.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:06.287: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 18 18:19:08.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:08.290: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 18 18:19:10.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:10.285: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 18 18:19:12.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 18 18:19:12.287: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:19:12.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8800" for this suite.

• [SLOW TEST:16.765 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":105,"skipped":1827,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:19:12.600: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-5077/configmap-test-dae7cdd8-681f-4024-ac7f-503abe9068dd
STEP: Creating a pod to test consume configMaps
Aug 18 18:19:12.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665" in namespace "configmap-5077" to be "success or failure"
Aug 18 18:19:12.929: INFO: Pod "pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665": Phase="Pending", Reason="", readiness=false. Elapsed: 14.053539ms
Aug 18 18:19:14.944: INFO: Pod "pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028909504s
Aug 18 18:19:16.962: INFO: Pod "pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047087403s
STEP: Saw pod success
Aug 18 18:19:16.962: INFO: Pod "pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665" satisfied condition "success or failure"
Aug 18 18:19:16.978: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665 container env-test: <nil>
STEP: delete the pod
Aug 18 18:19:17.067: INFO: Waiting for pod pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665 to disappear
Aug 18 18:19:17.090: INFO: Pod pod-configmaps-2f3eb6f9-2eb1-49b1-ba0d-20914eb25665 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:19:17.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5077" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1837,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:19:17.154: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:19:18.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:19:20.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371558, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371558, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371558, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733371558, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:19:23.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:19:36.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8300" for this suite.
STEP: Destroying namespace "webhook-8300-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":107,"skipped":1850,"failed":0}
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:19:36.553: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:19:36.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 version'
Aug 18 18:19:36.898: INFO: stderr: ""
Aug 18 18:19:36.898: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.11\", GitCommit:\"ea5f00d93211b7c80247bf607cfa422ad6fb5347\", GitTreeState:\"clean\", BuildDate:\"2020-08-13T15:20:25Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.11+IKS\", GitCommit:\"8f73c4ab44865a242f2f2ee450d5e0043f7bc341\", GitTreeState:\"clean\", BuildDate:\"2020-08-13T19:41:30Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:19:36.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6274" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":108,"skipped":1850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:19:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4980
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 18 18:19:37.208: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 18 18:20:03.728: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.244.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4980 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:20:03.728: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:20:04.915: INFO: Found all expected endpoints: [netserver-0]
Aug 18 18:20:04.929: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.13.99 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4980 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:20:04.929: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:20:06.110: INFO: Found all expected endpoints: [netserver-1]
Aug 18 18:20:06.139: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.14.236 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4980 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:20:06.139: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:20:07.288: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:20:07.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4980" for this suite.

• [SLOW TEST:30.393 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":109,"skipped":1879,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:20:07.343: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:20:13.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2053" for this suite.

• [SLOW TEST:6.298 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":110,"skipped":1886,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:20:13.646: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-0d7322b2-f0c7-482a-ab8b-d1f077bb2cf1
STEP: Creating a pod to test consume secrets
Aug 18 18:20:13.989: INFO: Waiting up to 5m0s for pod "pod-secrets-39864091-a044-4d64-b931-fa6f697338c6" in namespace "secrets-7784" to be "success or failure"
Aug 18 18:20:14.002: INFO: Pod "pod-secrets-39864091-a044-4d64-b931-fa6f697338c6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.646525ms
Aug 18 18:20:16.021: INFO: Pod "pod-secrets-39864091-a044-4d64-b931-fa6f697338c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031447894s
Aug 18 18:20:18.034: INFO: Pod "pod-secrets-39864091-a044-4d64-b931-fa6f697338c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04503595s
STEP: Saw pod success
Aug 18 18:20:18.034: INFO: Pod "pod-secrets-39864091-a044-4d64-b931-fa6f697338c6" satisfied condition "success or failure"
Aug 18 18:20:18.050: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-39864091-a044-4d64-b931-fa6f697338c6 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:20:18.136: INFO: Waiting for pod pod-secrets-39864091-a044-4d64-b931-fa6f697338c6 to disappear
Aug 18 18:20:18.149: INFO: Pod pod-secrets-39864091-a044-4d64-b931-fa6f697338c6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:20:18.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7784" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:20:18.207: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 18 18:20:24.651: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0818 18:20:24.651356      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:20:24.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9753" for this suite.

• [SLOW TEST:6.491 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":112,"skipped":1919,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:20:24.699: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:325
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Aug 18 18:20:24.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-5416'
Aug 18 18:20:25.422: INFO: stderr: ""
Aug 18 18:20:25.422: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 18 18:20:25.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:25.526: INFO: stderr: ""
Aug 18 18:20:25.526: INFO: stdout: "update-demo-nautilus-r4x78 update-demo-nautilus-z5lws "
Aug 18 18:20:25.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:25.647: INFO: stderr: ""
Aug 18 18:20:25.647: INFO: stdout: ""
Aug 18 18:20:25.647: INFO: update-demo-nautilus-r4x78 is created but not running
Aug 18 18:20:30.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:30.769: INFO: stderr: ""
Aug 18 18:20:30.769: INFO: stdout: "update-demo-nautilus-r4x78 update-demo-nautilus-z5lws "
Aug 18 18:20:30.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:30.866: INFO: stderr: ""
Aug 18 18:20:30.867: INFO: stdout: "true"
Aug 18 18:20:30.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:30.965: INFO: stderr: ""
Aug 18 18:20:30.965: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:20:30.965: INFO: validating pod update-demo-nautilus-r4x78
Aug 18 18:20:30.992: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:20:30.992: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:20:30.992: INFO: update-demo-nautilus-r4x78 is verified up and running
Aug 18 18:20:30.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-z5lws -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:31.088: INFO: stderr: ""
Aug 18 18:20:31.088: INFO: stdout: "true"
Aug 18 18:20:31.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-z5lws -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:31.189: INFO: stderr: ""
Aug 18 18:20:31.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:20:31.189: INFO: validating pod update-demo-nautilus-z5lws
Aug 18 18:20:31.213: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:20:31.213: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:20:31.213: INFO: update-demo-nautilus-z5lws is verified up and running
STEP: scaling down the replication controller
Aug 18 18:20:31.215: INFO: scanned /root for discovery docs: <nil>
Aug 18 18:20:31.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5416'
Aug 18 18:20:32.731: INFO: stderr: ""
Aug 18 18:20:32.731: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 18 18:20:32.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:32.836: INFO: stderr: ""
Aug 18 18:20:32.836: INFO: stdout: "update-demo-nautilus-r4x78 update-demo-nautilus-z5lws "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 18 18:20:37.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:37.938: INFO: stderr: ""
Aug 18 18:20:37.938: INFO: stdout: "update-demo-nautilus-r4x78 update-demo-nautilus-z5lws "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 18 18:20:42.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:43.057: INFO: stderr: ""
Aug 18 18:20:43.057: INFO: stdout: "update-demo-nautilus-r4x78 "
Aug 18 18:20:43.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:43.392: INFO: stderr: ""
Aug 18 18:20:43.392: INFO: stdout: "true"
Aug 18 18:20:43.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:43.487: INFO: stderr: ""
Aug 18 18:20:43.487: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:20:43.487: INFO: validating pod update-demo-nautilus-r4x78
Aug 18 18:20:43.510: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:20:43.510: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:20:43.510: INFO: update-demo-nautilus-r4x78 is verified up and running
STEP: scaling up the replication controller
Aug 18 18:20:43.513: INFO: scanned /root for discovery docs: <nil>
Aug 18 18:20:43.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5416'
Aug 18 18:20:45.246: INFO: stderr: ""
Aug 18 18:20:45.246: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 18 18:20:45.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:45.349: INFO: stderr: ""
Aug 18 18:20:45.349: INFO: stdout: "update-demo-nautilus-4lnh8 update-demo-nautilus-r4x78 "
Aug 18 18:20:45.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-4lnh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:45.447: INFO: stderr: ""
Aug 18 18:20:45.447: INFO: stdout: ""
Aug 18 18:20:45.447: INFO: update-demo-nautilus-4lnh8 is created but not running
Aug 18 18:20:50.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5416'
Aug 18 18:20:50.563: INFO: stderr: ""
Aug 18 18:20:50.563: INFO: stdout: "update-demo-nautilus-4lnh8 update-demo-nautilus-r4x78 "
Aug 18 18:20:50.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-4lnh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:50.659: INFO: stderr: ""
Aug 18 18:20:50.659: INFO: stdout: "true"
Aug 18 18:20:50.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-4lnh8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:50.765: INFO: stderr: ""
Aug 18 18:20:50.765: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:20:50.765: INFO: validating pod update-demo-nautilus-4lnh8
Aug 18 18:20:50.795: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:20:50.795: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:20:50.795: INFO: update-demo-nautilus-4lnh8 is verified up and running
Aug 18 18:20:50.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:50.907: INFO: stderr: ""
Aug 18 18:20:50.907: INFO: stdout: "true"
Aug 18 18:20:50.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods update-demo-nautilus-r4x78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5416'
Aug 18 18:20:51.007: INFO: stderr: ""
Aug 18 18:20:51.008: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 18 18:20:51.008: INFO: validating pod update-demo-nautilus-r4x78
Aug 18 18:20:51.025: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 18 18:20:51.025: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 18 18:20:51.025: INFO: update-demo-nautilus-r4x78 is verified up and running
STEP: using delete to clean up resources
Aug 18 18:20:51.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-5416'
Aug 18 18:20:51.159: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:20:51.159: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 18 18:20:51.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5416'
Aug 18 18:20:51.270: INFO: stderr: "No resources found in kubectl-5416 namespace.\n"
Aug 18 18:20:51.270: INFO: stdout: ""
Aug 18 18:20:51.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -l name=update-demo --namespace=kubectl-5416 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 18 18:20:51.367: INFO: stderr: ""
Aug 18 18:20:51.367: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:20:51.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5416" for this suite.

• [SLOW TEST:26.729 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:323
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":113,"skipped":1940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:20:51.431: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 18 18:20:51.721: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:21:01.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8974" for this suite.

• [SLOW TEST:10.124 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1996,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:21:01.555: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 18 18:21:01.811: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:21:06.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2948" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":115,"skipped":1999,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:21:06.397: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5005
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-94f86514-282b-4c17-884e-9a5dead6cc12
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:21:08.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5005" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":2009,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:21:08.912: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6194
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Aug 18 18:21:09.243: INFO: Found 0 stateful pods, waiting for 3
Aug 18 18:21:19.259: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:21:19.259: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:21:19.259: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 18 18:21:19.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6194 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 18:21:19.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 18:21:19.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 18:21:19.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 18 18:21:29.948: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 18 18:21:40.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6194 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:21:40.316: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 18:21:40.316: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 18:21:40.316: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 18:21:50.409: INFO: Waiting for StatefulSet statefulset-6194/ss2 to complete update
Aug 18 18:21:50.409: INFO: Waiting for Pod statefulset-6194/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:21:50.409: INFO: Waiting for Pod statefulset-6194/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:21:50.409: INFO: Waiting for Pod statefulset-6194/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:22:00.439: INFO: Waiting for StatefulSet statefulset-6194/ss2 to complete update
Aug 18 18:22:00.439: INFO: Waiting for Pod statefulset-6194/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 18 18:22:10.471: INFO: Waiting for StatefulSet statefulset-6194/ss2 to complete update
Aug 18 18:22:10.471: INFO: Waiting for Pod statefulset-6194/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Aug 18 18:22:20.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6194 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 18 18:22:20.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 18 18:22:20.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 18 18:22:20.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 18 18:22:30.815: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 18 18:22:40.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=statefulset-6194 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 18 18:22:41.156: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 18 18:22:41.157: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 18 18:22:41.157: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 18 18:22:51.247: INFO: Waiting for StatefulSet statefulset-6194/ss2 to complete update
Aug 18 18:22:51.247: INFO: Waiting for Pod statefulset-6194/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 18 18:22:51.247: INFO: Waiting for Pod statefulset-6194/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 18 18:23:01.283: INFO: Waiting for StatefulSet statefulset-6194/ss2 to complete update
Aug 18 18:23:01.283: INFO: Waiting for Pod statefulset-6194/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Aug 18 18:23:11.275: INFO: Waiting for StatefulSet statefulset-6194/ss2 to complete update
Aug 18 18:23:11.275: INFO: Waiting for Pod statefulset-6194/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 18 18:23:21.277: INFO: Deleting all statefulset in ns statefulset-6194
Aug 18 18:23:21.290: INFO: Scaling statefulset ss2 to 0
Aug 18 18:23:51.357: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 18:23:51.375: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:23:51.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6194" for this suite.

• [SLOW TEST:162.602 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":117,"skipped":2019,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:23:51.515: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:23:53.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9305" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":118,"skipped":2025,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:23:53.935: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 18 18:23:54.183: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 18 18:23:54.238: INFO: Waiting for terminating namespaces to be deleted...
Aug 18 18:23:54.255: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.114 before test
Aug 18 18:23:54.335: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-pw6pj from ibm-system started at 2020-08-18 17:41:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.335: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:23:54.335: INFO: vpn-f66c45467-kh4hm from kube-system started at 2020-08-18 16:16:37 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.336: INFO: 	Container vpn ready: true, restart count 0
Aug 18 18:23:54.336: INFO: calico-node-2ngjr from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.336: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:23:54.336: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:23:54.336: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:23:54.336: INFO: ibm-master-proxy-static-10.13.3.114 from kube-system started at 2020-08-18 15:51:09 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.336: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:23:54.336: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:23:54.336: INFO: metrics-server-797d668946-ltqcd from kube-system started at 2020-08-18 15:52:24 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.337: INFO: 	Container metrics-server ready: true, restart count 0
Aug 18 18:23:54.337: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 18 18:23:54.337: INFO: ibm-keepalived-watcher-q59v8 from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.337: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:23:54.337: INFO: calico-kube-controllers-5754cfb59d-8hh6k from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.337: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 18 18:23:54.337: INFO: coredns-6567db4fff-6tjgz from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.337: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:23:54.337: INFO: coredns-6567db4fff-78ghm from kube-system started at 2020-08-18 17:45:40 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.337: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:23:54.337: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb from kube-system started at 2020-08-18 15:56:04 +0000 UTC (4 container statuses recorded)
Aug 18 18:23:54.337: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:23:54.337: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:23:54.338: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:23:54.338: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:23:54.338: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.115 before test
Aug 18 18:23:54.428: INFO: ibm-file-plugin-ff7c989f9-fsrmg from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 18 18:23:54.428: INFO: sonobuoy-e2e-job-715a84e433104f63 from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container e2e ready: true, restart count 0
Aug 18 18:23:54.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:23:54.428: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-rb2hb from kube-system started at 2020-08-18 17:41:38 +0000 UTC (4 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:23:54.428: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:23:54.428: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:23:54.428: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:23:54.428: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-g6ck9 from ibm-system started at 2020-08-18 15:52:58 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:23:54.428: INFO: coredns-6567db4fff-wxj8p from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:23:54.428: INFO: ibm-master-proxy-static-10.13.3.115 from kube-system started at 2020-08-18 15:51:10 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:23:54.428: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:23:54.428: INFO: calico-node-j6sx8 from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:23:54.428: INFO: dashboard-metrics-scraper-5789d44f58-nsshl from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.428: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 18 18:23:54.429: INFO: catalog-operator-67646bfcdb-77lhs from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 18 18:23:54.429: INFO: olm-operator-787498c9b7-9cmpc from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container olm-operator ready: true, restart count 0
Aug 18 18:23:54.429: INFO: ibm-keepalived-watcher-zktgc from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:23:54.429: INFO: kubernetes-dashboard-984c5c57-bl4dx from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 18 18:23:54.429: INFO: coredns-autoscaler-649976fbf4-q69hh from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container autoscaler ready: true, restart count 0
Aug 18 18:23:54.429: INFO: sonobuoy from sonobuoy started at 2020-08-18 17:39:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 18 18:23:54.429: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-cpwhn from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:23:54.429: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:23:54.429: INFO: ibm-storage-watcher-56b6fd445c-dqt8p from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.429: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 18 18:23:54.429: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.84 before test
Aug 18 18:23:54.460: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-kdbbz from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:23:54.460: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:23:54.460: INFO: ibm-master-proxy-static-10.13.3.84 from kube-system started at 2020-08-18 15:51:51 +0000 UTC (2 container statuses recorded)
Aug 18 18:23:54.460: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:23:54.460: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:23:54.460: INFO: ibm-keepalived-watcher-228kf from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.460: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:23:54.460: INFO: calico-node-b68l4 from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.460: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:23:54.460: INFO: addon-catalog-source-62ppj from ibm-system started at 2020-08-18 15:52:32 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.460: INFO: 	Container configmap-registry-server ready: true, restart count 0
Aug 18 18:23:54.460: INFO: client-containers-b2121fac-3a1d-474d-b30f-fd6bfeeaa321 from containers-9305 started at 2020-08-18 18:23:51 +0000 UTC (1 container statuses recorded)
Aug 18 18:23:54.460: INFO: 	Container test-container ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6080a4bf-efc7-41cb-9dd6-dca7e8ca61e1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6080a4bf-efc7-41cb-9dd6-dca7e8ca61e1 off the node 10.13.3.84
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6080a4bf-efc7-41cb-9dd6-dca7e8ca61e1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:29:02.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4089" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:308.923 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":119,"skipped":2028,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:29:02.858: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8396
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8396
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 18 18:29:03.126: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 18 18:29:27.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.14.194:8080/dial?request=hostname&protocol=http&host=172.30.244.195&port=8080&tries=1'] Namespace:pod-network-test-8396 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:29:27.436: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:29:27.609: INFO: Waiting for responses: map[]
Aug 18 18:29:27.623: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.14.194:8080/dial?request=hostname&protocol=http&host=172.30.13.106&port=8080&tries=1'] Namespace:pod-network-test-8396 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:29:27.623: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:29:27.820: INFO: Waiting for responses: map[]
Aug 18 18:29:27.833: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.14.194:8080/dial?request=hostname&protocol=http&host=172.30.14.253&port=8080&tries=1'] Namespace:pod-network-test-8396 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:29:27.833: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:29:28.031: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:29:28.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8396" for this suite.

• [SLOW TEST:25.232 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":120,"skipped":2030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:29:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-e1f17ef6-db6d-4bd1-992b-5ec846416227
STEP: Creating a pod to test consume configMaps
Aug 18 18:29:28.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25" in namespace "configmap-8321" to be "success or failure"
Aug 18 18:29:28.442: INFO: Pod "pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25": Phase="Pending", Reason="", readiness=false. Elapsed: 13.796271ms
Aug 18 18:29:30.456: INFO: Pod "pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027650688s
STEP: Saw pod success
Aug 18 18:29:30.456: INFO: Pod "pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25" satisfied condition "success or failure"
Aug 18 18:29:30.470: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:29:30.800: INFO: Waiting for pod pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25 to disappear
Aug 18 18:29:30.812: INFO: Pod pod-configmaps-2db8ecc9-ebcc-4b42-bc74-adf17be81a25 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:29:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8321" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":2055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:29:30.861: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 18 18:29:39.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:39.320: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:41.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:41.334: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:43.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:43.334: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:45.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:45.333: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:47.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:47.334: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:49.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:49.335: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:51.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:51.333: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 18 18:29:53.320: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 18 18:29:53.334: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:29:53.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8805" for this suite.

• [SLOW TEST:22.522 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":122,"skipped":2093,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:29:53.383: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:29:53.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a" in namespace "projected-2075" to be "success or failure"
Aug 18 18:29:53.683: INFO: Pod "downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.932525ms
Aug 18 18:29:55.696: INFO: Pod "downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029247338s
STEP: Saw pod success
Aug 18 18:29:55.696: INFO: Pod "downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a" satisfied condition "success or failure"
Aug 18 18:29:55.711: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a container client-container: <nil>
STEP: delete the pod
Aug 18 18:29:55.822: INFO: Waiting for pod downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a to disappear
Aug 18 18:29:55.836: INFO: Pod downwardapi-volume-f0381da2-3bed-419a-9d85-7f0f12bced1a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:29:55.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2075" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:29:55.890: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:29:56.285: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 18 18:29:56.316: INFO: Number of nodes with available pods: 0
Aug 18 18:29:56.316: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 18 18:29:56.417: INFO: Number of nodes with available pods: 0
Aug 18 18:29:56.417: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:29:57.432: INFO: Number of nodes with available pods: 0
Aug 18 18:29:57.432: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:29:58.431: INFO: Number of nodes with available pods: 0
Aug 18 18:29:58.431: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:29:59.432: INFO: Number of nodes with available pods: 1
Aug 18 18:29:59.432: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 18 18:29:59.507: INFO: Number of nodes with available pods: 1
Aug 18 18:29:59.508: INFO: Number of running nodes: 0, number of available pods: 1
Aug 18 18:30:00.521: INFO: Number of nodes with available pods: 0
Aug 18 18:30:00.521: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 18 18:30:00.553: INFO: Number of nodes with available pods: 0
Aug 18 18:30:00.553: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:01.568: INFO: Number of nodes with available pods: 0
Aug 18 18:30:01.568: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:02.568: INFO: Number of nodes with available pods: 0
Aug 18 18:30:02.568: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:03.568: INFO: Number of nodes with available pods: 0
Aug 18 18:30:03.568: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:04.569: INFO: Number of nodes with available pods: 0
Aug 18 18:30:04.569: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:05.567: INFO: Number of nodes with available pods: 0
Aug 18 18:30:05.567: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:06.566: INFO: Number of nodes with available pods: 0
Aug 18 18:30:06.566: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:07.567: INFO: Number of nodes with available pods: 0
Aug 18 18:30:07.567: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:08.570: INFO: Number of nodes with available pods: 0
Aug 18 18:30:08.570: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:09.567: INFO: Number of nodes with available pods: 0
Aug 18 18:30:09.567: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:10.636: INFO: Number of nodes with available pods: 0
Aug 18 18:30:10.636: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:11.574: INFO: Number of nodes with available pods: 0
Aug 18 18:30:11.574: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:30:12.567: INFO: Number of nodes with available pods: 1
Aug 18 18:30:12.567: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2685, will wait for the garbage collector to delete the pods
Aug 18 18:30:12.703: INFO: Deleting DaemonSet.extensions daemon-set took: 34.053618ms
Aug 18 18:30:13.103: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.366749ms
Aug 18 18:30:20.018: INFO: Number of nodes with available pods: 0
Aug 18 18:30:20.018: INFO: Number of running nodes: 0, number of available pods: 0
Aug 18 18:30:20.033: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2685/daemonsets","resourceVersion":"44596"},"items":null}

Aug 18 18:30:20.050: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2685/pods","resourceVersion":"44596"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2685" for this suite.

• [SLOW TEST:24.317 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":124,"skipped":2154,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:20.207: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 18 18:30:20.492: INFO: Waiting up to 5m0s for pod "downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081" in namespace "downward-api-3263" to be "success or failure"
Aug 18 18:30:20.529: INFO: Pod "downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081": Phase="Pending", Reason="", readiness=false. Elapsed: 36.344251ms
Aug 18 18:30:22.542: INFO: Pod "downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05006502s
STEP: Saw pod success
Aug 18 18:30:22.543: INFO: Pod "downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081" satisfied condition "success or failure"
Aug 18 18:30:22.555: INFO: Trying to get logs from node 10.13.3.84 pod downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:30:22.641: INFO: Waiting for pod downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081 to disappear
Aug 18 18:30:22.658: INFO: Pod downward-api-d65d12e5-0731-4a9b-84ca-698e74c0d081 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:22.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3263" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":2158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:22.708: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 18 18:30:29.833: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:29.833: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:30.260: INFO: Exec stderr: ""
Aug 18 18:30:30.261: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:30.261: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:30.418: INFO: Exec stderr: ""
Aug 18 18:30:30.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:30.564: INFO: Exec stderr: ""
Aug 18 18:30:30.564: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:30.565: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:30.686: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 18 18:30:30.686: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:30.897: INFO: Exec stderr: ""
Aug 18 18:30:30.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:30.897: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:31.106: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 18 18:30:31.106: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:31.106: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:31.239: INFO: Exec stderr: ""
Aug 18 18:30:31.239: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:31.239: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:31.395: INFO: Exec stderr: ""
Aug 18 18:30:31.395: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:31.395: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:31.541: INFO: Exec stderr: ""
Aug 18 18:30:31.541: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5022 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:30:31.541: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:30:31.688: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:31.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5022" for this suite.

• [SLOW TEST:9.050 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":2254,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:31.759: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 18 18:30:35.144: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6236" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2258,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:35.589: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 18 18:30:35.886: INFO: Waiting up to 5m0s for pod "pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff" in namespace "emptydir-437" to be "success or failure"
Aug 18 18:30:35.901: INFO: Pod "pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff": Phase="Pending", Reason="", readiness=false. Elapsed: 14.734016ms
Aug 18 18:30:37.915: INFO: Pod "pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028187738s
STEP: Saw pod success
Aug 18 18:30:37.915: INFO: Pod "pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff" satisfied condition "success or failure"
Aug 18 18:30:37.926: INFO: Trying to get logs from node 10.13.3.84 pod pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff container test-container: <nil>
STEP: delete the pod
Aug 18 18:30:38.013: INFO: Waiting for pod pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff to disappear
Aug 18 18:30:38.028: INFO: Pod pod-0d764891-a1ae-4fa0-bd6e-cc7f9f949cff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:38.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-437" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":128,"skipped":2269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:38.076: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:30:38.349: INFO: Creating deployment "webserver-deployment"
Aug 18 18:30:38.365: INFO: Waiting for observed generation 1
Aug 18 18:30:40.392: INFO: Waiting for all required pods to come up
Aug 18 18:30:40.410: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 18 18:30:42.462: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 18 18:30:42.493: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 18 18:30:42.523: INFO: Updating deployment webserver-deployment
Aug 18 18:30:42.523: INFO: Waiting for observed generation 2
Aug 18 18:30:44.550: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 18 18:30:44.569: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 18 18:30:44.582: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 18 18:30:44.635: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 18 18:30:44.635: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 18 18:30:44.652: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 18 18:30:44.680: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 18 18:30:44.680: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 18 18:30:44.716: INFO: Updating deployment webserver-deployment
Aug 18 18:30:44.716: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 18 18:30:44.754: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 18 18:30:46.800: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 18 18:30:46.835: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9782 /apis/apps/v1/namespaces/deployment-9782/deployments/webserver-deployment e4bcae6e-3bc4-4f6a-9707-c1a456b9e7f3 45284 3 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002fe5e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-18 18:30:44 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-08-18 18:30:46 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

Aug 18 18:30:46.852: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-9782 /apis/apps/v1/namespaces/deployment-9782/replicasets/webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 45155 3 2020-08-18 18:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e4bcae6e-3bc4-4f6a-9707-c1a456b9e7f3 0xc0039bc317 0xc0039bc318}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039bc388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:30:46.852: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 18 18:30:46.852: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-9782 /apis/apps/v1/namespaces/deployment-9782/replicasets/webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 45281 3 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e4bcae6e-3bc4-4f6a-9707-c1a456b9e7f3 0xc0039bc257 0xc0039bc258}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0039bc2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:30:46.885: INFO: Pod "webserver-deployment-595b5b9587-b4tl9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b4tl9 webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-b4tl9 499a3cf3-e5a5-4458-bc83-b7837aa9fdaa 45183 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253417 0xc003253418}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.886: INFO: Pod "webserver-deployment-595b5b9587-b94pj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b94pj webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-b94pj a8f672d2-062a-4a00-9061-413ec9dffa2a 45308 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253577 0xc003253578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:172.30.244.204,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7104bf543c93a0f7dc6e075a027ae135e811c0cba5bac04ee84a7a8e5c30d370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.886: INFO: Pod "webserver-deployment-595b5b9587-blzk7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-blzk7 webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-blzk7 c4435bbb-1b09-4ca4-b147-c8b147a3438f 44944 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0032536f7 0xc0032536f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:172.30.13.107,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5c3940861b255b9f7fffb235fde066ac26381842af8958b580f6993c3895bf2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.13.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.886: INFO: Pod "webserver-deployment-595b5b9587-btkhw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-btkhw webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-btkhw d70663d4-6eba-469b-9bae-17945bd3321a 44963 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253877 0xc003253878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:172.30.244.201,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://dcc1a75754449d6f4c131d718978c743316d2053a7f4e221c5f4dde8f1848c8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.887: INFO: Pod "webserver-deployment-595b5b9587-chb8v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-chb8v webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-chb8v 8613d3bb-59a2-4578-899b-79355ab1dd04 44949 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0032539f7 0xc0032539f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:172.30.13.109,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://651944922b929a32b6be4cfc703bbeea7c91473372a8b0b251965d5c1b977933,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.13.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.887: INFO: Pod "webserver-deployment-595b5b9587-ckqlp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ckqlp webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-ckqlp c562d5d6-fe33-423f-a6fe-604b82b2bfae 45268 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253b77 0xc003253b78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:172.30.13.104,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://21e84abee97dd009a713551dac193ccec9e60f5f380ad54b288179ad3b79a3b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.13.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.887: INFO: Pod "webserver-deployment-595b5b9587-crdw6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-crdw6 webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-crdw6 ccfa4b26-2c36-4ae1-bda2-64110649068e 44959 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253cf7 0xc003253cf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.205,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://163f7f31c474b5d78114baec12df15802d9cf3fa145c4ac1746b6d2d77fe6402,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.887: INFO: Pod "webserver-deployment-595b5b9587-fjmvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fjmvd webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-fjmvd d06df03d-bb95-4f2e-ac8f-912a0701af66 45197 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253e70 0xc003253e71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.888: INFO: Pod "webserver-deployment-595b5b9587-g9fzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g9fzj webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-g9fzj 333586e3-797f-45be-96f8-2be554ddad2e 45192 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc003253fc0 0xc003253fc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.888: INFO: Pod "webserver-deployment-595b5b9587-k26h5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-k26h5 webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-k26h5 738df37b-de21-49b2-a513-8ea8849f08e1 45207 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4187 0xc0039c4188}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.888: INFO: Pod "webserver-deployment-595b5b9587-mc5pr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mc5pr webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-mc5pr 2bfe5d91-747c-4f88-8bfe-d2fbe1b7ad65 45202 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c42e0 0xc0039c42e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.888: INFO: Pod "webserver-deployment-595b5b9587-mt7pp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mt7pp webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-mt7pp 036d8108-27e8-4384-a94e-ca4ab73ef6b0 44973 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4430 0xc0039c4431}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:172.30.244.206,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a3f4d452242e8cebe7929605edbd6a2feee317055bbc9712ba44cf0e61a103aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.889: INFO: Pod "webserver-deployment-595b5b9587-nqhld" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nqhld webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-nqhld bf2907b0-0da8-48ab-b12b-60160e1a0443 44942 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c45b7 0xc0039c45b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:172.30.13.108,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e43f96eb8f73ed0798786e4ffd7cded46694f0944c937e312e30db4289e7261c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.13.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.889: INFO: Pod "webserver-deployment-595b5b9587-p76bk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p76bk webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-p76bk 8f25e0bd-4f1d-4fbf-9ee6-35c4df2e4196 44955 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4737 0xc0039c4738}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.204,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://77dad419686d8aff32cd92374042aee9cfabd7d092f942d62f5565a9adf89291,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.889: INFO: Pod "webserver-deployment-595b5b9587-phkwm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-phkwm webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-phkwm 2bd08786-69ae-47c1-bcb6-99dc7098c1c4 45159 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c48b0 0xc0039c48b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.889: INFO: Pod "webserver-deployment-595b5b9587-rgv9j" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rgv9j webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-rgv9j ce9362c7-99ee-45cb-9c92-329a173045e2 44976 0 2020-08-18 18:30:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4a00 0xc0039c4a01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:172.30.244.202,StartTime:2020-08-18 18:30:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://12b759ecfa61a2e8acddab4c69cce20b3397881ca4e35656f70522d7a8b1dac3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.890: INFO: Pod "webserver-deployment-595b5b9587-wbsn2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wbsn2 webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-wbsn2 a6a15e1f-c489-4d6d-a277-fffc411159cd 45180 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4b77 0xc0039c4b78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.890: INFO: Pod "webserver-deployment-595b5b9587-xd76j" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xd76j webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-xd76j 1f621db6-0e0d-45d8-9fd0-7fda08ded2db 45279 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4cd7 0xc0039c4cd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.208,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://0d5ecea95e2f462cf8072047d479b7f3b8eb333e703bf915e542414a9ed4f103,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.890: INFO: Pod "webserver-deployment-595b5b9587-xdh47" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xdh47 webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-xdh47 6414b6eb-0891-4ba9-885f-edf9c146a010 45166 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4e50 0xc0039c4e51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.890: INFO: Pod "webserver-deployment-595b5b9587-xw2sf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xw2sf webserver-deployment-595b5b9587- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-595b5b9587-xw2sf ad506874-0db9-42df-8430-9279336d9cb7 45196 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 85e9373b-e9d0-41e7-bc2f-261cf879c15a 0xc0039c4fa7 0xc0039c4fa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.891: INFO: Pod "webserver-deployment-c7997dcc8-5h99s" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5h99s webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-5h99s 31a06047-9b47-4a69-9a98-14d492164e9a 45177 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5107 0xc0039c5108}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.891: INFO: Pod "webserver-deployment-c7997dcc8-796wb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-796wb webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-796wb a8cf89af-a97b-4066-861a-92b47d79e9d5 45171 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5280 0xc0039c5281}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.891: INFO: Pod "webserver-deployment-c7997dcc8-b7l9x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b7l9x webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-b7l9x c7d7f523-b314-4f13-9ebf-a34610b5d572 45189 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c53f0 0xc0039c53f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.891: INFO: Pod "webserver-deployment-c7997dcc8-cvhp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cvhp8 webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-cvhp8 739ca3fa-ed4e-4cec-bc55-4226ea184e34 45014 0 2020-08-18 18:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5560 0xc0039c5561}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.892: INFO: Pod "webserver-deployment-c7997dcc8-ktpth" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ktpth webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-ktpth 6da135ea-f065-41aa-a798-6f0476b2754c 45045 0 2020-08-18 18:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c56d0 0xc0039c56d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:,StartTime:2020-08-18 18:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.892: INFO: Pod "webserver-deployment-c7997dcc8-ltmln" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ltmln webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-ltmln 18622e1c-b835-4ad2-8a3f-08cc3fed200e 45210 0 2020-08-18 18:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5840 0xc0039c5841}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.207,StartTime:2020-08-18 18:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.892: INFO: Pod "webserver-deployment-c7997dcc8-m8bjx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-m8bjx webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-m8bjx cc14ba66-8719-467c-898c-ec7e1f897f7d 45208 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c59e0 0xc0039c59e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.892: INFO: Pod "webserver-deployment-c7997dcc8-mc9n8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mc9n8 webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-mc9n8 6c0858b6-24ae-4972-a221-43b3ac85d49e 45274 0 2020-08-18 18:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5b50 0xc0039c5b51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:172.30.13.110,StartTime:2020-08-18 18:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.13.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.893: INFO: Pod "webserver-deployment-c7997dcc8-ng6tk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ng6tk webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-ng6tk 4b14bcbd-7608-408a-9fe5-acd2e3080625 45200 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5cf0 0xc0039c5cf1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.893: INFO: Pod "webserver-deployment-c7997dcc8-pr4p5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pr4p5 webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-pr4p5 eeecdd48-28b1-449a-b7a4-c976c59c47d0 45224 0 2020-08-18 18:30:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0039c5e60 0xc0039c5e61}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:172.30.244.203,StartTime:2020-08-18 18:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.893: INFO: Pod "webserver-deployment-c7997dcc8-qb52g" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qb52g webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-qb52g b4aed27d-e25d-4403-a3b4-b56e292dbba9 45137 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0034c4000 0xc0034c4001}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.115,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.893: INFO: Pod "webserver-deployment-c7997dcc8-vcmdh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vcmdh webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-vcmdh 03ceb32a-836e-4cb0-a7bc-91c250c108db 45172 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0034c45d0 0xc0034c45d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.114,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 18 18:30:46.894: INFO: Pod "webserver-deployment-c7997dcc8-zx7dr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zx7dr webserver-deployment-c7997dcc8- deployment-9782 /api/v1/namespaces/deployment-9782/pods/webserver-deployment-c7997dcc8-zx7dr 4ec150fe-39f3-4e48-8848-8d90118f84d9 45169 0 2020-08-18 18:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 144de049-1a67-45f2-8982-f9c4de051fe0 0xc0034c4740 0xc0034c4741}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v825,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v825,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v825,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:46.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9782" for this suite.

• [SLOW TEST:8.886 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":129,"skipped":2291,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:46.962: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Aug 18 18:30:51.475: INFO: Pod pod-hostip-7bea3050-146a-4fdc-aeb7-512be6a057aa has hostIP: 10.13.3.84
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:51.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6580" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":130,"skipped":2293,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:51.525: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 18 18:30:51.768: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 18 18:30:51.830: INFO: Waiting for terminating namespaces to be deleted...
Aug 18 18:30:51.845: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.114 before test
Aug 18 18:30:51.928: INFO: ibm-keepalived-watcher-q59v8 from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.928: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:30:51.928: INFO: metrics-server-797d668946-ltqcd from kube-system started at 2020-08-18 15:52:24 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:51.928: INFO: 	Container metrics-server ready: true, restart count 0
Aug 18 18:30:51.928: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 18 18:30:51.928: INFO: webserver-deployment-595b5b9587-btkhw from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.928: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.928: INFO: webserver-deployment-595b5b9587-mt7pp from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.928: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.928: INFO: webserver-deployment-c7997dcc8-pr4p5 from deployment-9782 started at 2020-08-18 18:30:42 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:51.929: INFO: webserver-deployment-c7997dcc8-ktpth from deployment-9782 started at 2020-08-18 18:30:42 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:51.929: INFO: webserver-deployment-c7997dcc8-vcmdh from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:51.929: INFO: webserver-deployment-595b5b9587-xw2sf from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.929: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb from kube-system started at 2020-08-18 15:56:04 +0000 UTC (4 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:30:51.929: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:30:51.929: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:30:51.929: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:30:51.929: INFO: calico-kube-controllers-5754cfb59d-8hh6k from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 18 18:30:51.929: INFO: coredns-6567db4fff-6tjgz from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:30:51.929: INFO: coredns-6567db4fff-78ghm from kube-system started at 2020-08-18 17:45:40 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:30:51.929: INFO: webserver-deployment-595b5b9587-b94pj from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.929: INFO: calico-node-2ngjr from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:30:51.929: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-pw6pj from ibm-system started at 2020-08-18 17:41:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:30:51.929: INFO: vpn-f66c45467-kh4hm from kube-system started at 2020-08-18 16:16:37 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container vpn ready: true, restart count 0
Aug 18 18:30:51.929: INFO: webserver-deployment-595b5b9587-wbsn2 from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.929: INFO: ibm-master-proxy-static-10.13.3.114 from kube-system started at 2020-08-18 15:51:09 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:30:51.929: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:30:51.929: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:51.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:30:51.929: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:30:51.930: INFO: webserver-deployment-595b5b9587-rgv9j from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.930: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.930: INFO: webserver-deployment-c7997dcc8-b7l9x from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.930: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:51.930: INFO: webserver-deployment-595b5b9587-g9fzj from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:51.930: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:51.930: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.115 before test
Aug 18 18:30:52.023: INFO: ibm-file-plugin-ff7c989f9-fsrmg from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 18 18:30:52.023: INFO: sonobuoy-e2e-job-715a84e433104f63 from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container e2e ready: true, restart count 0
Aug 18 18:30:52.023: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:30:52.023: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-rb2hb from kube-system started at 2020-08-18 17:41:38 +0000 UTC (4 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:30:52.023: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:30:52.023: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:30:52.023: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-595b5b9587-nqhld from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-595b5b9587-ckqlp from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.023: INFO: coredns-6567db4fff-wxj8p from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-c7997dcc8-796wb from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.023: INFO: ibm-master-proxy-static-10.13.3.115 from kube-system started at 2020-08-18 15:51:10 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:30:52.023: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:30:52.023: INFO: calico-node-j6sx8 from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:30:52.023: INFO: dashboard-metrics-scraper-5789d44f58-nsshl from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 18 18:30:52.023: INFO: catalog-operator-67646bfcdb-77lhs from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 18 18:30:52.023: INFO: olm-operator-787498c9b7-9cmpc from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container olm-operator ready: true, restart count 0
Aug 18 18:30:52.023: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-g6ck9 from ibm-system started at 2020-08-18 15:52:58 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-c7997dcc8-5h99s from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-595b5b9587-chb8v from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-595b5b9587-xdh47 from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.023: INFO: ibm-keepalived-watcher-zktgc from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:30:52.023: INFO: kubernetes-dashboard-984c5c57-bl4dx from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 18 18:30:52.023: INFO: coredns-autoscaler-649976fbf4-q69hh from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container autoscaler ready: true, restart count 0
Aug 18 18:30:52.023: INFO: sonobuoy from sonobuoy started at 2020-08-18 17:39:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 18 18:30:52.023: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-cpwhn from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:30:52.023: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-595b5b9587-blzk7 from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-595b5b9587-b4tl9 from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.023: INFO: ibm-storage-watcher-56b6fd445c-dqt8p from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-c7997dcc8-mc9n8 from deployment-9782 started at 2020-08-18 18:30:42 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.023: INFO: webserver-deployment-c7997dcc8-qb52g from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.023: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.023: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.84 before test
Aug 18 18:30:52.089: INFO: webserver-deployment-c7997dcc8-zx7dr from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.089: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.089: INFO: webserver-deployment-595b5b9587-fjmvd from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.089: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.089: INFO: ibm-master-proxy-static-10.13.3.84 from kube-system started at 2020-08-18 15:51:51 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:52.089: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:30:52.089: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:30:52.089: INFO: ibm-keepalived-watcher-228kf from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.089: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:30:52.089: INFO: addon-catalog-source-62ppj from ibm-system started at 2020-08-18 15:52:32 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.089: INFO: 	Container configmap-registry-server ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-c7997dcc8-ltmln from deployment-9782 started at 2020-08-18 18:30:42 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.090: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-kdbbz from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:30:52.090: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:30:52.090: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-5022 started at 2020-08-18 18:30:27 +0000 UTC (2 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container busybox-1 ready: true, restart count 0
Aug 18 18:30:52.090: INFO: 	Container busybox-2 ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-c7997dcc8-m8bjx from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-595b5b9587-phkwm from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-595b5b9587-mc5pr from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.090: INFO: calico-node-b68l4 from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-595b5b9587-crdw6 from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-c7997dcc8-cvhp8 from deployment-9782 started at 2020-08-18 18:30:42 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-595b5b9587-xd76j from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.090: INFO: pod-hostip-7bea3050-146a-4fdc-aeb7-512be6a057aa from pods-6580 started at 2020-08-18 18:30:47 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container test ready: true, restart count 0
Aug 18 18:30:52.090: INFO: test-pod from e2e-kubelet-etc-hosts-5022 started at 2020-08-18 18:30:23 +0000 UTC (3 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container busybox-1 ready: true, restart count 0
Aug 18 18:30:52.090: INFO: 	Container busybox-2 ready: true, restart count 0
Aug 18 18:30:52.090: INFO: 	Container busybox-3 ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-595b5b9587-p76bk from deployment-9782 started at 2020-08-18 18:30:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: true, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-c7997dcc8-ng6tk from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: false, restart count 0
Aug 18 18:30:52.090: INFO: webserver-deployment-595b5b9587-k26h5 from deployment-9782 started at 2020-08-18 18:30:44 +0000 UTC (1 container statuses recorded)
Aug 18 18:30:52.090: INFO: 	Container httpd ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d59b8d67-a26b-47f9-9521-0c1f561714fb 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d59b8d67-a26b-47f9-9521-0c1f561714fb off the node 10.13.3.114
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d59b8d67-a26b-47f9-9521-0c1f561714fb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:30:56.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3143" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":131,"skipped":2296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:30:56.476: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-16b785c9-91eb-474f-8fdc-21482145dd06
STEP: Creating a pod to test consume secrets
Aug 18 18:30:56.816: INFO: Waiting up to 5m0s for pod "pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809" in namespace "secrets-4789" to be "success or failure"
Aug 18 18:30:56.829: INFO: Pod "pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809": Phase="Pending", Reason="", readiness=false. Elapsed: 12.8299ms
Aug 18 18:30:58.844: INFO: Pod "pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809": Phase="Running", Reason="", readiness=true. Elapsed: 2.028313666s
Aug 18 18:31:00.863: INFO: Pod "pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047113106s
STEP: Saw pod success
Aug 18 18:31:00.863: INFO: Pod "pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809" satisfied condition "success or failure"
Aug 18 18:31:00.878: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:31:01.211: INFO: Waiting for pod pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809 to disappear
Aug 18 18:31:01.225: INFO: Pod pod-secrets-1b66d2c2-a5e8-472f-acbb-e8e040c03809 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:31:01.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4789" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":132,"skipped":2379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:31:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:31:02.828: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:31:04.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372262, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372262, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372262, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372262, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:31:07.967: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:31:08.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6445" for this suite.
STEP: Destroying namespace "webhook-6445-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.402 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":133,"skipped":2405,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:31:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:31:15.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8267" for this suite.

• [SLOW TEST:6.468 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:31:15.153: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:31:15.809: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:31:19.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:31:19.132: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:31:49.814: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-6804-crd failed: Post https://e2e-test-crd-conversion-webhook.crd-webhook-181.svc:9443/crdconvert?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:31:50.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-181" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:36.097 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":135,"skipped":2433,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:31:51.251: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 18 18:31:51.965: INFO: Waiting up to 5m0s for pod "downward-api-18ab01fa-440a-4449-aba0-2fd691537001" in namespace "downward-api-5199" to be "success or failure"
Aug 18 18:31:51.979: INFO: Pod "downward-api-18ab01fa-440a-4449-aba0-2fd691537001": Phase="Pending", Reason="", readiness=false. Elapsed: 13.463987ms
Aug 18 18:31:53.992: INFO: Pod "downward-api-18ab01fa-440a-4449-aba0-2fd691537001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026897261s
Aug 18 18:31:56.006: INFO: Pod "downward-api-18ab01fa-440a-4449-aba0-2fd691537001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040395289s
STEP: Saw pod success
Aug 18 18:31:56.006: INFO: Pod "downward-api-18ab01fa-440a-4449-aba0-2fd691537001" satisfied condition "success or failure"
Aug 18 18:31:56.018: INFO: Trying to get logs from node 10.13.3.84 pod downward-api-18ab01fa-440a-4449-aba0-2fd691537001 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:31:56.111: INFO: Waiting for pod downward-api-18ab01fa-440a-4449-aba0-2fd691537001 to disappear
Aug 18 18:31:56.128: INFO: Pod downward-api-18ab01fa-440a-4449-aba0-2fd691537001 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:31:56.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5199" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":136,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:31:56.179: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4789 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4789;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4789 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4789;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4789.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4789.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4789.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4789.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4789.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4789.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4789.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.8.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.8.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.8.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.8.239_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4789 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4789;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4789 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4789;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4789.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4789.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4789.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4789.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4789.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4789.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4789.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4789.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4789.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.8.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.8.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.8.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.8.239_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:32:00.633: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.655: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.676: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.715: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.733: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.754: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.926: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.945: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.971: INFO: Unable to read jessie_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:00.989: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:01.007: INFO: Unable to read jessie_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:01.027: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:01.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:01.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:01.322: INFO: Lookups using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4789 wheezy_tcp@dns-test-service.dns-4789 wheezy_udp@dns-test-service.dns-4789.svc wheezy_tcp@dns-test-service.dns-4789.svc wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4789 jessie_tcp@dns-test-service.dns-4789 jessie_udp@dns-test-service.dns-4789.svc jessie_tcp@dns-test-service.dns-4789.svc jessie_udp@_http._tcp.dns-test-service.dns-4789.svc jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc]

Aug 18 18:32:06.343: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.365: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.387: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.408: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.427: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.448: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.471: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.493: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.634: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.654: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.673: INFO: Unable to read jessie_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.693: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.712: INFO: Unable to read jessie_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.751: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.773: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:06.915: INFO: Lookups using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4789 wheezy_tcp@dns-test-service.dns-4789 wheezy_udp@dns-test-service.dns-4789.svc wheezy_tcp@dns-test-service.dns-4789.svc wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4789 jessie_tcp@dns-test-service.dns-4789 jessie_udp@dns-test-service.dns-4789.svc jessie_tcp@dns-test-service.dns-4789.svc jessie_udp@_http._tcp.dns-test-service.dns-4789.svc jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc]

Aug 18 18:32:11.343: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.362: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.383: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.424: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.460: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.477: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.620: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.639: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.660: INFO: Unable to read jessie_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.674: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.694: INFO: Unable to read jessie_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.709: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.724: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.739: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:11.865: INFO: Lookups using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4789 wheezy_tcp@dns-test-service.dns-4789 wheezy_udp@dns-test-service.dns-4789.svc wheezy_tcp@dns-test-service.dns-4789.svc wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4789 jessie_tcp@dns-test-service.dns-4789 jessie_udp@dns-test-service.dns-4789.svc jessie_tcp@dns-test-service.dns-4789.svc jessie_udp@_http._tcp.dns-test-service.dns-4789.svc jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc]

Aug 18 18:32:16.345: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.366: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.385: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.405: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.459: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.476: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.617: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.638: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.656: INFO: Unable to read jessie_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.693: INFO: Unable to read jessie_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.714: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.731: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.751: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:16.881: INFO: Lookups using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4789 wheezy_tcp@dns-test-service.dns-4789 wheezy_udp@dns-test-service.dns-4789.svc wheezy_tcp@dns-test-service.dns-4789.svc wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4789 jessie_tcp@dns-test-service.dns-4789 jessie_udp@dns-test-service.dns-4789.svc jessie_tcp@dns-test-service.dns-4789.svc jessie_udp@_http._tcp.dns-test-service.dns-4789.svc jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc]

Aug 18 18:32:21.703: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.723: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.744: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.762: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.788: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.806: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.825: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.853: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:21.986: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.020: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.038: INFO: Unable to read jessie_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.058: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.075: INFO: Unable to read jessie_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.094: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.116: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.138: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:22.266: INFO: Lookups using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4789 wheezy_tcp@dns-test-service.dns-4789 wheezy_udp@dns-test-service.dns-4789.svc wheezy_tcp@dns-test-service.dns-4789.svc wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4789 jessie_tcp@dns-test-service.dns-4789 jessie_udp@dns-test-service.dns-4789.svc jessie_tcp@dns-test-service.dns-4789.svc jessie_udp@_http._tcp.dns-test-service.dns-4789.svc jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc]

Aug 18 18:32:26.345: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.364: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.384: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.404: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.429: INFO: Unable to read wheezy_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.449: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.470: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.644: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.672: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.700: INFO: Unable to read jessie_udp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.721: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789 from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.741: INFO: Unable to read jessie_udp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.761: INFO: Unable to read jessie_tcp@dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.781: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.800: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc from pod dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2: the server could not find the requested resource (get pods dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2)
Aug 18 18:32:26.935: INFO: Lookups using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4789 wheezy_tcp@dns-test-service.dns-4789 wheezy_udp@dns-test-service.dns-4789.svc wheezy_tcp@dns-test-service.dns-4789.svc wheezy_udp@_http._tcp.dns-test-service.dns-4789.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4789.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4789 jessie_tcp@dns-test-service.dns-4789 jessie_udp@dns-test-service.dns-4789.svc jessie_tcp@dns-test-service.dns-4789.svc jessie_udp@_http._tcp.dns-test-service.dns-4789.svc jessie_tcp@_http._tcp.dns-test-service.dns-4789.svc]

Aug 18 18:32:31.943: INFO: DNS probes using dns-4789/dns-test-94d67457-2a65-4a81-858f-9824ac7dbef2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:32:32.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4789" for this suite.

• [SLOW TEST:36.064 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":137,"skipped":2473,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:32:32.243: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-7c9b6d57-1beb-4951-8c02-2016e0c5a358
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:32:32.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4959" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":138,"skipped":2475,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:32:32.568: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6262
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 18 18:32:33.609: INFO: Waiting up to 5m0s for pod "pod-002d038c-8a56-44be-a0eb-951feb629512" in namespace "emptydir-6262" to be "success or failure"
Aug 18 18:32:33.623: INFO: Pod "pod-002d038c-8a56-44be-a0eb-951feb629512": Phase="Pending", Reason="", readiness=false. Elapsed: 14.123761ms
Aug 18 18:32:35.638: INFO: Pod "pod-002d038c-8a56-44be-a0eb-951feb629512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029137116s
Aug 18 18:32:37.660: INFO: Pod "pod-002d038c-8a56-44be-a0eb-951feb629512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051504522s
STEP: Saw pod success
Aug 18 18:32:37.661: INFO: Pod "pod-002d038c-8a56-44be-a0eb-951feb629512" satisfied condition "success or failure"
Aug 18 18:32:37.675: INFO: Trying to get logs from node 10.13.3.84 pod pod-002d038c-8a56-44be-a0eb-951feb629512 container test-container: <nil>
STEP: delete the pod
Aug 18 18:32:37.976: INFO: Waiting for pod pod-002d038c-8a56-44be-a0eb-951feb629512 to disappear
Aug 18 18:32:37.988: INFO: Pod pod-002d038c-8a56-44be-a0eb-951feb629512 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:32:37.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6262" for this suite.

• [SLOW TEST:5.480 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:32:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 18 18:32:39.090: INFO: Pod name wrapped-volume-race-c853f380-5830-4a2e-8372-58bb847ddf18: Found 0 pods out of 5
Aug 18 18:32:44.116: INFO: Pod name wrapped-volume-race-c853f380-5830-4a2e-8372-58bb847ddf18: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c853f380-5830-4a2e-8372-58bb847ddf18 in namespace emptydir-wrapper-6957, will wait for the garbage collector to delete the pods
Aug 18 18:32:44.296: INFO: Deleting ReplicationController wrapped-volume-race-c853f380-5830-4a2e-8372-58bb847ddf18 took: 33.85991ms
Aug 18 18:32:44.596: INFO: Terminating ReplicationController wrapped-volume-race-c853f380-5830-4a2e-8372-58bb847ddf18 pods took: 300.319239ms
STEP: Creating RC which spawns configmap-volume pods
Aug 18 18:32:50.843: INFO: Pod name wrapped-volume-race-cba8ae96-57c0-47c6-be96-36102ccfea4c: Found 0 pods out of 5
Aug 18 18:32:55.872: INFO: Pod name wrapped-volume-race-cba8ae96-57c0-47c6-be96-36102ccfea4c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cba8ae96-57c0-47c6-be96-36102ccfea4c in namespace emptydir-wrapper-6957, will wait for the garbage collector to delete the pods
Aug 18 18:32:56.112: INFO: Deleting ReplicationController wrapped-volume-race-cba8ae96-57c0-47c6-be96-36102ccfea4c took: 94.542296ms
Aug 18 18:32:56.312: INFO: Terminating ReplicationController wrapped-volume-race-cba8ae96-57c0-47c6-be96-36102ccfea4c pods took: 200.463269ms
STEP: Creating RC which spawns configmap-volume pods
Aug 18 18:33:11.085: INFO: Pod name wrapped-volume-race-cfc9ad5a-4d8c-4c28-98ff-6eebbd4d2043: Found 0 pods out of 5
Aug 18 18:33:16.117: INFO: Pod name wrapped-volume-race-cfc9ad5a-4d8c-4c28-98ff-6eebbd4d2043: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cfc9ad5a-4d8c-4c28-98ff-6eebbd4d2043 in namespace emptydir-wrapper-6957, will wait for the garbage collector to delete the pods
Aug 18 18:33:16.301: INFO: Deleting ReplicationController wrapped-volume-race-cfc9ad5a-4d8c-4c28-98ff-6eebbd4d2043 took: 37.579354ms
Aug 18 18:33:16.601: INFO: Terminating ReplicationController wrapped-volume-race-cfc9ad5a-4d8c-4c28-98ff-6eebbd4d2043 pods took: 300.265182ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:33:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6957" for this suite.

• [SLOW TEST:56.771 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":140,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:33:34.826: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Aug 18 18:33:35.133: INFO: Waiting up to 5m0s for pod "var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0" in namespace "var-expansion-7442" to be "success or failure"
Aug 18 18:33:35.149: INFO: Pod "var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.863089ms
Aug 18 18:33:37.162: INFO: Pod "var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029171963s
STEP: Saw pod success
Aug 18 18:33:37.162: INFO: Pod "var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0" satisfied condition "success or failure"
Aug 18 18:33:37.185: INFO: Trying to get logs from node 10.13.3.84 pod var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:33:37.269: INFO: Waiting for pod var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0 to disappear
Aug 18 18:33:37.280: INFO: Pod var-expansion-cc371ae5-46bb-4cb2-bb2b-99c4edd088a0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:33:37.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7442" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:33:37.341: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 18 18:33:37.625: INFO: Waiting up to 5m0s for pod "pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6" in namespace "emptydir-1242" to be "success or failure"
Aug 18 18:33:37.641: INFO: Pod "pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.092198ms
Aug 18 18:33:39.661: INFO: Pod "pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6": Phase="Running", Reason="", readiness=true. Elapsed: 2.035250999s
Aug 18 18:33:41.676: INFO: Pod "pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050181991s
STEP: Saw pod success
Aug 18 18:33:41.676: INFO: Pod "pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6" satisfied condition "success or failure"
Aug 18 18:33:41.689: INFO: Trying to get logs from node 10.13.3.84 pod pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6 container test-container: <nil>
STEP: delete the pod
Aug 18 18:33:41.772: INFO: Waiting for pod pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6 to disappear
Aug 18 18:33:41.785: INFO: Pod pod-4ddf5141-37ca-4c29-b60b-42674fd6cce6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:33:41.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1242" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2575,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:33:41.836: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:33:42.097: INFO: Creating ReplicaSet my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec
Aug 18 18:33:42.127: INFO: Pod name my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec: Found 0 pods out of 1
Aug 18 18:33:47.140: INFO: Pod name my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec: Found 1 pods out of 1
Aug 18 18:33:47.140: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec" is running
Aug 18 18:33:47.152: INFO: Pod "my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec-mzgbf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:33:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:33:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:33:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:33:42 +0000 UTC Reason: Message:}])
Aug 18 18:33:47.152: INFO: Trying to dial the pod
Aug 18 18:33:52.210: INFO: Controller my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec: Got expected result from replica 1 [my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec-mzgbf]: "my-hostname-basic-ea8e3951-c42b-4336-a2d2-0032f28d2eec-mzgbf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:33:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2829" for this suite.

• [SLOW TEST:10.425 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":143,"skipped":2590,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:33:52.262: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 18 18:33:52.973: INFO: Waiting up to 5m0s for pod "pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9" in namespace "emptydir-6896" to be "success or failure"
Aug 18 18:33:52.988: INFO: Pod "pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.203957ms
Aug 18 18:33:55.003: INFO: Pod "pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029812359s
STEP: Saw pod success
Aug 18 18:33:55.003: INFO: Pod "pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9" satisfied condition "success or failure"
Aug 18 18:33:55.015: INFO: Trying to get logs from node 10.13.3.84 pod pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9 container test-container: <nil>
STEP: delete the pod
Aug 18 18:33:55.125: INFO: Waiting for pod pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9 to disappear
Aug 18 18:33:55.138: INFO: Pod pod-fc63cad1-ea5d-49e5-82ce-80aeb81ccfb9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:33:55.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6896" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":144,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:33:55.183: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:33:55.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb" in namespace "downward-api-8681" to be "success or failure"
Aug 18 18:33:55.477: INFO: Pod "downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.393775ms
Aug 18 18:33:57.490: INFO: Pod "downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0287694s
STEP: Saw pod success
Aug 18 18:33:57.490: INFO: Pod "downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb" satisfied condition "success or failure"
Aug 18 18:33:57.504: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb container client-container: <nil>
STEP: delete the pod
Aug 18 18:33:57.579: INFO: Waiting for pod downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb to disappear
Aug 18 18:33:57.595: INFO: Pod downwardapi-volume-7847baa6-a515-411c-882c-0175b5be8ffb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:33:57.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8681" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:33:57.647: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 18 18:33:57.956: INFO: Created pod &Pod{ObjectMeta:{dns-2374  dns-2374 /api/v1/namespaces/dns-2374/pods/dns-2374 24da9056-77c4-4eb9-8425-9b5f88fdfaeb 47455 0 2020-08-18 18:33:57 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mr8q2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mr8q2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mr8q2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 18 18:34:01.988: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2374 PodName:dns-2374 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:34:01.988: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Verifying customized DNS server is configured on pod...
Aug 18 18:34:02.174: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2374 PodName:dns-2374 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:34:02.174: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:34:02.363: INFO: Deleting pod dns-2374...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:34:02.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2374" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":146,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:34:02.462: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 18 18:34:05.879: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:34:05.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6990" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:34:05.993: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:34:17.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7425" for this suite.

• [SLOW TEST:11.799 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":148,"skipped":2713,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:34:17.793: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:34:18.064: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 18 18:34:18.118: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 18 18:34:23.132: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 18 18:34:23.132: INFO: Creating deployment "test-rolling-update-deployment"
Aug 18 18:34:23.151: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 18 18:34:23.181: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 18 18:34:25.211: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 18 18:34:25.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372463, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372463, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372463, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372463, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:34:27.238: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 18 18:34:27.281: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8528 /apis/apps/v1/namespaces/deployment-8528/deployments/test-rolling-update-deployment 0c376081-db9d-4311-8bed-866cbf3b21f1 47710 1 2020-08-18 18:34:23 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0027db958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-18 18:34:23 +0000 UTC,LastTransitionTime:2020-08-18 18:34:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-08-18 18:34:26 +0000 UTC,LastTransitionTime:2020-08-18 18:34:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 18 18:34:27.298: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-8528 /apis/apps/v1/namespaces/deployment-8528/replicasets/test-rolling-update-deployment-67cf4f6444 a5551841-9f4a-4140-b209-ec527a160bbc 47699 1 2020-08-18 18:34:23 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0c376081-db9d-4311-8bed-866cbf3b21f1 0xc0028448c7 0xc0028448c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002844948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:34:27.298: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 18 18:34:27.298: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8528 /apis/apps/v1/namespaces/deployment-8528/replicasets/test-rolling-update-controller 4b3f2d8d-683c-4812-8126-2997372648b0 47709 2 2020-08-18 18:34:18 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0c376081-db9d-4311-8bed-866cbf3b21f1 0xc0028447f7 0xc0028447f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002844858 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:34:27.313: INFO: Pod "test-rolling-update-deployment-67cf4f6444-jf6xt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-jf6xt test-rolling-update-deployment-67cf4f6444- deployment-8528 /api/v1/namespaces/deployment-8528/pods/test-rolling-update-deployment-67cf4f6444-jf6xt bab39bed-851e-47bc-a026-77f0f9fb747b 47698 0 2020-08-18 18:34:23 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 a5551841-9f4a-4140-b209-ec527a160bbc 0xc002845117 0xc002845118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k2pwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k2pwp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k2pwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.235,StartTime:2020-08-18 18:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:34:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://853f88c6c82d732a0a60db7ba12548efed9c9d587b30b5e9fe4ea55e40436380,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:34:27.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8528" for this suite.

• [SLOW TEST:9.574 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":149,"skipped":2717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:34:27.368: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
STEP: creating an pod
Aug 18 18:34:27.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-3715 -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 18 18:34:27.866: INFO: stderr: ""
Aug 18 18:34:27.866: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Aug 18 18:34:27.866: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 18 18:34:27.866: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3715" to be "running and ready, or succeeded"
Aug 18 18:34:27.879: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.34475ms
Aug 18 18:34:29.896: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030359772s
Aug 18 18:34:31.910: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.044351377s
Aug 18 18:34:31.910: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 18 18:34:31.910: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 18 18:34:31.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs logs-generator logs-generator --namespace=kubectl-3715'
Aug 18 18:34:32.069: INFO: stderr: ""
Aug 18 18:34:32.069: INFO: stdout: "I0818 18:34:29.493429       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/6mw 352\nI0818 18:34:29.693593       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/cvzh 299\nI0818 18:34:29.893626       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/85m 313\nI0818 18:34:30.093651       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/cwc 351\nI0818 18:34:30.293624       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/7q4 261\nI0818 18:34:30.493628       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/pcz9 520\nI0818 18:34:30.693623       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/l6f 386\nI0818 18:34:30.893629       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/bfgb 544\nI0818 18:34:31.093617       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/cpr 479\nI0818 18:34:31.293623       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/zmmz 395\nI0818 18:34:31.493617       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/p5t 250\nI0818 18:34:31.693616       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/ng2 424\nI0818 18:34:31.893660       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/s4r 273\n"
STEP: limiting log lines
Aug 18 18:34:32.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs logs-generator logs-generator --namespace=kubectl-3715 --tail=1'
Aug 18 18:34:32.257: INFO: stderr: ""
Aug 18 18:34:32.257: INFO: stdout: "I0818 18:34:32.093619       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/x68l 429\n"
Aug 18 18:34:32.257: INFO: got output "I0818 18:34:32.093619       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/x68l 429\n"
STEP: limiting log bytes
Aug 18 18:34:32.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs logs-generator logs-generator --namespace=kubectl-3715 --limit-bytes=1'
Aug 18 18:34:32.394: INFO: stderr: ""
Aug 18 18:34:32.394: INFO: stdout: "I"
Aug 18 18:34:32.394: INFO: got output "I"
STEP: exposing timestamps
Aug 18 18:34:32.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs logs-generator logs-generator --namespace=kubectl-3715 --tail=1 --timestamps'
Aug 18 18:34:32.531: INFO: stderr: ""
Aug 18 18:34:32.531: INFO: stdout: "2020-08-18T18:34:32.493829057Z I0818 18:34:32.493665       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/c6br 489\n"
Aug 18 18:34:32.531: INFO: got output "2020-08-18T18:34:32.493829057Z I0818 18:34:32.493665       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/c6br 489\n"
STEP: restricting to a time range
Aug 18 18:34:35.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs logs-generator logs-generator --namespace=kubectl-3715 --since=1s'
Aug 18 18:34:36.203: INFO: stderr: ""
Aug 18 18:34:36.203: INFO: stdout: "I0818 18:34:35.293617       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/2l9 216\nI0818 18:34:35.493630       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/9xv9 236\nI0818 18:34:35.693616       1 logs_generator.go:76] 31 POST /api/v1/namespaces/ns/pods/nt82 385\nI0818 18:34:35.893632       1 logs_generator.go:76] 32 POST /api/v1/namespaces/ns/pods/zhw 381\nI0818 18:34:36.093607       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/ns/pods/hlgt 277\n"
Aug 18 18:34:36.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 logs logs-generator logs-generator --namespace=kubectl-3715 --since=24h'
Aug 18 18:34:36.721: INFO: stderr: ""
Aug 18 18:34:36.721: INFO: stdout: "I0818 18:34:29.493429       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/6mw 352\nI0818 18:34:29.693593       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/cvzh 299\nI0818 18:34:29.893626       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/85m 313\nI0818 18:34:30.093651       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/cwc 351\nI0818 18:34:30.293624       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/7q4 261\nI0818 18:34:30.493628       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/pcz9 520\nI0818 18:34:30.693623       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/l6f 386\nI0818 18:34:30.893629       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/bfgb 544\nI0818 18:34:31.093617       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/cpr 479\nI0818 18:34:31.293623       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/zmmz 395\nI0818 18:34:31.493617       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/p5t 250\nI0818 18:34:31.693616       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/ng2 424\nI0818 18:34:31.893660       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/s4r 273\nI0818 18:34:32.093619       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/x68l 429\nI0818 18:34:32.293619       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/h9d 503\nI0818 18:34:32.493665       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/c6br 489\nI0818 18:34:32.693628       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/z7h 208\nI0818 18:34:32.893667       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/hp7h 430\nI0818 18:34:33.093617       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/2kj9 401\nI0818 18:34:33.293651       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/2grj 526\nI0818 18:34:33.493636       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/qmv 285\nI0818 18:34:33.693616       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/nrc 565\nI0818 18:34:33.893625       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/gwx 226\nI0818 18:34:34.093642       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/gm6t 440\nI0818 18:34:34.293644       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/jlv8 548\nI0818 18:34:34.493612       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/85x 423\nI0818 18:34:34.693615       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/xdhr 334\nI0818 18:34:34.893629       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/wsk5 404\nI0818 18:34:35.093617       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/p8w 530\nI0818 18:34:35.293617       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/2l9 216\nI0818 18:34:35.493630       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/9xv9 236\nI0818 18:34:35.693616       1 logs_generator.go:76] 31 POST /api/v1/namespaces/ns/pods/nt82 385\nI0818 18:34:35.893632       1 logs_generator.go:76] 32 POST /api/v1/namespaces/ns/pods/zhw 381\nI0818 18:34:36.093607       1 logs_generator.go:76] 33 PUT /api/v1/namespaces/ns/pods/hlgt 277\nI0818 18:34:36.293618       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/ns/pods/f69 348\nI0818 18:34:36.493664       1 logs_generator.go:76] 35 POST /api/v1/namespaces/kube-system/pods/z5v 532\nI0818 18:34:36.694149       1 logs_generator.go:76] 36 GET /api/v1/namespaces/default/pods/xtg 318\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Aug 18 18:34:36.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete pod logs-generator --namespace=kubectl-3715'
Aug 18 18:34:41.388: INFO: stderr: ""
Aug 18 18:34:41.388: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:34:41.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3715" for this suite.

• [SLOW TEST:14.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":150,"skipped":2739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:34:41.438: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:34:41.700: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:34:45.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5395" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":151,"skipped":2764,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:34:45.915: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-530
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 18 18:34:46.161: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:34:49.970: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:09.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-530" for this suite.

• [SLOW TEST:23.639 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":152,"skipped":2774,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:09.555: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:35:09.816: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 18 18:35:12.156: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:13.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3569" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":153,"skipped":2777,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:13.233: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-2410/configmap-test-965912df-2f8b-4230-a2c7-66bdbbc6bd47
STEP: Creating a pod to test consume configMaps
Aug 18 18:35:13.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe" in namespace "configmap-2410" to be "success or failure"
Aug 18 18:35:14.004: INFO: Pod "pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.891505ms
Aug 18 18:35:16.018: INFO: Pod "pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027496212s
Aug 18 18:35:18.035: INFO: Pod "pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044437129s
STEP: Saw pod success
Aug 18 18:35:18.035: INFO: Pod "pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe" satisfied condition "success or failure"
Aug 18 18:35:18.050: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe container env-test: <nil>
STEP: delete the pod
Aug 18 18:35:18.135: INFO: Waiting for pod pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe to disappear
Aug 18 18:35:18.147: INFO: Pod pod-configmaps-981a9fb4-fa2a-4713-bc21-702d7660f6fe no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:18.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2410" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":154,"skipped":2778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:18.198: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c
Aug 18 18:35:18.513: INFO: Pod name my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c: Found 0 pods out of 1
Aug 18 18:35:23.529: INFO: Pod name my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c: Found 1 pods out of 1
Aug 18 18:35:23.529: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c" are running
Aug 18 18:35:23.543: INFO: Pod "my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c-v4qqk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:35:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:35:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:35:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-18 18:35:18 +0000 UTC Reason: Message:}])
Aug 18 18:35:23.543: INFO: Trying to dial the pod
Aug 18 18:35:28.889: INFO: Controller my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c: Got expected result from replica 1 [my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c-v4qqk]: "my-hostname-basic-55d911f4-ff09-4511-a058-4ec31ce2c44c-v4qqk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:28.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6081" for this suite.

• [SLOW TEST:10.735 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":155,"skipped":2800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:28.933: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Aug 18 18:35:29.215: INFO: Waiting up to 5m0s for pod "var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165" in namespace "var-expansion-5706" to be "success or failure"
Aug 18 18:35:29.252: INFO: Pod "var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165": Phase="Pending", Reason="", readiness=false. Elapsed: 36.936827ms
Aug 18 18:35:31.268: INFO: Pod "var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052892491s
STEP: Saw pod success
Aug 18 18:35:31.268: INFO: Pod "var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165" satisfied condition "success or failure"
Aug 18 18:35:31.283: INFO: Trying to get logs from node 10.13.3.84 pod var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:35:31.378: INFO: Waiting for pod var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165 to disappear
Aug 18 18:35:31.391: INFO: Pod var-expansion-62f058e2-e67b-46ae-861e-dbfbea4a1165 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:31.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5706" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2843,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:31.449: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:35.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7013" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2860,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:35.852: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 18 18:35:36.171: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 18 18:35:41.545: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:42.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4242" for this suite.

• [SLOW TEST:6.859 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":158,"skipped":2862,"failed":0}
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:42.712: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:51.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6699" for this suite.

• [SLOW TEST:8.681 seconds]
[sig-apps] Job
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":159,"skipped":2862,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:35:51.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316" in namespace "downward-api-5126" to be "success or failure"
Aug 18 18:35:51.709: INFO: Pod "downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316": Phase="Pending", Reason="", readiness=false. Elapsed: 15.881827ms
Aug 18 18:35:53.724: INFO: Pod "downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0304079s
STEP: Saw pod success
Aug 18 18:35:53.724: INFO: Pod "downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316" satisfied condition "success or failure"
Aug 18 18:35:53.740: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316 container client-container: <nil>
STEP: delete the pod
Aug 18 18:35:53.871: INFO: Waiting for pod downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316 to disappear
Aug 18 18:35:53.885: INFO: Pod downwardapi-volume-cf604f7c-4802-4c53-bab2-2cb8c1df4316 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:53.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5126" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2865,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:53.939: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Aug 18 18:35:54.253: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-222924798 proxy --unix-socket=/tmp/kubectl-proxy-unix496077873/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:54.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7538" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":161,"skipped":2868,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:54.370: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-dd780c12-fdca-4713-8b16-bfdd2f3f6e3b
STEP: Creating a pod to test consume secrets
Aug 18 18:35:54.676: INFO: Waiting up to 5m0s for pod "pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1" in namespace "secrets-8334" to be "success or failure"
Aug 18 18:35:54.690: INFO: Pod "pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.150918ms
Aug 18 18:35:56.703: INFO: Pod "pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026418348s
STEP: Saw pod success
Aug 18 18:35:56.703: INFO: Pod "pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1" satisfied condition "success or failure"
Aug 18 18:35:56.715: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:35:56.880: INFO: Waiting for pod pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1 to disappear
Aug 18 18:35:56.891: INFO: Pod pod-secrets-3f8c568f-62b3-45a1-b3e8-f72f6d823ae1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:35:56.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8334" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:35:56.951: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 18 18:35:57.221: INFO: Waiting up to 5m0s for pod "downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712" in namespace "downward-api-4515" to be "success or failure"
Aug 18 18:35:57.235: INFO: Pod "downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712": Phase="Pending", Reason="", readiness=false. Elapsed: 13.944463ms
Aug 18 18:35:59.248: INFO: Pod "downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027326818s
Aug 18 18:36:01.261: INFO: Pod "downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040246935s
STEP: Saw pod success
Aug 18 18:36:01.261: INFO: Pod "downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712" satisfied condition "success or failure"
Aug 18 18:36:01.277: INFO: Trying to get logs from node 10.13.3.84 pod downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:36:01.362: INFO: Waiting for pod downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712 to disappear
Aug 18 18:36:01.374: INFO: Pod downward-api-4ed5dc5e-1ab6-4fb2-8c73-5d7827332712 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:36:01.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4515" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2951,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:36:01.425: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Aug 18 18:36:01.749: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-222924798 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:36:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4311" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":164,"skipped":2951,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:36:02.281: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6124
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:36:02.808: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 18 18:36:06.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6124 create -f -'
Aug 18 18:36:07.198: INFO: stderr: ""
Aug 18 18:36:07.198: INFO: stdout: "e2e-test-crd-publish-openapi-3978-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 18 18:36:07.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6124 delete e2e-test-crd-publish-openapi-3978-crds test-cr'
Aug 18 18:36:07.346: INFO: stderr: ""
Aug 18 18:36:07.346: INFO: stdout: "e2e-test-crd-publish-openapi-3978-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 18 18:36:07.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6124 apply -f -'
Aug 18 18:36:07.889: INFO: stderr: ""
Aug 18 18:36:07.889: INFO: stdout: "e2e-test-crd-publish-openapi-3978-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 18 18:36:07.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-6124 delete e2e-test-crd-publish-openapi-3978-crds test-cr'
Aug 18 18:36:08.273: INFO: stderr: ""
Aug 18 18:36:08.273: INFO: stdout: "e2e-test-crd-publish-openapi-3978-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 18 18:36:08.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-3978-crds'
Aug 18 18:36:08.622: INFO: stderr: ""
Aug 18 18:36:08.622: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3978-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:36:12.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6124" for this suite.

• [SLOW TEST:10.172 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":165,"skipped":2965,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:36:12.454: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:36:12.744: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 18 18:36:17.969: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 18 18:36:17.970: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 18 18:36:20.074: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2303 /apis/apps/v1/namespaces/deployment-2303/deployments/test-cleanup-deployment e3b4e034-3843-4ac3-a57f-8f97917990a2 48826 1 2020-08-18 18:36:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00622a518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-18 18:36:18 +0000 UTC,LastTransitionTime:2020-08-18 18:36:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-08-18 18:36:19 +0000 UTC,LastTransitionTime:2020-08-18 18:36:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 18 18:36:20.093: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-2303 /apis/apps/v1/namespaces/deployment-2303/replicasets/test-cleanup-deployment-55ffc6b7b6 ac9a74eb-fd61-4753-a432-a72f04af6139 48815 1 2020-08-18 18:36:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e3b4e034-3843-4ac3-a57f-8f97917990a2 0xc00629adb7 0xc00629adb8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00629ae28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:36:20.107: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-zhj57" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-zhj57 test-cleanup-deployment-55ffc6b7b6- deployment-2303 /api/v1/namespaces/deployment-2303/pods/test-cleanup-deployment-55ffc6b7b6-zhj57 72558b53-0f1d-4790-aca4-5257452f227b 48814 0 2020-08-18 18:36:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 ac9a74eb-fd61-4753-a432-a72f04af6139 0xc00629b217 0xc00629b218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kkhzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kkhzs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kkhzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:36:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:36:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.251,StartTime:2020-08-18 18:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:36:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://9fab23831d9dcd9142bcb0cddde8eb98cfc10e0f06996556abe38c9b99fb34b9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:36:20.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2303" for this suite.

• [SLOW TEST:7.725 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":166,"skipped":2974,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:36:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-jnjz
STEP: Creating a pod to test atomic-volume-subpath
Aug 18 18:36:20.519: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jnjz" in namespace "subpath-8285" to be "success or failure"
Aug 18 18:36:20.542: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Pending", Reason="", readiness=false. Elapsed: 22.884169ms
Aug 18 18:36:22.559: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 2.040654299s
Aug 18 18:36:24.575: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 4.056218394s
Aug 18 18:36:26.591: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 6.072476101s
Aug 18 18:36:28.606: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 8.086807956s
Aug 18 18:36:30.621: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 10.102475501s
Aug 18 18:36:32.726: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 12.207191681s
Aug 18 18:36:34.741: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 14.221841547s
Aug 18 18:36:36.755: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 16.235900924s
Aug 18 18:36:38.769: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 18.250713332s
Aug 18 18:36:40.784: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Running", Reason="", readiness=true. Elapsed: 20.264980382s
Aug 18 18:36:42.798: INFO: Pod "pod-subpath-test-secret-jnjz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.279245899s
STEP: Saw pod success
Aug 18 18:36:42.798: INFO: Pod "pod-subpath-test-secret-jnjz" satisfied condition "success or failure"
Aug 18 18:36:42.812: INFO: Trying to get logs from node 10.13.3.84 pod pod-subpath-test-secret-jnjz container test-container-subpath-secret-jnjz: <nil>
STEP: delete the pod
Aug 18 18:36:42.977: INFO: Waiting for pod pod-subpath-test-secret-jnjz to disappear
Aug 18 18:36:42.991: INFO: Pod pod-subpath-test-secret-jnjz no longer exists
STEP: Deleting pod pod-subpath-test-secret-jnjz
Aug 18 18:36:42.991: INFO: Deleting pod "pod-subpath-test-secret-jnjz" in namespace "subpath-8285"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:36:43.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8285" for this suite.

• [SLOW TEST:22.885 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":167,"skipped":2976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:36:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:36:43.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-2179'
Aug 18 18:36:43.650: INFO: stderr: ""
Aug 18 18:36:43.650: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Aug 18 18:36:43.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-2179'
Aug 18 18:36:44.008: INFO: stderr: ""
Aug 18 18:36:44.008: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Aug 18 18:36:45.023: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:36:45.023: INFO: Found 0 / 1
Aug 18 18:36:46.025: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:36:46.025: INFO: Found 1 / 1
Aug 18 18:36:46.025: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 18 18:36:46.042: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 18 18:36:46.042: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 18 18:36:46.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 describe pod agnhost-master-qcg8b --namespace=kubectl-2179'
Aug 18 18:36:46.179: INFO: stderr: ""
Aug 18 18:36:46.179: INFO: stdout: "Name:         agnhost-master-qcg8b\nNamespace:    kubectl-2179\nPriority:     0\nNode:         10.13.3.84/10.13.3.84\nStart Time:   Tue, 18 Aug 2020 18:36:43 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.14.252\nIPs:\n  IP:           172.30.14.252\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://d5a871ac58b2d174c0279afdca80883d5cb4fdf220f23d2f76677af3add42a5a\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 18 Aug 2020 18:36:45 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nhcbc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-nhcbc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nhcbc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 600s\n                 node.kubernetes.io/unreachable:NoExecute for 600s\nEvents:\n  Type    Reason     Age   From                 Message\n  ----    ------     ----  ----                 -------\n  Normal  Scheduled  3s    default-scheduler    Successfully assigned kubectl-2179/agnhost-master-qcg8b to 10.13.3.84\n  Normal  Pulled     1s    kubelet, 10.13.3.84  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s    kubelet, 10.13.3.84  Created container agnhost-master\n  Normal  Started    1s    kubelet, 10.13.3.84  Started container agnhost-master\n"
Aug 18 18:36:46.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 describe rc agnhost-master --namespace=kubectl-2179'
Aug 18 18:36:46.334: INFO: stderr: ""
Aug 18 18:36:46.334: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2179\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-qcg8b\n"
Aug 18 18:36:46.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 describe service agnhost-master --namespace=kubectl-2179'
Aug 18 18:36:46.491: INFO: stderr: ""
Aug 18 18:36:46.491: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2179\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.21.9.93\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.14.252:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 18 18:36:46.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 describe node 10.13.3.114'
Aug 18 18:36:46.679: INFO: stderr: ""
Aug 18 18:36:46.679: INFO: stdout: "Name:               10.13.3.114\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=149.81.149.60\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.13.3.114\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bstv947f0fav3stm5itg-kubee2epvge-default-000002e3\n                    ibm-cloud.kubernetes.io/worker-pool-id=bstv947f0fav3stm5itg-1de35ca\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.17.11_1537\n                    ibm-cloud.kubernetes.io/zone=fra05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.13.3.114\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2723042\n                    publicVLAN=2723040\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 18 Aug 2020 15:51:11 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.13.3.114\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 18 Aug 2020 18:36:45 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 18 Aug 2020 18:33:24 +0000   Tue, 18 Aug 2020 15:51:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 18 Aug 2020 18:33:24 +0000   Tue, 18 Aug 2020 15:51:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 18 Aug 2020 18:33:24 +0000   Tue, 18 Aug 2020 15:51:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 18 Aug 2020 18:33:24 +0000   Tue, 18 Aug 2020 15:51:21 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.13.3.114\n  ExternalIP:  149.81.149.60\n  Hostname:    10.13.3.114\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102685624Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16419668Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  99892574949\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13627220Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 db1f3713ebcc4c0b9fb6536a1f8bd777\n  System UUID:                7173427F-D8B8-430E-6637-D6841242DC23\n  Boot ID:                    34d7d20d-14e2-42e0-b25f-869865223628\n  Kernel Version:             4.15.0-112-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.4\n  Kubelet Version:            v1.17.11+IKS\n  Kube-Proxy Version:         v1.17.11+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///bstv947f0fav3stm5itg/kube-bstv947f0fav3stm5itg-kubee2epvge-default-000002e3\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-pw6pj       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         55m\n  kube-system                 calico-kube-controllers-5754cfb59d-8hh6k                   10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      171m\n  kube-system                 calico-node-2ngjr                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         165m\n  kube-system                 coredns-6567db4fff-6tjgz                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     139m\n  kube-system                 coredns-6567db4fff-78ghm                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     51m\n  kube-system                 ibm-keepalived-watcher-q59v8                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         165m\n  kube-system                 ibm-master-proxy-static-10.13.3.114                        25m (0%)      300m (7%)   32M (0%)         512M (3%)      165m\n  kube-system                 metrics-server-797d668946-ltqcd                            121m (3%)     216m (5%)   186Mi (1%)       436Mi (3%)     164m\n  kube-system                 public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         160m\n  kube-system                 vpn-f66c45467-kh4hm                                        5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         140m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l    0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                631m (16%)     516m (13%)\n  memory             600594Ki (4%)  4911392Ki (36%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Aug 18 18:36:46.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 describe namespace kubectl-2179'
Aug 18 18:36:46.805: INFO: stderr: ""
Aug 18 18:36:46.805: INFO: stdout: "Name:         kubectl-2179\nLabels:       e2e-framework=kubectl\n              e2e-run=d6a52bd9-248c-4f24-bf94-bf6d9af69632\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:36:46.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2179" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":168,"skipped":3013,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:36:46.851: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7952
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-c3474b2d-9f3b-487e-8b19-4aaeec872bcb
STEP: Creating secret with name s-test-opt-upd-a342d5a7-be7b-401e-93e7-69dae746a910
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c3474b2d-9f3b-487e-8b19-4aaeec872bcb
STEP: Updating secret s-test-opt-upd-a342d5a7-be7b-401e-93e7-69dae746a910
STEP: Creating secret with name s-test-opt-create-f39544a7-9711-452d-84eb-dec31441c31e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:38:18.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7952" for this suite.

• [SLOW TEST:92.162 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":3015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:38:19.014: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-c6695398-5c23-4181-bfa6-db8fe2ce1dfe
STEP: Creating a pod to test consume secrets
Aug 18 18:38:19.310: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415" in namespace "projected-3876" to be "success or failure"
Aug 18 18:38:19.324: INFO: Pod "pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415": Phase="Pending", Reason="", readiness=false. Elapsed: 13.768851ms
Aug 18 18:38:21.338: INFO: Pod "pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028083626s
Aug 18 18:38:23.351: INFO: Pod "pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041489326s
STEP: Saw pod success
Aug 18 18:38:23.351: INFO: Pod "pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415" satisfied condition "success or failure"
Aug 18 18:38:23.365: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:38:23.450: INFO: Waiting for pod pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415 to disappear
Aug 18 18:38:23.464: INFO: Pod pod-projected-secrets-93af6675-31d4-46de-813b-0ee554440415 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:38:23.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3876" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":3045,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:38:23.515: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:38:27.905: INFO: Waiting up to 5m0s for pod "client-envvars-e861da6b-b143-438f-9d59-cf4154b89069" in namespace "pods-7637" to be "success or failure"
Aug 18 18:38:27.920: INFO: Pod "client-envvars-e861da6b-b143-438f-9d59-cf4154b89069": Phase="Pending", Reason="", readiness=false. Elapsed: 15.45987ms
Aug 18 18:38:29.933: INFO: Pod "client-envvars-e861da6b-b143-438f-9d59-cf4154b89069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027717125s
STEP: Saw pod success
Aug 18 18:38:29.933: INFO: Pod "client-envvars-e861da6b-b143-438f-9d59-cf4154b89069" satisfied condition "success or failure"
Aug 18 18:38:29.947: INFO: Trying to get logs from node 10.13.3.84 pod client-envvars-e861da6b-b143-438f-9d59-cf4154b89069 container env3cont: <nil>
STEP: delete the pod
Aug 18 18:38:30.062: INFO: Waiting for pod client-envvars-e861da6b-b143-438f-9d59-cf4154b89069 to disappear
Aug 18 18:38:30.076: INFO: Pod client-envvars-e861da6b-b143-438f-9d59-cf4154b89069 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:38:30.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7637" for this suite.

• [SLOW TEST:6.631 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":171,"skipped":3064,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:38:30.146: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Aug 18 18:38:30.602: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Aug 18 18:38:30.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-8602'
Aug 18 18:38:30.821: INFO: stderr: ""
Aug 18 18:38:30.821: INFO: stdout: "service/agnhost-slave created\n"
Aug 18 18:38:30.821: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Aug 18 18:38:30.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-8602'
Aug 18 18:38:31.034: INFO: stderr: ""
Aug 18 18:38:31.034: INFO: stdout: "service/agnhost-master created\n"
Aug 18 18:38:31.034: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 18 18:38:31.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-8602'
Aug 18 18:38:31.372: INFO: stderr: ""
Aug 18 18:38:31.372: INFO: stdout: "service/frontend created\n"
Aug 18 18:38:31.373: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 18 18:38:31.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-8602'
Aug 18 18:38:31.782: INFO: stderr: ""
Aug 18 18:38:31.782: INFO: stdout: "deployment.apps/frontend created\n"
Aug 18 18:38:31.783: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 18 18:38:31.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-8602'
Aug 18 18:38:32.353: INFO: stderr: ""
Aug 18 18:38:32.353: INFO: stdout: "deployment.apps/agnhost-master created\n"
Aug 18 18:38:32.353: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 18 18:38:32.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-8602'
Aug 18 18:38:32.726: INFO: stderr: ""
Aug 18 18:38:32.726: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Aug 18 18:38:32.726: INFO: Waiting for all frontend pods to be Running.
Aug 18 18:38:37.776: INFO: Waiting for frontend to serve content.
Aug 18 18:38:42.822: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Aug 18 18:38:47.863: INFO: Trying to add a new entry to the guestbook.
Aug 18 18:38:48.129: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 18 18:38:48.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-8602'
Aug 18 18:38:48.338: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:38:48.338: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 18 18:38:48.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-8602'
Aug 18 18:38:48.509: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:38:48.509: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 18 18:38:48.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-8602'
Aug 18 18:38:48.676: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:38:48.676: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 18 18:38:48.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-8602'
Aug 18 18:38:48.829: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:38:48.829: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 18 18:38:48.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-8602'
Aug 18 18:38:48.941: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:38:48.941: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 18 18:38:48.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-8602'
Aug 18 18:38:49.063: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:38:49.063: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:38:49.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8602" for this suite.

• [SLOW TEST:18.960 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:381
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":172,"skipped":3075,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:38:49.107: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 18 18:38:54.194: INFO: Successfully updated pod "pod-update-activedeadlineseconds-48519b41-3508-452e-94e8-178b41c79728"
Aug 18 18:38:54.194: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-48519b41-3508-452e-94e8-178b41c79728" in namespace "pods-1391" to be "terminated due to deadline exceeded"
Aug 18 18:38:54.207: INFO: Pod "pod-update-activedeadlineseconds-48519b41-3508-452e-94e8-178b41c79728": Phase="Running", Reason="", readiness=true. Elapsed: 12.668955ms
Aug 18 18:38:56.229: INFO: Pod "pod-update-activedeadlineseconds-48519b41-3508-452e-94e8-178b41c79728": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.034522803s
Aug 18 18:38:56.229: INFO: Pod "pod-update-activedeadlineseconds-48519b41-3508-452e-94e8-178b41c79728" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:38:56.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1391" for this suite.

• [SLOW TEST:7.177 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":3080,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:38:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3772.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3772.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3772.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3772.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3772.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3772.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:39:01.195: INFO: DNS probes using dns-3772/dns-test-2aae224c-f8ad-4319-9612-0743f2b61df2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:01.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3772" for this suite.

• [SLOW TEST:5.133 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":174,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:01.418: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9947
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:39:01.728: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 18 18:39:05.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-9947 create -f -'
Aug 18 18:39:06.260: INFO: stderr: ""
Aug 18 18:39:06.260: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 18 18:39:06.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-9947 delete e2e-test-crd-publish-openapi-9357-crds test-cr'
Aug 18 18:39:06.540: INFO: stderr: ""
Aug 18 18:39:06.540: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 18 18:39:06.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-9947 apply -f -'
Aug 18 18:39:06.931: INFO: stderr: ""
Aug 18 18:39:06.931: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 18 18:39:06.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 --namespace=crd-publish-openapi-9947 delete e2e-test-crd-publish-openapi-9357-crds test-cr'
Aug 18 18:39:07.132: INFO: stderr: ""
Aug 18 18:39:07.133: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 18 18:39:07.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 explain e2e-test-crd-publish-openapi-9357-crds'
Aug 18 18:39:07.315: INFO: stderr: ""
Aug 18 18:39:07.315: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9357-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:11.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9947" for this suite.

• [SLOW TEST:9.773 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":175,"skipped":3170,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:11.192: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 18 18:39:11.944: INFO: Number of nodes with available pods: 0
Aug 18 18:39:11.944: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:12.982: INFO: Number of nodes with available pods: 0
Aug 18 18:39:12.982: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:13.978: INFO: Number of nodes with available pods: 3
Aug 18 18:39:13.978: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 18 18:39:14.069: INFO: Number of nodes with available pods: 2
Aug 18 18:39:14.069: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:15.107: INFO: Number of nodes with available pods: 2
Aug 18 18:39:15.107: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:16.116: INFO: Number of nodes with available pods: 2
Aug 18 18:39:16.117: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:17.106: INFO: Number of nodes with available pods: 2
Aug 18 18:39:17.106: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:18.105: INFO: Number of nodes with available pods: 2
Aug 18 18:39:18.105: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:19.103: INFO: Number of nodes with available pods: 2
Aug 18 18:39:19.103: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:39:20.105: INFO: Number of nodes with available pods: 3
Aug 18 18:39:20.105: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5307, will wait for the garbage collector to delete the pods
Aug 18 18:39:20.235: INFO: Deleting DaemonSet.extensions daemon-set took: 46.312428ms
Aug 18 18:39:20.436: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.9535ms
Aug 18 18:39:31.451: INFO: Number of nodes with available pods: 0
Aug 18 18:39:31.451: INFO: Number of running nodes: 0, number of available pods: 0
Aug 18 18:39:31.466: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5307/daemonsets","resourceVersion":"50089"},"items":null}

Aug 18 18:39:31.479: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5307/pods","resourceVersion":"50089"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:31.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5307" for this suite.

• [SLOW TEST:20.416 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":176,"skipped":3177,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:31.608: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:39:32.548: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:39:34.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372772, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372772, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372772, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372772, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:39:37.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:37.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7611" for this suite.
STEP: Destroying namespace "webhook-7611-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.556 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":177,"skipped":3177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:38.167: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1796
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 18 18:39:38.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2395'
Aug 18 18:39:38.819: INFO: stderr: ""
Aug 18 18:39:38.819: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 18 18:39:43.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pod e2e-test-httpd-pod --namespace=kubectl-2395 -o json'
Aug 18 18:39:43.967: INFO: stderr: ""
Aug 18 18:39:43.967: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-08-18T18:39:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2395\",\n        \"resourceVersion\": \"50244\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2395/pods/e2e-test-httpd-pod\",\n        \"uid\": \"1e9b8018-cfbe-4285-8ace-6fe80b812d6b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9r952\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.13.3.84\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9r952\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9r952\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-18T18:39:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-18T18:39:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-18T18:39:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-18T18:39:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6e738ac475523e883f95a67d97104b6bb905e3eb62af66d81ba5d834c1d08c5b\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-08-18T18:39:40Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.13.3.84\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.14.205\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.14.205\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-18T18:39:38Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 18 18:39:43.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 replace -f - --namespace=kubectl-2395'
Aug 18 18:39:44.320: INFO: stderr: ""
Aug 18 18:39:44.320: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1801
Aug 18 18:39:44.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete pods e2e-test-httpd-pod --namespace=kubectl-2395'
Aug 18 18:39:45.927: INFO: stderr: ""
Aug 18 18:39:45.927: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:45.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2395" for this suite.

• [SLOW TEST:7.805 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1792
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":178,"skipped":3230,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:45.972: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Aug 18 18:39:46.258: INFO: Waiting up to 5m0s for pod "var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524" in namespace "var-expansion-6354" to be "success or failure"
Aug 18 18:39:46.271: INFO: Pod "var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524": Phase="Pending", Reason="", readiness=false. Elapsed: 12.640299ms
Aug 18 18:39:48.288: INFO: Pod "var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029813848s
STEP: Saw pod success
Aug 18 18:39:48.288: INFO: Pod "var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524" satisfied condition "success or failure"
Aug 18 18:39:48.301: INFO: Trying to get logs from node 10.13.3.84 pod var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:39:48.392: INFO: Waiting for pod var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524 to disappear
Aug 18 18:39:48.408: INFO: Pod var-expansion-3f6a5c8b-e329-4bb3-b39f-b6c40178c524 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:48.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6354" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":3236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:48.453: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-57a2abe6-8218-46a8-a04b-649b24157cd7
STEP: Creating a pod to test consume secrets
Aug 18 18:39:48.787: INFO: Waiting up to 5m0s for pod "pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c" in namespace "secrets-3757" to be "success or failure"
Aug 18 18:39:48.803: INFO: Pod "pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.640053ms
Aug 18 18:39:50.819: INFO: Pod "pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032054278s
Aug 18 18:39:52.831: INFO: Pod "pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044709236s
STEP: Saw pod success
Aug 18 18:39:52.832: INFO: Pod "pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c" satisfied condition "success or failure"
Aug 18 18:39:52.845: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:39:52.921: INFO: Waiting for pod pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c to disappear
Aug 18 18:39:52.933: INFO: Pod pod-secrets-3cd7a42b-26cf-4cb2-a45f-51e221c2d60c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:52.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3757" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":180,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:52.978: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-853f223f-3bf3-4393-93ec-ae63cfe48310
STEP: Creating a pod to test consume configMaps
Aug 18 18:39:53.279: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2" in namespace "projected-9765" to be "success or failure"
Aug 18 18:39:53.294: INFO: Pod "pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.40493ms
Aug 18 18:39:55.307: INFO: Pod "pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027804512s
Aug 18 18:39:57.322: INFO: Pod "pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042892463s
STEP: Saw pod success
Aug 18 18:39:57.322: INFO: Pod "pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2" satisfied condition "success or failure"
Aug 18 18:39:57.336: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:39:57.660: INFO: Waiting for pod pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2 to disappear
Aug 18 18:39:57.675: INFO: Pod pod-projected-configmaps-879c85e7-4e25-4f19-a43e-5febebdaa8b2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:57.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9765" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":3299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:57.728: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1048
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:39:57.991: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:39:59.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1048" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":182,"skipped":3325,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:39:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:39:59.469: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb" in namespace "projected-1966" to be "success or failure"
Aug 18 18:39:59.483: INFO: Pod "downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.231108ms
Aug 18 18:40:01.497: INFO: Pod "downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027606418s
STEP: Saw pod success
Aug 18 18:40:01.497: INFO: Pod "downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb" satisfied condition "success or failure"
Aug 18 18:40:01.511: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb container client-container: <nil>
STEP: delete the pod
Aug 18 18:40:01.595: INFO: Waiting for pod downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb to disappear
Aug 18 18:40:01.609: INFO: Pod downwardapi-volume-b4cc7f90-1324-4ca8-ba99-aadc99a954cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:40:01.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1966" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":3336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:40:01.685: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:40:13.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5343" for this suite.

• [SLOW TEST:11.538 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":184,"skipped":3363,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:40:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 18 18:40:18.445: INFO: Successfully updated pod "annotationupdatef68107d5-0a11-4dc8-b6f2-185285240f00"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:40:20.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8577" for this suite.

• [SLOW TEST:7.353 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":3371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:40:20.582: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-89ccb4c6-2d8e-4a6a-8fcc-ff4113a08bd9
STEP: Creating a pod to test consume configMaps
Aug 18 18:40:20.905: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e" in namespace "configmap-9213" to be "success or failure"
Aug 18 18:40:20.919: INFO: Pod "pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.85213ms
Aug 18 18:40:22.937: INFO: Pod "pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031280011s
Aug 18 18:40:24.950: INFO: Pod "pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044652495s
STEP: Saw pod success
Aug 18 18:40:24.950: INFO: Pod "pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e" satisfied condition "success or failure"
Aug 18 18:40:24.964: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:40:25.253: INFO: Waiting for pod pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e to disappear
Aug 18 18:40:25.266: INFO: Pod pod-configmaps-3f51e2fe-1dd6-4145-a14f-5b8e395da73e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:40:25.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9213" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":186,"skipped":3430,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:40:25.311: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:40:25.666: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 18 18:40:25.724: INFO: Number of nodes with available pods: 0
Aug 18 18:40:25.725: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:40:26.771: INFO: Number of nodes with available pods: 0
Aug 18 18:40:26.771: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:40:27.770: INFO: Number of nodes with available pods: 1
Aug 18 18:40:27.770: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:40:28.762: INFO: Number of nodes with available pods: 3
Aug 18 18:40:28.763: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 18 18:40:28.887: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:28.887: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:28.887: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:29.923: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:29.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:29.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:30.924: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:30.924: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:30.924: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:30.924: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:31.925: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:31.925: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:31.925: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:31.925: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:32.923: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:32.923: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:32.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:32.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:33.926: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:33.926: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:33.926: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:33.926: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:34.925: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:34.925: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:34.925: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:34.925: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:35.924: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:35.924: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:35.924: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:35.924: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:36.962: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:36.962: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:36.962: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:36.962: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:37.923: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:37.923: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:37.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:37.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:38.926: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:38.926: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:38.926: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:38.926: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:39.922: INFO: Wrong image for pod: daemon-set-66d9l. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:39.922: INFO: Pod daemon-set-66d9l is not available
Aug 18 18:40:39.922: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:39.922: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:40.924: INFO: Pod daemon-set-pgck4 is not available
Aug 18 18:40:40.924: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:40.924: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:41.923: INFO: Pod daemon-set-pgck4 is not available
Aug 18 18:40:41.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:41.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:42.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:42.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:43.922: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:43.922: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:44.921: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:44.921: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:44.921: INFO: Pod daemon-set-vjjct is not available
Aug 18 18:40:45.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:45.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:45.923: INFO: Pod daemon-set-vjjct is not available
Aug 18 18:40:46.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:46.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:46.923: INFO: Pod daemon-set-vjjct is not available
Aug 18 18:40:47.923: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:47.923: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:47.923: INFO: Pod daemon-set-vjjct is not available
Aug 18 18:40:48.924: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:48.924: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:48.924: INFO: Pod daemon-set-vjjct is not available
Aug 18 18:40:49.925: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:49.925: INFO: Wrong image for pod: daemon-set-vjjct. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:49.925: INFO: Pod daemon-set-vjjct is not available
Aug 18 18:40:50.924: INFO: Pod daemon-set-5vms4 is not available
Aug 18 18:40:50.924: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:51.921: INFO: Pod daemon-set-5vms4 is not available
Aug 18 18:40:51.921: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:52.921: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:53.921: INFO: Wrong image for pod: daemon-set-splbf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Aug 18 18:40:53.921: INFO: Pod daemon-set-splbf is not available
Aug 18 18:40:54.924: INFO: Pod daemon-set-8kmt5 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 18 18:40:54.974: INFO: Number of nodes with available pods: 2
Aug 18 18:40:54.974: INFO: Node 10.13.3.84 is running more than one daemon pod
Aug 18 18:40:56.009: INFO: Number of nodes with available pods: 2
Aug 18 18:40:56.009: INFO: Node 10.13.3.84 is running more than one daemon pod
Aug 18 18:40:57.004: INFO: Number of nodes with available pods: 3
Aug 18 18:40:57.004: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8584, will wait for the garbage collector to delete the pods
Aug 18 18:40:57.186: INFO: Deleting DaemonSet.extensions daemon-set took: 40.496606ms
Aug 18 18:40:57.386: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.386028ms
Aug 18 18:41:10.703: INFO: Number of nodes with available pods: 0
Aug 18 18:41:10.703: INFO: Number of running nodes: 0, number of available pods: 0
Aug 18 18:41:10.721: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8584/daemonsets","resourceVersion":"50956"},"items":null}

Aug 18 18:41:10.737: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8584/pods","resourceVersion":"50956"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:10.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8584" for this suite.

• [SLOW TEST:45.536 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":187,"skipped":3433,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:10.847: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:41:11.176: INFO: (0) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 50.994385ms)
Aug 18 18:41:11.200: INFO: (1) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.799032ms)
Aug 18 18:41:11.221: INFO: (2) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.370281ms)
Aug 18 18:41:11.241: INFO: (3) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.003585ms)
Aug 18 18:41:11.263: INFO: (4) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.881892ms)
Aug 18 18:41:11.288: INFO: (5) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.713987ms)
Aug 18 18:41:11.310: INFO: (6) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.183867ms)
Aug 18 18:41:11.336: INFO: (7) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.172699ms)
Aug 18 18:41:11.357: INFO: (8) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.203166ms)
Aug 18 18:41:11.375: INFO: (9) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.920101ms)
Aug 18 18:41:11.397: INFO: (10) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.253974ms)
Aug 18 18:41:11.419: INFO: (11) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.175885ms)
Aug 18 18:41:11.441: INFO: (12) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.933988ms)
Aug 18 18:41:11.463: INFO: (13) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.48549ms)
Aug 18 18:41:11.484: INFO: (14) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.612643ms)
Aug 18 18:41:11.503: INFO: (15) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 19.252947ms)
Aug 18 18:41:11.522: INFO: (16) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 18.982341ms)
Aug 18 18:41:11.544: INFO: (17) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.942343ms)
Aug 18 18:41:11.565: INFO: (18) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.664731ms)
Aug 18 18:41:11.593: INFO: (19) /api/v1/nodes/10.13.3.115:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 28.539598ms)
[AfterEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:11.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5088" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":188,"skipped":3436,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Aug 18 18:41:12.482: INFO: created pod pod-service-account-defaultsa
Aug 18 18:41:12.482: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 18 18:41:12.503: INFO: created pod pod-service-account-mountsa
Aug 18 18:41:12.503: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 18 18:41:12.524: INFO: created pod pod-service-account-nomountsa
Aug 18 18:41:12.524: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 18 18:41:12.539: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 18 18:41:12.539: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 18 18:41:12.553: INFO: created pod pod-service-account-mountsa-mountspec
Aug 18 18:41:12.553: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 18 18:41:12.567: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 18 18:41:12.567: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 18 18:41:12.582: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 18 18:41:12.582: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 18 18:41:12.598: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 18 18:41:12.598: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 18 18:41:12.617: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 18 18:41:12.617: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:12.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7351" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":189,"skipped":3445,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:12.669: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4708
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-84b41838-f620-487e-85c6-b1d5b7f27dfb
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-84b41838-f620-487e-85c6-b1d5b7f27dfb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:21.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4708" for this suite.

• [SLOW TEST:8.589 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3449,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:21.260: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 18 18:41:24.094: INFO: Successfully updated pod "adopt-release-2glb4"
STEP: Checking that the Job readopts the Pod
Aug 18 18:41:24.094: INFO: Waiting up to 15m0s for pod "adopt-release-2glb4" in namespace "job-5751" to be "adopted"
Aug 18 18:41:24.133: INFO: Pod "adopt-release-2glb4": Phase="Running", Reason="", readiness=true. Elapsed: 39.005978ms
Aug 18 18:41:26.148: INFO: Pod "adopt-release-2glb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.054629641s
Aug 18 18:41:26.149: INFO: Pod "adopt-release-2glb4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 18 18:41:26.713: INFO: Successfully updated pod "adopt-release-2glb4"
STEP: Checking that the Job releases the Pod
Aug 18 18:41:26.713: INFO: Waiting up to 15m0s for pod "adopt-release-2glb4" in namespace "job-5751" to be "released"
Aug 18 18:41:26.735: INFO: Pod "adopt-release-2glb4": Phase="Running", Reason="", readiness=true. Elapsed: 21.775384ms
Aug 18 18:41:28.749: INFO: Pod "adopt-release-2glb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.036062888s
Aug 18 18:41:28.749: INFO: Pod "adopt-release-2glb4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:28.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5751" for this suite.

• [SLOW TEST:7.538 seconds]
[sig-apps] Job
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":191,"skipped":3453,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:28.798: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:29.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6028" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":192,"skipped":3472,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:29.630: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:41:30.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:41:32.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372890, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372890, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372890, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733372890, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:41:35.677: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:41:35.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1639" for this suite.
STEP: Destroying namespace "webhook-1639-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.716 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":193,"skipped":3483,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:41:36.346: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-6eeb25be-1a62-4e2d-baf4-6fa08b4d0d8b in namespace container-probe-3784
Aug 18 18:41:40.668: INFO: Started pod busybox-6eeb25be-1a62-4e2d-baf4-6fa08b4d0d8b in namespace container-probe-3784
STEP: checking the pod's current state and verifying that restartCount is present
Aug 18 18:41:40.682: INFO: Initial restart count of pod busybox-6eeb25be-1a62-4e2d-baf4-6fa08b4d0d8b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:45:42.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3784" for this suite.

• [SLOW TEST:246.318 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3497,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:45:42.665: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:45:42.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-997" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":195,"skipped":3508,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:45:43.239: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:45:43.595: INFO: (0) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 64.444257ms)
Aug 18 18:45:43.617: INFO: (1) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.536578ms)
Aug 18 18:45:43.642: INFO: (2) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.41223ms)
Aug 18 18:45:43.668: INFO: (3) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.340623ms)
Aug 18 18:45:43.690: INFO: (4) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.630102ms)
Aug 18 18:45:43.710: INFO: (5) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.720986ms)
Aug 18 18:45:43.734: INFO: (6) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.655502ms)
Aug 18 18:45:43.757: INFO: (7) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.564075ms)
Aug 18 18:45:43.782: INFO: (8) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.201293ms)
Aug 18 18:45:43.804: INFO: (9) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.639735ms)
Aug 18 18:45:43.827: INFO: (10) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.607132ms)
Aug 18 18:45:43.851: INFO: (11) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.656818ms)
Aug 18 18:45:43.876: INFO: (12) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.015792ms)
Aug 18 18:45:43.898: INFO: (13) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.683912ms)
Aug 18 18:45:43.920: INFO: (14) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.643294ms)
Aug 18 18:45:43.941: INFO: (15) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.569452ms)
Aug 18 18:45:43.963: INFO: (16) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.315245ms)
Aug 18 18:45:43.984: INFO: (17) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 20.777871ms)
Aug 18 18:45:44.005: INFO: (18) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 21.33078ms)
Aug 18 18:45:44.030: INFO: (19) /api/v1/nodes/10.13.3.114/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.941955ms)
[AfterEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:45:44.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8531" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":196,"skipped":3528,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:45:44.077: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5952
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:45:44.342: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Creating first CR 
Aug 18 18:45:45.123: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-18T18:45:45Z generation:1 name:name1 resourceVersion:52198 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:00aed417-a8ed-4003-a857-0c59694f7763] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 18 18:45:55.143: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-18T18:45:55Z generation:1 name:name2 resourceVersion:52252 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:859cbf10-0b79-49d5-a140-00ffc72bbb69] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 18 18:46:05.166: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-18T18:45:45Z generation:2 name:name1 resourceVersion:52281 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:00aed417-a8ed-4003-a857-0c59694f7763] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 18 18:46:15.186: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-18T18:45:55Z generation:2 name:name2 resourceVersion:52308 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:859cbf10-0b79-49d5-a140-00ffc72bbb69] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 18 18:46:25.230: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-18T18:45:45Z generation:2 name:name1 resourceVersion:52333 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:00aed417-a8ed-4003-a857-0c59694f7763] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 18 18:46:35.274: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-18T18:45:55Z generation:2 name:name2 resourceVersion:52358 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:859cbf10-0b79-49d5-a140-00ffc72bbb69] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:46:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5952" for this suite.

• [SLOW TEST:61.943 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":197,"skipped":3532,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:46:46.021: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:46:46.868: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:46:48.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373206, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373206, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373206, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373206, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:46:51.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:46:52.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1386" for this suite.
STEP: Destroying namespace "webhook-1386-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.723 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":198,"skipped":3545,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:46:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Aug 18 18:46:53.231: INFO: Waiting up to 5m0s for pod "client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2" in namespace "containers-4759" to be "success or failure"
Aug 18 18:46:53.248: INFO: Pod "client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.642244ms
Aug 18 18:46:55.262: INFO: Pod "client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030958479s
STEP: Saw pod success
Aug 18 18:46:55.262: INFO: Pod "client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2" satisfied condition "success or failure"
Aug 18 18:46:55.278: INFO: Trying to get logs from node 10.13.3.84 pod client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2 container test-container: <nil>
STEP: delete the pod
Aug 18 18:46:55.378: INFO: Waiting for pod client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2 to disappear
Aug 18 18:46:55.390: INFO: Pod client-containers-709a43dd-bcc3-4986-98f5-233cafa2c9d2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:46:55.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4759" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:46:55.448: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 18 18:46:57.791: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:46:57.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2410" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3575,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:46:57.900: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:46:59.179: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Aug 18 18:47:01.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373219, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373219, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373219, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373219, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:47:04.311: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:47:04.328: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:47:34.942: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5701-crd failed: Post https://e2e-test-crd-conversion-webhook.crd-webhook-2798.svc:9443/crdconvert?timeout=30s: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:47:36.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2798" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:38.570 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":201,"skipped":3586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:47:36.470: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:47:36.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352" in namespace "projected-7619" to be "success or failure"
Aug 18 18:47:36.774: INFO: Pod "downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352": Phase="Pending", Reason="", readiness=false. Elapsed: 12.281218ms
Aug 18 18:47:38.789: INFO: Pod "downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02716475s
Aug 18 18:47:40.803: INFO: Pod "downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040555351s
STEP: Saw pod success
Aug 18 18:47:40.803: INFO: Pod "downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352" satisfied condition "success or failure"
Aug 18 18:47:40.816: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352 container client-container: <nil>
STEP: delete the pod
Aug 18 18:47:40.905: INFO: Waiting for pod downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352 to disappear
Aug 18 18:47:40.920: INFO: Pod downwardapi-volume-c985aa8b-d7f2-4cd5-8e00-0f2c19fa7352 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:47:40.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7619" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3610,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:47:40.964: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:47:58.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9473" for this suite.

• [SLOW TEST:17.846 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":203,"skipped":3616,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:47:58.812: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 18 18:48:29.950: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0818 18:48:29.950181      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 18 18:48:29.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2871" for this suite.

• [SLOW TEST:31.191 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":204,"skipped":3632,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:48:30.003: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-3534dfb4-ef32-4c45-a034-e704cba671cb
STEP: Creating a pod to test consume configMaps
Aug 18 18:48:30.321: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67" in namespace "projected-4426" to be "success or failure"
Aug 18 18:48:30.336: INFO: Pod "pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67": Phase="Pending", Reason="", readiness=false. Elapsed: 14.682107ms
Aug 18 18:48:32.351: INFO: Pod "pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029931333s
STEP: Saw pod success
Aug 18 18:48:32.351: INFO: Pod "pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67" satisfied condition "success or failure"
Aug 18 18:48:32.372: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:48:32.496: INFO: Waiting for pod pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67 to disappear
Aug 18 18:48:32.510: INFO: Pod pod-projected-configmaps-52d5b675-cade-44c9-9d5e-34463bb5dd67 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:48:32.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4426" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":205,"skipped":3636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:48:32.570: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7200
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 18 18:48:32.956: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 18 18:48:57.483: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.14.238:8080/dial?request=hostname&protocol=udp&host=172.30.244.242&port=8081&tries=1'] Namespace:pod-network-test-7200 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:48:57.483: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:48:57.675: INFO: Waiting for responses: map[]
Aug 18 18:48:57.969: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.14.238:8080/dial?request=hostname&protocol=udp&host=172.30.13.120&port=8081&tries=1'] Namespace:pod-network-test-7200 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:48:57.969: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:48:58.134: INFO: Waiting for responses: map[]
Aug 18 18:48:58.364: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.14.238:8080/dial?request=hostname&protocol=udp&host=172.30.14.233&port=8081&tries=1'] Namespace:pod-network-test-7200 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:48:58.364: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:48:58.495: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:48:58.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7200" for this suite.

• [SLOW TEST:25.978 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3658,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:48:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 18 18:49:05.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:05.233: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:07.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:07.249: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:09.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:09.247: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:11.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:11.249: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:13.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:13.247: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:15.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:15.247: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:17.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:17.248: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:19.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:19.247: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:21.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:21.248: INFO: Pod pod-with-poststart-http-hook still exists
Aug 18 18:49:23.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 18 18:49:23.248: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:49:23.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2641" for this suite.

• [SLOW TEST:24.751 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3679,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:49:23.300: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6233
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6233
STEP: creating replication controller externalsvc in namespace services-6233
I0818 18:49:23.786109      26 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6233, replica count: 2
I0818 18:49:26.836618      26 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 18 18:49:26.945: INFO: Creating new exec pod
Aug 18 18:49:29.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-6233 execpodt9vcg -- /bin/sh -x -c nslookup nodeport-service'
Aug 18 18:49:29.563: INFO: stderr: "+ nslookup nodeport-service\n"
Aug 18 18:49:29.563: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6233.svc.cluster.local\tcanonical name = externalsvc.services-6233.svc.cluster.local.\nName:\texternalsvc.services-6233.svc.cluster.local\nAddress: 172.21.119.9\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6233, will wait for the garbage collector to delete the pods
Aug 18 18:49:29.665: INFO: Deleting ReplicationController externalsvc took: 36.167327ms
Aug 18 18:49:29.865: INFO: Terminating ReplicationController externalsvc pods took: 200.261572ms
Aug 18 18:49:41.615: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:49:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6233" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:18.495 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":208,"skipped":3689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:49:41.795: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:49:42.779: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:49:44.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373382, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373382, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373382, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373382, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:49:47.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
Aug 18 18:49:58.713: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:49:59.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8388" for this suite.
STEP: Destroying namespace "webhook-8388-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.414 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":209,"skipped":3730,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:00.211: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3187
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-62e21c97-41f8-411f-ad46-f9e2e8288344
STEP: Creating configMap with name cm-test-opt-upd-33270d44-6e79-4a2f-a93f-2b92bc75889b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-62e21c97-41f8-411f-ad46-f9e2e8288344
STEP: Updating configmap cm-test-opt-upd-33270d44-6e79-4a2f-a93f-2b92bc75889b
STEP: Creating configMap with name cm-test-opt-create-40ea8608-f0a6-4ac8-adc0-49b301076e10
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:05.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3187" for this suite.

• [SLOW TEST:5.460 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3732,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:05.671: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-1bf6a433-165e-4dc5-9032-4df76ab82f82
STEP: Creating a pod to test consume configMaps
Aug 18 18:50:06.605: INFO: Waiting up to 5m0s for pod "pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db" in namespace "configmap-8267" to be "success or failure"
Aug 18 18:50:06.619: INFO: Pod "pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db": Phase="Pending", Reason="", readiness=false. Elapsed: 13.909193ms
Aug 18 18:50:08.634: INFO: Pod "pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028232038s
STEP: Saw pod success
Aug 18 18:50:08.634: INFO: Pod "pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db" satisfied condition "success or failure"
Aug 18 18:50:08.648: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:50:08.755: INFO: Waiting for pod pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db to disappear
Aug 18 18:50:08.769: INFO: Pod pod-configmaps-afe64678-b9bb-47c7-a4a6-0c109434d5db no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:08.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8267" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":211,"skipped":3732,"failed":0}
SSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:08.825: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-7609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Aug 18 18:50:09.122: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7609" to be "success or failure"
Aug 18 18:50:09.135: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 13.638441ms
Aug 18 18:50:11.149: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026745976s
Aug 18 18:50:13.162: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040137403s
STEP: Saw pod success
Aug 18 18:50:13.162: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 18 18:50:13.176: INFO: Trying to get logs from node 10.13.3.84 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 18 18:50:13.278: INFO: Waiting for pod pod-host-path-test to disappear
Aug 18 18:50:13.295: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:13.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7609" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":212,"skipped":3739,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:13.353: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:50:14.611: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:50:17.714: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:50:17.733: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4173-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:19.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4325" for this suite.
STEP: Destroying namespace "webhook-4325-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.325 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":213,"skipped":3741,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:19.678: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 18 18:50:24.565: INFO: Successfully updated pod "labelsupdate72f90433-3367-49ab-8e91-4c408032ef9f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:26.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8181" for this suite.

• [SLOW TEST:7.029 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3742,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:26.708: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-e5171369-21fb-4be0-9edd-1b1ebd4463d3
STEP: Creating a pod to test consume configMaps
Aug 18 18:50:27.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de" in namespace "configmap-3891" to be "success or failure"
Aug 18 18:50:27.243: INFO: Pod "pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de": Phase="Pending", Reason="", readiness=false. Elapsed: 16.245504ms
Aug 18 18:50:29.256: INFO: Pod "pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030234306s
Aug 18 18:50:31.271: INFO: Pod "pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044869478s
STEP: Saw pod success
Aug 18 18:50:31.271: INFO: Pod "pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de" satisfied condition "success or failure"
Aug 18 18:50:31.285: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:50:31.358: INFO: Waiting for pod pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de to disappear
Aug 18 18:50:31.375: INFO: Pod pod-configmaps-85e6881f-4d75-4c12-90b0-abba716de2de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:31.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3891" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3744,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:31.421: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:50:31.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95" in namespace "projected-5918" to be "success or failure"
Aug 18 18:50:31.713: INFO: Pod "downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95": Phase="Pending", Reason="", readiness=false. Elapsed: 14.598558ms
Aug 18 18:50:33.731: INFO: Pod "downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032118271s
STEP: Saw pod success
Aug 18 18:50:33.731: INFO: Pod "downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95" satisfied condition "success or failure"
Aug 18 18:50:33.749: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95 container client-container: <nil>
STEP: delete the pod
Aug 18 18:50:33.833: INFO: Waiting for pod downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95 to disappear
Aug 18 18:50:33.848: INFO: Pod downwardapi-volume-a6ba0d22-6a53-4080-b693-be48f780dd95 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:33.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5918" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":216,"skipped":3746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:33.903: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Aug 18 18:50:34.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 api-versions'
Aug 18 18:50:34.266: INFO: stderr: ""
Aug 18 18:50:34.266: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:34.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7079" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":217,"skipped":3779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:34.321: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 18 18:50:35.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8565 /api/v1/namespaces/watch-8565/configmaps/e2e-watch-test-resource-version 0cb369f9-f269-46f8-ae5f-8860d8cbd93a 54125 0 2020-08-18 18:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 18 18:50:35.117: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8565 /api/v1/namespaces/watch-8565/configmaps/e2e-watch-test-resource-version 0cb369f9-f269-46f8-ae5f-8860d8cbd93a 54126 0 2020-08-18 18:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:35.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8565" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":218,"skipped":3851,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:35.163: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-8051/secret-test-b4402713-b20b-46ad-bade-7ee3d0f19593
STEP: Creating a pod to test consume secrets
Aug 18 18:50:35.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50" in namespace "secrets-8051" to be "success or failure"
Aug 18 18:50:35.527: INFO: Pod "pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50": Phase="Pending", Reason="", readiness=false. Elapsed: 14.984162ms
Aug 18 18:50:37.544: INFO: Pod "pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031244622s
STEP: Saw pod success
Aug 18 18:50:37.544: INFO: Pod "pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50" satisfied condition "success or failure"
Aug 18 18:50:37.557: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50 container env-test: <nil>
STEP: delete the pod
Aug 18 18:50:37.657: INFO: Waiting for pod pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50 to disappear
Aug 18 18:50:37.673: INFO: Pod pod-configmaps-0695b789-6568-4dfa-97fb-13a474e87f50 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:50:37.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8051" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":219,"skipped":3854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:50:37.723: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:51:38.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-551" for this suite.

• [SLOW TEST:60.353 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3940,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:51:38.079: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:51:38.857: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 18:51:40.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373498, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373498, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373498, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373498, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:51:43.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 18 18:51:46.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 attach --namespace=webhook-957 to-be-attached-pod -i -c=container1'
Aug 18 18:51:46.233: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:51:46.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-957" for this suite.
STEP: Destroying namespace "webhook-957-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.521 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":221,"skipped":3942,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:51:46.600: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3502.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3502.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3502.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3502.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3502.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3502.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:51:51.083: INFO: DNS probes using dns-3502/dns-test-6428a236-7ee4-40ef-bc74-124df7048d82 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:51:51.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3502" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":222,"skipped":3955,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:51:51.403: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1216
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Aug 18 18:51:51.895: INFO: Waiting up to 5m0s for pod "downward-api-0360b0c4-aecc-4588-bd21-7d7124999356" in namespace "downward-api-1216" to be "success or failure"
Aug 18 18:51:51.908: INFO: Pod "downward-api-0360b0c4-aecc-4588-bd21-7d7124999356": Phase="Pending", Reason="", readiness=false. Elapsed: 13.603821ms
Aug 18 18:51:53.924: INFO: Pod "downward-api-0360b0c4-aecc-4588-bd21-7d7124999356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028664323s
STEP: Saw pod success
Aug 18 18:51:53.924: INFO: Pod "downward-api-0360b0c4-aecc-4588-bd21-7d7124999356" satisfied condition "success or failure"
Aug 18 18:51:53.941: INFO: Trying to get logs from node 10.13.3.84 pod downward-api-0360b0c4-aecc-4588-bd21-7d7124999356 container dapi-container: <nil>
STEP: delete the pod
Aug 18 18:51:54.027: INFO: Waiting for pod downward-api-0360b0c4-aecc-4588-bd21-7d7124999356 to disappear
Aug 18 18:51:54.041: INFO: Pod downward-api-0360b0c4-aecc-4588-bd21-7d7124999356 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:51:54.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1216" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":223,"skipped":3956,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:51:54.089: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:01.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8970" for this suite.

• [SLOW TEST:7.357 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":224,"skipped":3957,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:01.446: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1276
STEP: creating the pod
Aug 18 18:52:01.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 create -f - --namespace=kubectl-9510'
Aug 18 18:52:02.698: INFO: stderr: ""
Aug 18 18:52:02.698: INFO: stdout: "pod/pause created\n"
Aug 18 18:52:02.698: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 18 18:52:02.698: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9510" to be "running and ready"
Aug 18 18:52:02.713: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.763217ms
Aug 18 18:52:04.727: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02895421s
Aug 18 18:52:06.746: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.047439645s
Aug 18 18:52:06.746: INFO: Pod "pause" satisfied condition "running and ready"
Aug 18 18:52:06.746: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 18 18:52:06.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 label pods pause testing-label=testing-label-value --namespace=kubectl-9510'
Aug 18 18:52:07.888: INFO: stderr: ""
Aug 18 18:52:07.888: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 18 18:52:07.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pod pause -L testing-label --namespace=kubectl-9510'
Aug 18 18:52:07.995: INFO: stderr: ""
Aug 18 18:52:07.995: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 18 18:52:07.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 label pods pause testing-label- --namespace=kubectl-9510'
Aug 18 18:52:08.147: INFO: stderr: ""
Aug 18 18:52:08.147: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 18 18:52:08.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pod pause -L testing-label --namespace=kubectl-9510'
Aug 18 18:52:08.252: INFO: stderr: ""
Aug 18 18:52:08.252: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1283
STEP: using delete to clean up resources
Aug 18 18:52:08.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 delete --grace-period=0 --force -f - --namespace=kubectl-9510'
Aug 18 18:52:08.380: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 18 18:52:08.380: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 18 18:52:08.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get rc,svc -l name=pause --no-headers --namespace=kubectl-9510'
Aug 18 18:52:08.500: INFO: stderr: "No resources found in kubectl-9510 namespace.\n"
Aug 18 18:52:08.500: INFO: stdout: ""
Aug 18 18:52:08.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 get pods -l name=pause --namespace=kubectl-9510 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 18 18:52:08.599: INFO: stderr: ""
Aug 18 18:52:08.599: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:08.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9510" for this suite.

• [SLOW TEST:7.203 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1273
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":225,"skipped":3961,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:08.650: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 18 18:52:09.021: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8974 /api/v1/namespaces/watch-8974/configmaps/e2e-watch-test-watch-closed 35d184ef-c06b-4f3c-a7bf-3a3a4c3faa72 54737 0 2020-08-18 18:52:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 18 18:52:09.021: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8974 /api/v1/namespaces/watch-8974/configmaps/e2e-watch-test-watch-closed 35d184ef-c06b-4f3c-a7bf-3a3a4c3faa72 54738 0 2020-08-18 18:52:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 18 18:52:09.130: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8974 /api/v1/namespaces/watch-8974/configmaps/e2e-watch-test-watch-closed 35d184ef-c06b-4f3c-a7bf-3a3a4c3faa72 54739 0 2020-08-18 18:52:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 18 18:52:09.131: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8974 /api/v1/namespaces/watch-8974/configmaps/e2e-watch-test-watch-closed 35d184ef-c06b-4f3c-a7bf-3a3a4c3faa72 54740 0 2020-08-18 18:52:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:09.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8974" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":226,"skipped":3977,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:09.183: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2339
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-6b3cdaa1-29b7-40cc-84a9-54c5782e309e
STEP: Creating configMap with name cm-test-opt-upd-62ffebe0-3527-4809-b1c0-36aba53aa11d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6b3cdaa1-29b7-40cc-84a9-54c5782e309e
STEP: Updating configmap cm-test-opt-upd-62ffebe0-3527-4809-b1c0-36aba53aa11d
STEP: Creating configMap with name cm-test-opt-create-e01968f2-9a20-45ec-a30d-d75c96604c24
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:13.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2339" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":227,"skipped":3980,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:13.998: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 18 18:52:14.256: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 18 18:52:14.304: INFO: Waiting for terminating namespaces to be deleted...
Aug 18 18:52:14.321: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.114 before test
Aug 18 18:52:14.479: INFO: calico-node-2ngjr from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:52:14.479: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-pw6pj from ibm-system started at 2020-08-18 17:41:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:52:14.479: INFO: vpn-f66c45467-kh4hm from kube-system started at 2020-08-18 16:16:37 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container vpn ready: true, restart count 0
Aug 18 18:52:14.479: INFO: ibm-master-proxy-static-10.13.3.114 from kube-system started at 2020-08-18 15:51:09 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:52:14.479: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:52:14.479: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 18 18:52:14.479: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:52:14.479: INFO: ibm-keepalived-watcher-q59v8 from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:52:14.479: INFO: metrics-server-797d668946-ltqcd from kube-system started at 2020-08-18 15:52:24 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container metrics-server ready: true, restart count 0
Aug 18 18:52:14.479: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 18 18:52:14.479: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb from kube-system started at 2020-08-18 15:56:04 +0000 UTC (4 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:52:14.479: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:52:14.479: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:52:14.479: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:52:14.479: INFO: calico-kube-controllers-5754cfb59d-8hh6k from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 18 18:52:14.479: INFO: coredns-6567db4fff-6tjgz from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:52:14.479: INFO: coredns-6567db4fff-78ghm from kube-system started at 2020-08-18 17:45:40 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.479: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:52:14.479: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.115 before test
Aug 18 18:52:14.607: INFO: ibm-storage-watcher-56b6fd445c-dqt8p from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 18 18:52:14.607: INFO: ibm-file-plugin-ff7c989f9-fsrmg from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 18 18:52:14.607: INFO: sonobuoy-e2e-job-715a84e433104f63 from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container e2e ready: true, restart count 0
Aug 18 18:52:14.607: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:52:14.607: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-rb2hb from kube-system started at 2020-08-18 17:41:38 +0000 UTC (4 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:52:14.607: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:52:14.607: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:52:14.607: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:52:14.607: INFO: ibm-master-proxy-static-10.13.3.115 from kube-system started at 2020-08-18 15:51:10 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:52:14.607: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:52:14.607: INFO: calico-node-j6sx8 from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:52:14.607: INFO: dashboard-metrics-scraper-5789d44f58-nsshl from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 18 18:52:14.607: INFO: catalog-operator-67646bfcdb-77lhs from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 18 18:52:14.607: INFO: olm-operator-787498c9b7-9cmpc from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container olm-operator ready: true, restart count 0
Aug 18 18:52:14.607: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-g6ck9 from ibm-system started at 2020-08-18 15:52:58 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:52:14.607: INFO: coredns-6567db4fff-wxj8p from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:52:14.607: INFO: ibm-keepalived-watcher-zktgc from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.607: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:52:14.608: INFO: kubernetes-dashboard-984c5c57-bl4dx from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.608: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 18 18:52:14.608: INFO: coredns-autoscaler-649976fbf4-q69hh from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.608: INFO: 	Container autoscaler ready: true, restart count 0
Aug 18 18:52:14.608: INFO: sonobuoy from sonobuoy started at 2020-08-18 17:39:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.608: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 18 18:52:14.608: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-cpwhn from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.608: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 18 18:52:14.608: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:52:14.608: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.84 before test
Aug 18 18:52:14.635: INFO: calico-node-b68l4 from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.635: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:52:14.635: INFO: pod-projected-configmaps-ddca32de-8972-4efd-b378-2471bdbd94f6 from projected-2339 started at 2020-08-18 18:52:09 +0000 UTC (3 container statuses recorded)
Aug 18 18:52:14.635: INFO: 	Container createcm-volume-test ready: true, restart count 0
Aug 18 18:52:14.635: INFO: 	Container delcm-volume-test ready: true, restart count 0
Aug 18 18:52:14.635: INFO: 	Container updcm-volume-test ready: true, restart count 0
Aug 18 18:52:14.635: INFO: ibm-master-proxy-static-10.13.3.84 from kube-system started at 2020-08-18 15:51:51 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.635: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:52:14.635: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:52:14.635: INFO: ibm-keepalived-watcher-228kf from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.635: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:52:14.635: INFO: addon-catalog-source-62ppj from ibm-system started at 2020-08-18 15:52:32 +0000 UTC (1 container statuses recorded)
Aug 18 18:52:14.635: INFO: 	Container configmap-registry-server ready: true, restart count 0
Aug 18 18:52:14.635: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-kdbbz from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:52:14.635: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 18 18:52:14.635: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162c717990677deb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:15.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3839" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":228,"skipped":3998,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:15.813: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-85
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-pzvm
STEP: Creating a pod to test atomic-volume-subpath
Aug 18 18:52:16.157: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pzvm" in namespace "subpath-85" to be "success or failure"
Aug 18 18:52:16.167: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.186646ms
Aug 18 18:52:18.185: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 2.027313296s
Aug 18 18:52:20.199: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 4.04162529s
Aug 18 18:52:22.214: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 6.056247244s
Aug 18 18:52:24.230: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 8.072262136s
Aug 18 18:52:26.243: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 10.085698373s
Aug 18 18:52:28.257: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 12.099889547s
Aug 18 18:52:30.271: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 14.113855932s
Aug 18 18:52:32.287: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 16.129761304s
Aug 18 18:52:34.301: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 18.14371028s
Aug 18 18:52:36.315: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Running", Reason="", readiness=true. Elapsed: 20.157362138s
Aug 18 18:52:38.330: INFO: Pod "pod-subpath-test-downwardapi-pzvm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.172614233s
STEP: Saw pod success
Aug 18 18:52:38.330: INFO: Pod "pod-subpath-test-downwardapi-pzvm" satisfied condition "success or failure"
Aug 18 18:52:38.345: INFO: Trying to get logs from node 10.13.3.84 pod pod-subpath-test-downwardapi-pzvm container test-container-subpath-downwardapi-pzvm: <nil>
STEP: delete the pod
Aug 18 18:52:38.494: INFO: Waiting for pod pod-subpath-test-downwardapi-pzvm to disappear
Aug 18 18:52:38.509: INFO: Pod pod-subpath-test-downwardapi-pzvm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-pzvm
Aug 18 18:52:38.509: INFO: Deleting pod "pod-subpath-test-downwardapi-pzvm" in namespace "subpath-85"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:38.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-85" for this suite.

• [SLOW TEST:22.762 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":229,"skipped":4007,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:38.576: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 18 18:52:38.864: INFO: Waiting up to 5m0s for pod "pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0" in namespace "emptydir-496" to be "success or failure"
Aug 18 18:52:38.879: INFO: Pod "pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.289333ms
Aug 18 18:52:40.894: INFO: Pod "pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.029487214s
Aug 18 18:52:42.909: INFO: Pod "pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044603601s
STEP: Saw pod success
Aug 18 18:52:42.909: INFO: Pod "pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0" satisfied condition "success or failure"
Aug 18 18:52:42.923: INFO: Trying to get logs from node 10.13.3.84 pod pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0 container test-container: <nil>
STEP: delete the pod
Aug 18 18:52:42.998: INFO: Waiting for pod pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0 to disappear
Aug 18 18:52:43.011: INFO: Pod pod-23b97a39-48e4-4cbc-aff3-c1a7b65f95d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:43.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-496" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":4014,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:43.056: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-f4949aa7-2148-49e7-b09a-193d8a49b1ba
STEP: Creating a pod to test consume secrets
Aug 18 18:52:43.413: INFO: Waiting up to 5m0s for pod "pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4" in namespace "secrets-9870" to be "success or failure"
Aug 18 18:52:43.426: INFO: Pod "pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.384194ms
Aug 18 18:52:45.441: INFO: Pod "pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027454159s
Aug 18 18:52:47.455: INFO: Pod "pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041087107s
STEP: Saw pod success
Aug 18 18:52:47.455: INFO: Pod "pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4" satisfied condition "success or failure"
Aug 18 18:52:47.469: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:52:47.552: INFO: Waiting for pod pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4 to disappear
Aug 18 18:52:47.566: INFO: Pod pod-secrets-ff5b7383-a4d1-4de7-9e18-3ac6e7579ee4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:47.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9870" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":231,"skipped":4018,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:47.617: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-271
STEP: creating replication controller nodeport-test in namespace services-271
I0818 18:52:47.980174      26 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-271, replica count: 2
I0818 18:52:51.030789      26 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 18 18:52:51.031: INFO: Creating new exec pod
Aug 18 18:52:54.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-271 execpod5kn84 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 18 18:52:54.390: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 18 18:52:54.390: INFO: stdout: ""
Aug 18 18:52:54.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-271 execpod5kn84 -- /bin/sh -x -c nc -zv -t -w 2 172.21.84.250 80'
Aug 18 18:52:54.641: INFO: stderr: "+ nc -zv -t -w 2 172.21.84.250 80\nConnection to 172.21.84.250 80 port [tcp/http] succeeded!\n"
Aug 18 18:52:54.641: INFO: stdout: ""
Aug 18 18:52:54.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-271 execpod5kn84 -- /bin/sh -x -c nc -zv -t -w 2 10.13.3.115 32566'
Aug 18 18:52:54.913: INFO: stderr: "+ nc -zv -t -w 2 10.13.3.115 32566\nConnection to 10.13.3.115 32566 port [tcp/32566] succeeded!\n"
Aug 18 18:52:54.913: INFO: stdout: ""
Aug 18 18:52:54.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-271 execpod5kn84 -- /bin/sh -x -c nc -zv -t -w 2 10.13.3.114 32566'
Aug 18 18:52:55.168: INFO: stderr: "+ nc -zv -t -w 2 10.13.3.114 32566\nConnection to 10.13.3.114 32566 port [tcp/32566] succeeded!\n"
Aug 18 18:52:55.168: INFO: stdout: ""
Aug 18 18:52:55.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-271 execpod5kn84 -- /bin/sh -x -c nc -zv -t -w 2 149.81.149.62 32566'
Aug 18 18:52:55.402: INFO: stderr: "+ nc -zv -t -w 2 149.81.149.62 32566\nConnection to 149.81.149.62 32566 port [tcp/32566] succeeded!\n"
Aug 18 18:52:55.403: INFO: stdout: ""
Aug 18 18:52:55.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-271 execpod5kn84 -- /bin/sh -x -c nc -zv -t -w 2 149.81.149.60 32566'
Aug 18 18:52:55.807: INFO: stderr: "+ nc -zv -t -w 2 149.81.149.60 32566\nConnection to 149.81.149.60 32566 port [tcp/32566] succeeded!\n"
Aug 18 18:52:55.807: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:55.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-271" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.234 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":232,"skipped":4025,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:55.852: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:52:56.145: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833" in namespace "downward-api-2205" to be "success or failure"
Aug 18 18:52:56.189: INFO: Pod "downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833": Phase="Pending", Reason="", readiness=false. Elapsed: 43.892752ms
Aug 18 18:52:58.203: INFO: Pod "downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057723071s
STEP: Saw pod success
Aug 18 18:52:58.203: INFO: Pod "downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833" satisfied condition "success or failure"
Aug 18 18:52:58.217: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833 container client-container: <nil>
STEP: delete the pod
Aug 18 18:52:58.298: INFO: Waiting for pod downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833 to disappear
Aug 18 18:52:58.309: INFO: Pod downwardapi-volume-f5c4487c-5aaf-424e-958c-869392ec2833 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:52:58.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2205" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":4032,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:52:58.359: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7744
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 18 18:52:58.648: INFO: Waiting up to 5m0s for pod "pod-5245ae5d-2727-4a44-ba33-52fc85684820" in namespace "emptydir-7744" to be "success or failure"
Aug 18 18:52:58.661: INFO: Pod "pod-5245ae5d-2727-4a44-ba33-52fc85684820": Phase="Pending", Reason="", readiness=false. Elapsed: 12.910136ms
Aug 18 18:53:00.676: INFO: Pod "pod-5245ae5d-2727-4a44-ba33-52fc85684820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027974438s
STEP: Saw pod success
Aug 18 18:53:00.676: INFO: Pod "pod-5245ae5d-2727-4a44-ba33-52fc85684820" satisfied condition "success or failure"
Aug 18 18:53:00.690: INFO: Trying to get logs from node 10.13.3.84 pod pod-5245ae5d-2727-4a44-ba33-52fc85684820 container test-container: <nil>
STEP: delete the pod
Aug 18 18:53:00.988: INFO: Waiting for pod pod-5245ae5d-2727-4a44-ba33-52fc85684820 to disappear
Aug 18 18:53:01.016: INFO: Pod pod-5245ae5d-2727-4a44-ba33-52fc85684820 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:53:01.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7744" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":4044,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:53:01.063: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-3a6ee2e0-3c98-49a7-aa96-19b8901f9c49
STEP: Creating a pod to test consume configMaps
Aug 18 18:53:01.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb" in namespace "projected-4628" to be "success or failure"
Aug 18 18:53:01.373: INFO: Pod "pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.371451ms
Aug 18 18:53:03.386: INFO: Pod "pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027242016s
Aug 18 18:53:05.399: INFO: Pod "pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041064107s
STEP: Saw pod success
Aug 18 18:53:05.400: INFO: Pod "pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb" satisfied condition "success or failure"
Aug 18 18:53:05.414: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:53:05.489: INFO: Waiting for pod pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb to disappear
Aug 18 18:53:05.504: INFO: Pod pod-projected-configmaps-18f9ccc3-3538-4d4b-a28e-46d1d72a3bfb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:53:05.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4628" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":235,"skipped":4046,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:53:05.557: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Aug 18 18:53:05.855: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:53:08.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8512" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":236,"skipped":4047,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:53:08.728: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:53:09.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c" in namespace "downward-api-5620" to be "success or failure"
Aug 18 18:53:09.038: INFO: Pod "downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.575541ms
Aug 18 18:53:11.051: INFO: Pod "downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025773632s
STEP: Saw pod success
Aug 18 18:53:11.051: INFO: Pod "downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c" satisfied condition "success or failure"
Aug 18 18:53:11.072: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c container client-container: <nil>
STEP: delete the pod
Aug 18 18:53:11.156: INFO: Waiting for pod downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c to disappear
Aug 18 18:53:11.169: INFO: Pod downwardapi-volume-b28ef3d6-d14b-41d1-a3a8-24ff462dd06c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:53:11.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5620" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":237,"skipped":4065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:53:11.216: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7061
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 18 18:53:11.465: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 18 18:53:31.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.244.243:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7061 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:53:31.764: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:53:31.905: INFO: Found all expected endpoints: [netserver-0]
Aug 18 18:53:31.920: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.13.122:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7061 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:53:31.920: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:53:32.084: INFO: Found all expected endpoints: [netserver-1]
Aug 18 18:53:32.099: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.14.205:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7061 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:53:32.099: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:53:32.248: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:53:32.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7061" for this suite.

• [SLOW TEST:21.082 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":238,"skipped":4087,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:53:32.299: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4250
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 18 18:53:36.641: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4250 PodName:pod-sharedvolume-92a62679-c34d-4120-94ac-e2ef76c75816 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 18 18:53:36.641: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
Aug 18 18:53:36.808: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:53:36.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4250" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":239,"skipped":4105,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:53:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2256
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-a8e1c43a-8b9b-4ce6-8250-52836091fcf5
STEP: Creating secret with name s-test-opt-upd-70bcb97d-f402-4d47-9155-56912615bac7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a8e1c43a-8b9b-4ce6-8250-52836091fcf5
STEP: Updating secret s-test-opt-upd-70bcb97d-f402-4d47-9155-56912615bac7
STEP: Creating secret with name s-test-opt-create-a1c1b6f5-7454-4b9b-83ba-21d2ac5deec7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:54:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2256" for this suite.

• [SLOW TEST:75.464 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":4112,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:54:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1727
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5842
STEP: Creating secret with name secret-test-5a98e017-3d22-451e-b14c-7d7eeda0c91e
STEP: Creating a pod to test consume secrets
Aug 18 18:54:52.882: INFO: Waiting up to 5m0s for pod "pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375" in namespace "secrets-1727" to be "success or failure"
Aug 18 18:54:52.894: INFO: Pod "pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080959ms
Aug 18 18:54:54.915: INFO: Pod "pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032550938s
Aug 18 18:54:56.929: INFO: Pod "pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046994727s
STEP: Saw pod success
Aug 18 18:54:56.929: INFO: Pod "pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375" satisfied condition "success or failure"
Aug 18 18:54:56.945: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:54:57.267: INFO: Waiting for pod pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375 to disappear
Aug 18 18:54:57.281: INFO: Pod pod-secrets-3aa81e6d-4ded-404a-9334-9502d4281375 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:54:57.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1727" for this suite.
STEP: Destroying namespace "secret-namespace-5842" for this suite.

• [SLOW TEST:5.049 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":4115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:54:57.371: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-28
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-28
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-28
STEP: creating replication controller externalsvc in namespace services-28
I0818 18:54:57.742565      26 runners.go:189] Created replication controller with name: externalsvc, namespace: services-28, replica count: 2
I0818 18:55:00.793105      26 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 18 18:55:00.891: INFO: Creating new exec pod
Aug 18 18:55:04.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-28 execpod92ncw -- /bin/sh -x -c nslookup clusterip-service'
Aug 18 18:55:05.230: INFO: stderr: "+ nslookup clusterip-service\n"
Aug 18 18:55:05.230: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-28.svc.cluster.local\tcanonical name = externalsvc.services-28.svc.cluster.local.\nName:\texternalsvc.services-28.svc.cluster.local\nAddress: 172.21.90.145\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-28, will wait for the garbage collector to delete the pods
Aug 18 18:55:05.330: INFO: Deleting ReplicationController externalsvc took: 36.362375ms
Aug 18 18:55:05.531: INFO: Terminating ReplicationController externalsvc pods took: 200.286796ms
Aug 18 18:55:20.153: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:55:20.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-28" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:22.925 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":242,"skipped":4142,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:55:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-aba5dbdc-7b48-4629-b84d-d99ba4104aa7
STEP: Creating a pod to test consume secrets
Aug 18 18:55:20.621: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484" in namespace "projected-8023" to be "success or failure"
Aug 18 18:55:20.634: INFO: Pod "pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484": Phase="Pending", Reason="", readiness=false. Elapsed: 13.474096ms
Aug 18 18:55:22.649: INFO: Pod "pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027788152s
STEP: Saw pod success
Aug 18 18:55:22.649: INFO: Pod "pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484" satisfied condition "success or failure"
Aug 18 18:55:22.669: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:55:22.758: INFO: Waiting for pod pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484 to disappear
Aug 18 18:55:22.772: INFO: Pod pod-projected-secrets-71a8ba6e-0664-46ad-8ad5-88f0a9dad484 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:55:22.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8023" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":4143,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:55:22.830: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-d4c53121-31de-4e65-86fc-4941190c2b5b
STEP: Creating a pod to test consume configMaps
Aug 18 18:55:23.250: INFO: Waiting up to 5m0s for pod "pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6" in namespace "configmap-5786" to be "success or failure"
Aug 18 18:55:23.265: INFO: Pod "pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.968382ms
Aug 18 18:55:25.280: INFO: Pod "pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029192512s
STEP: Saw pod success
Aug 18 18:55:25.280: INFO: Pod "pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6" satisfied condition "success or failure"
Aug 18 18:55:25.297: INFO: Trying to get logs from node 10.13.3.84 pod pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 18:55:25.380: INFO: Waiting for pod pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6 to disappear
Aug 18 18:55:25.393: INFO: Pod pod-configmaps-af072aba-5222-420f-af9f-0ef8cef7ebd6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:55:25.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5786" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":244,"skipped":4150,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:55:25.448: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Aug 18 18:55:30.405: INFO: Successfully updated pod "labelsupdate3c82bf54-9ec1-4e29-98f3-3c7aa933f474"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:55:32.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8270" for this suite.

• [SLOW TEST:7.441 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":4154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:55:32.889: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 18:55:33.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac" in namespace "downward-api-258" to be "success or failure"
Aug 18 18:55:33.212: INFO: Pod "downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.850271ms
Aug 18 18:55:35.226: INFO: Pod "downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028792124s
STEP: Saw pod success
Aug 18 18:55:35.226: INFO: Pod "downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac" satisfied condition "success or failure"
Aug 18 18:55:35.238: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac container client-container: <nil>
STEP: delete the pod
Aug 18 18:55:35.325: INFO: Waiting for pod downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac to disappear
Aug 18 18:55:35.345: INFO: Pod downwardapi-volume-a8e12ce8-09b7-47d6-aa0c-8ba48a310dac no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:55:35.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-258" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":4179,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:55:35.625: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 18 18:55:39.978: INFO: &Pod{ObjectMeta:{send-events-40838cab-2565-4a19-9759-502289eb084f  events-365 /api/v1/namespaces/events-365/pods/send-events-40838cab-2565-4a19-9759-502289eb084f 92f68e52-09db-464e-a854-d96692b51e26 56308 0 2020-08-18 18:55:35 +0000 UTC <nil> <nil> map[name:foo time:889124223] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkd76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkd76,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkd76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:55:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:55:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:55:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:172.30.14.220,StartTime:2020-08-18 18:55:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-18 18:55:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://713beff5b5e51dcd66d3a8cdf6b7998b46719c7987f66a6c2e424091f7ca426b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.14.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 18 18:55:41.993: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 18 18:55:44.011: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:55:44.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-365" for this suite.

• [SLOW TEST:8.479 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":247,"skipped":4183,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:55:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 18 18:55:50.550: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:55:50.565: INFO: Pod pod-with-prestop-http-hook still exists
Aug 18 18:55:52.566: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:55:52.582: INFO: Pod pod-with-prestop-http-hook still exists
Aug 18 18:55:54.566: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:55:54.794: INFO: Pod pod-with-prestop-http-hook still exists
Aug 18 18:55:56.566: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:55:56.579: INFO: Pod pod-with-prestop-http-hook still exists
Aug 18 18:55:58.566: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:55:58.793: INFO: Pod pod-with-prestop-http-hook still exists
Aug 18 18:56:00.566: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:56:00.579: INFO: Pod pod-with-prestop-http-hook still exists
Aug 18 18:56:02.566: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 18 18:56:02.579: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:02.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-114" for this suite.

• [SLOW TEST:18.558 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":248,"skipped":4192,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:02.664: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-9459e7a9-26b7-4b11-836d-84b575e0e814
STEP: Creating a pod to test consume secrets
Aug 18 18:56:02.993: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17" in namespace "projected-6170" to be "success or failure"
Aug 18 18:56:03.007: INFO: Pod "pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17": Phase="Pending", Reason="", readiness=false. Elapsed: 14.325953ms
Aug 18 18:56:05.052: INFO: Pod "pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059632634s
Aug 18 18:56:07.065: INFO: Pod "pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072807277s
STEP: Saw pod success
Aug 18 18:56:07.066: INFO: Pod "pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17" satisfied condition "success or failure"
Aug 18 18:56:07.079: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 18:56:07.205: INFO: Waiting for pod pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17 to disappear
Aug 18 18:56:07.219: INFO: Pod pod-projected-secrets-76058cdc-6d49-4b7e-8645-fb49b3e79f17 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:07.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6170" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":4210,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:56:07.654: INFO: Create a RollingUpdate DaemonSet
Aug 18 18:56:07.671: INFO: Check that daemon pods launch on every node of the cluster
Aug 18 18:56:07.706: INFO: Number of nodes with available pods: 0
Aug 18 18:56:07.706: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:56:08.740: INFO: Number of nodes with available pods: 0
Aug 18 18:56:08.740: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:56:09.740: INFO: Number of nodes with available pods: 0
Aug 18 18:56:09.740: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 18:56:10.740: INFO: Number of nodes with available pods: 3
Aug 18 18:56:10.740: INFO: Number of running nodes: 3, number of available pods: 3
Aug 18 18:56:10.740: INFO: Update the DaemonSet to trigger a rollout
Aug 18 18:56:10.779: INFO: Updating DaemonSet daemon-set
Aug 18 18:56:14.850: INFO: Roll back the DaemonSet before rollout is complete
Aug 18 18:56:14.883: INFO: Updating DaemonSet daemon-set
Aug 18 18:56:14.883: INFO: Make sure DaemonSet rollback is complete
Aug 18 18:56:14.897: INFO: Wrong image for pod: daemon-set-4dhf4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 18 18:56:14.897: INFO: Pod daemon-set-4dhf4 is not available
Aug 18 18:56:15.933: INFO: Wrong image for pod: daemon-set-4dhf4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 18 18:56:15.933: INFO: Pod daemon-set-4dhf4 is not available
Aug 18 18:56:16.930: INFO: Pod daemon-set-njcl5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2267, will wait for the garbage collector to delete the pods
Aug 18 18:56:17.086: INFO: Deleting DaemonSet.extensions daemon-set took: 42.259465ms
Aug 18 18:56:17.286: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.327911ms
Aug 18 18:56:30.701: INFO: Number of nodes with available pods: 0
Aug 18 18:56:30.701: INFO: Number of running nodes: 0, number of available pods: 0
Aug 18 18:56:30.717: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2267/daemonsets","resourceVersion":"56714"},"items":null}

Aug 18 18:56:30.736: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2267/pods","resourceVersion":"56714"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:30.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2267" for this suite.

• [SLOW TEST:23.581 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":250,"skipped":4219,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:30.850: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-9891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9891 to expose endpoints map[]
Aug 18 18:56:31.165: INFO: Get endpoints failed (18.271265ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 18 18:56:32.184: INFO: successfully validated that service endpoint-test2 in namespace services-9891 exposes endpoints map[] (1.03715013s elapsed)
STEP: Creating pod pod1 in namespace services-9891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9891 to expose endpoints map[pod1:[80]]
Aug 18 18:56:34.317: INFO: successfully validated that service endpoint-test2 in namespace services-9891 exposes endpoints map[pod1:[80]] (2.100412069s elapsed)
STEP: Creating pod pod2 in namespace services-9891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9891 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 18 18:56:37.510: INFO: successfully validated that service endpoint-test2 in namespace services-9891 exposes endpoints map[pod1:[80] pod2:[80]] (3.176224591s elapsed)
STEP: Deleting pod pod1 in namespace services-9891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9891 to expose endpoints map[pod2:[80]]
Aug 18 18:56:38.608: INFO: successfully validated that service endpoint-test2 in namespace services-9891 exposes endpoints map[pod2:[80]] (1.067307837s elapsed)
STEP: Deleting pod pod2 in namespace services-9891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9891 to expose endpoints map[]
Aug 18 18:56:38.656: INFO: successfully validated that service endpoint-test2 in namespace services-9891 exposes endpoints map[] (16.087288ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:38.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9891" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.962 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":251,"skipped":4238,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:38.816: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-211
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 18 18:56:39.111: INFO: Waiting up to 5m0s for pod "pod-cee67296-08fd-49a1-9771-43a63a0e8b99" in namespace "emptydir-211" to be "success or failure"
Aug 18 18:56:39.133: INFO: Pod "pod-cee67296-08fd-49a1-9771-43a63a0e8b99": Phase="Pending", Reason="", readiness=false. Elapsed: 21.660221ms
Aug 18 18:56:41.159: INFO: Pod "pod-cee67296-08fd-49a1-9771-43a63a0e8b99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048040692s
Aug 18 18:56:43.175: INFO: Pod "pod-cee67296-08fd-49a1-9771-43a63a0e8b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0634734s
STEP: Saw pod success
Aug 18 18:56:43.175: INFO: Pod "pod-cee67296-08fd-49a1-9771-43a63a0e8b99" satisfied condition "success or failure"
Aug 18 18:56:43.187: INFO: Trying to get logs from node 10.13.3.84 pod pod-cee67296-08fd-49a1-9771-43a63a0e8b99 container test-container: <nil>
STEP: delete the pod
Aug 18 18:56:43.265: INFO: Waiting for pod pod-cee67296-08fd-49a1-9771-43a63a0e8b99 to disappear
Aug 18 18:56:43.277: INFO: Pod pod-cee67296-08fd-49a1-9771-43a63a0e8b99 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:43.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-211" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":4243,"failed":0}
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:43.326: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:56:43.572: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8769
I0818 18:56:43.593937      26 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8769, replica count: 1
I0818 18:56:44.644435      26 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0818 18:56:45.644707      26 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 18 18:56:45.789: INFO: Created: latency-svc-mwnx4
Aug 18 18:56:45.806: INFO: Got endpoints: latency-svc-mwnx4 [61.544134ms]
Aug 18 18:56:45.844: INFO: Created: latency-svc-8zrqn
Aug 18 18:56:45.860: INFO: Got endpoints: latency-svc-8zrqn [53.323621ms]
Aug 18 18:56:45.864: INFO: Created: latency-svc-j67kw
Aug 18 18:56:45.879: INFO: Got endpoints: latency-svc-j67kw [72.923765ms]
Aug 18 18:56:45.898: INFO: Created: latency-svc-hz46m
Aug 18 18:56:45.909: INFO: Got endpoints: latency-svc-hz46m [102.689931ms]
Aug 18 18:56:45.924: INFO: Created: latency-svc-vlgn5
Aug 18 18:56:45.942: INFO: Got endpoints: latency-svc-vlgn5 [135.281292ms]
Aug 18 18:56:45.950: INFO: Created: latency-svc-7jg57
Aug 18 18:56:45.967: INFO: Got endpoints: latency-svc-7jg57 [159.873405ms]
Aug 18 18:56:45.968: INFO: Created: latency-svc-6t6mm
Aug 18 18:56:45.981: INFO: Got endpoints: latency-svc-6t6mm [174.734445ms]
Aug 18 18:56:45.995: INFO: Created: latency-svc-fwsjr
Aug 18 18:56:46.010: INFO: Got endpoints: latency-svc-fwsjr [203.198906ms]
Aug 18 18:56:46.026: INFO: Created: latency-svc-rqgwr
Aug 18 18:56:46.043: INFO: Got endpoints: latency-svc-rqgwr [236.207578ms]
Aug 18 18:56:46.050: INFO: Created: latency-svc-4p2ss
Aug 18 18:56:46.068: INFO: Got endpoints: latency-svc-4p2ss [261.23927ms]
Aug 18 18:56:46.076: INFO: Created: latency-svc-kvwxd
Aug 18 18:56:46.092: INFO: Got endpoints: latency-svc-kvwxd [284.637615ms]
Aug 18 18:56:46.107: INFO: Created: latency-svc-ptgtv
Aug 18 18:56:46.125: INFO: Got endpoints: latency-svc-ptgtv [318.230183ms]
Aug 18 18:56:46.136: INFO: Created: latency-svc-tzqkg
Aug 18 18:56:46.153: INFO: Got endpoints: latency-svc-tzqkg [345.782276ms]
Aug 18 18:56:46.160: INFO: Created: latency-svc-76w8f
Aug 18 18:56:46.172: INFO: Created: latency-svc-6frrx
Aug 18 18:56:46.176: INFO: Got endpoints: latency-svc-76w8f [368.568993ms]
Aug 18 18:56:46.182: INFO: Got endpoints: latency-svc-6frrx [374.701701ms]
Aug 18 18:56:46.200: INFO: Created: latency-svc-ncw57
Aug 18 18:56:46.214: INFO: Got endpoints: latency-svc-ncw57 [407.53638ms]
Aug 18 18:56:46.225: INFO: Created: latency-svc-2m7gf
Aug 18 18:56:46.240: INFO: Got endpoints: latency-svc-2m7gf [379.25677ms]
Aug 18 18:56:46.245: INFO: Created: latency-svc-fldm2
Aug 18 18:56:46.260: INFO: Got endpoints: latency-svc-fldm2 [380.868884ms]
Aug 18 18:56:46.271: INFO: Created: latency-svc-ztqcl
Aug 18 18:56:46.285: INFO: Got endpoints: latency-svc-ztqcl [375.380695ms]
Aug 18 18:56:46.296: INFO: Created: latency-svc-d6h27
Aug 18 18:56:46.313: INFO: Got endpoints: latency-svc-d6h27 [371.180425ms]
Aug 18 18:56:46.320: INFO: Created: latency-svc-s4hjf
Aug 18 18:56:46.338: INFO: Got endpoints: latency-svc-s4hjf [371.721237ms]
Aug 18 18:56:46.345: INFO: Created: latency-svc-j7zvn
Aug 18 18:56:46.361: INFO: Got endpoints: latency-svc-j7zvn [379.676005ms]
Aug 18 18:56:46.372: INFO: Created: latency-svc-dbdrt
Aug 18 18:56:46.387: INFO: Got endpoints: latency-svc-dbdrt [376.597704ms]
Aug 18 18:56:46.396: INFO: Created: latency-svc-mqhkz
Aug 18 18:56:46.416: INFO: Got endpoints: latency-svc-mqhkz [373.08678ms]
Aug 18 18:56:46.420: INFO: Created: latency-svc-cwcxb
Aug 18 18:56:46.440: INFO: Got endpoints: latency-svc-cwcxb [371.471802ms]
Aug 18 18:56:46.442: INFO: Created: latency-svc-27cqh
Aug 18 18:56:46.459: INFO: Got endpoints: latency-svc-27cqh [366.969353ms]
Aug 18 18:56:46.464: INFO: Created: latency-svc-7wxnd
Aug 18 18:56:46.480: INFO: Got endpoints: latency-svc-7wxnd [354.939941ms]
Aug 18 18:56:46.493: INFO: Created: latency-svc-2lv5x
Aug 18 18:56:46.521: INFO: Got endpoints: latency-svc-2lv5x [368.319411ms]
Aug 18 18:56:46.522: INFO: Created: latency-svc-bjl5h
Aug 18 18:56:46.537: INFO: Created: latency-svc-bgstz
Aug 18 18:56:46.537: INFO: Got endpoints: latency-svc-bjl5h [361.792284ms]
Aug 18 18:56:46.557: INFO: Got endpoints: latency-svc-bgstz [375.456772ms]
Aug 18 18:56:46.567: INFO: Created: latency-svc-rjscf
Aug 18 18:56:46.584: INFO: Got endpoints: latency-svc-rjscf [369.492115ms]
Aug 18 18:56:46.588: INFO: Created: latency-svc-7x8fz
Aug 18 18:56:46.606: INFO: Got endpoints: latency-svc-7x8fz [365.787472ms]
Aug 18 18:56:46.612: INFO: Created: latency-svc-hcz74
Aug 18 18:56:46.629: INFO: Got endpoints: latency-svc-hcz74 [368.052504ms]
Aug 18 18:56:46.633: INFO: Created: latency-svc-vl9p4
Aug 18 18:56:46.648: INFO: Got endpoints: latency-svc-vl9p4 [362.898791ms]
Aug 18 18:56:46.657: INFO: Created: latency-svc-k6xsj
Aug 18 18:56:46.677: INFO: Got endpoints: latency-svc-k6xsj [364.218394ms]
Aug 18 18:56:46.685: INFO: Created: latency-svc-sjxzq
Aug 18 18:56:46.701: INFO: Got endpoints: latency-svc-sjxzq [362.602938ms]
Aug 18 18:56:46.708: INFO: Created: latency-svc-vpt6j
Aug 18 18:56:46.728: INFO: Got endpoints: latency-svc-vpt6j [366.703179ms]
Aug 18 18:56:46.735: INFO: Created: latency-svc-mhchr
Aug 18 18:56:46.752: INFO: Got endpoints: latency-svc-mhchr [365.607026ms]
Aug 18 18:56:46.759: INFO: Created: latency-svc-dgrrv
Aug 18 18:56:46.775: INFO: Got endpoints: latency-svc-dgrrv [358.563449ms]
Aug 18 18:56:47.009: INFO: Created: latency-svc-88tvm
Aug 18 18:56:47.027: INFO: Got endpoints: latency-svc-88tvm [586.915136ms]
Aug 18 18:56:47.033: INFO: Created: latency-svc-zr5rt
Aug 18 18:56:47.051: INFO: Got endpoints: latency-svc-zr5rt [592.273637ms]
Aug 18 18:56:47.065: INFO: Created: latency-svc-rhn6n
Aug 18 18:56:47.080: INFO: Got endpoints: latency-svc-rhn6n [599.273459ms]
Aug 18 18:56:47.096: INFO: Created: latency-svc-d8jc4
Aug 18 18:56:47.115: INFO: Got endpoints: latency-svc-d8jc4 [593.433272ms]
Aug 18 18:56:47.122: INFO: Created: latency-svc-dxgbz
Aug 18 18:56:47.142: INFO: Got endpoints: latency-svc-dxgbz [604.288351ms]
Aug 18 18:56:47.162: INFO: Created: latency-svc-hb2rg
Aug 18 18:56:47.176: INFO: Created: latency-svc-nmxvk
Aug 18 18:56:47.177: INFO: Got endpoints: latency-svc-hb2rg [620.182793ms]
Aug 18 18:56:47.194: INFO: Got endpoints: latency-svc-nmxvk [610.094118ms]
Aug 18 18:56:47.201: INFO: Created: latency-svc-mrrdr
Aug 18 18:56:47.226: INFO: Created: latency-svc-8ssfc
Aug 18 18:56:47.229: INFO: Got endpoints: latency-svc-mrrdr [623.454732ms]
Aug 18 18:56:47.245: INFO: Got endpoints: latency-svc-8ssfc [616.432798ms]
Aug 18 18:56:47.259: INFO: Created: latency-svc-th4d2
Aug 18 18:56:47.278: INFO: Got endpoints: latency-svc-th4d2 [630.069765ms]
Aug 18 18:56:47.281: INFO: Created: latency-svc-98mbl
Aug 18 18:56:47.295: INFO: Got endpoints: latency-svc-98mbl [617.809249ms]
Aug 18 18:56:47.304: INFO: Created: latency-svc-4r9wt
Aug 18 18:56:47.325: INFO: Created: latency-svc-pmz55
Aug 18 18:56:47.325: INFO: Got endpoints: latency-svc-4r9wt [624.462903ms]
Aug 18 18:56:47.353: INFO: Got endpoints: latency-svc-pmz55 [624.76128ms]
Aug 18 18:56:47.357: INFO: Created: latency-svc-mffc6
Aug 18 18:56:47.374: INFO: Created: latency-svc-w8xsp
Aug 18 18:56:47.377: INFO: Got endpoints: latency-svc-mffc6 [624.58606ms]
Aug 18 18:56:47.392: INFO: Got endpoints: latency-svc-w8xsp [617.27663ms]
Aug 18 18:56:47.398: INFO: Created: latency-svc-jj2q5
Aug 18 18:56:47.416: INFO: Got endpoints: latency-svc-jj2q5 [389.394036ms]
Aug 18 18:56:47.426: INFO: Created: latency-svc-qzsbm
Aug 18 18:56:47.441: INFO: Got endpoints: latency-svc-qzsbm [389.65898ms]
Aug 18 18:56:47.450: INFO: Created: latency-svc-gfv8r
Aug 18 18:56:47.470: INFO: Got endpoints: latency-svc-gfv8r [389.986694ms]
Aug 18 18:56:47.474: INFO: Created: latency-svc-lts2t
Aug 18 18:56:47.495: INFO: Got endpoints: latency-svc-lts2t [380.579988ms]
Aug 18 18:56:47.496: INFO: Created: latency-svc-qs4zs
Aug 18 18:56:47.517: INFO: Got endpoints: latency-svc-qs4zs [375.3773ms]
Aug 18 18:56:47.524: INFO: Created: latency-svc-bhszg
Aug 18 18:56:47.539: INFO: Got endpoints: latency-svc-bhszg [362.01574ms]
Aug 18 18:56:47.552: INFO: Created: latency-svc-6gmzm
Aug 18 18:56:47.575: INFO: Got endpoints: latency-svc-6gmzm [380.463043ms]
Aug 18 18:56:47.575: INFO: Created: latency-svc-qn7hw
Aug 18 18:56:47.593: INFO: Got endpoints: latency-svc-qn7hw [363.818602ms]
Aug 18 18:56:47.601: INFO: Created: latency-svc-vqvqs
Aug 18 18:56:47.617: INFO: Got endpoints: latency-svc-vqvqs [372.069659ms]
Aug 18 18:56:47.624: INFO: Created: latency-svc-dmcwz
Aug 18 18:56:47.638: INFO: Got endpoints: latency-svc-dmcwz [360.427599ms]
Aug 18 18:56:47.643: INFO: Created: latency-svc-6prnt
Aug 18 18:56:47.659: INFO: Got endpoints: latency-svc-6prnt [363.834535ms]
Aug 18 18:56:47.679: INFO: Created: latency-svc-ks6p2
Aug 18 18:56:47.701: INFO: Got endpoints: latency-svc-ks6p2 [375.783213ms]
Aug 18 18:56:47.703: INFO: Created: latency-svc-zkqss
Aug 18 18:56:47.716: INFO: Got endpoints: latency-svc-zkqss [363.479845ms]
Aug 18 18:56:47.727: INFO: Created: latency-svc-zhfqj
Aug 18 18:56:47.748: INFO: Got endpoints: latency-svc-zhfqj [370.669094ms]
Aug 18 18:56:47.756: INFO: Created: latency-svc-txsw6
Aug 18 18:56:47.775: INFO: Got endpoints: latency-svc-txsw6 [382.411766ms]
Aug 18 18:56:47.785: INFO: Created: latency-svc-gj6pq
Aug 18 18:56:47.801: INFO: Got endpoints: latency-svc-gj6pq [385.060246ms]
Aug 18 18:56:47.811: INFO: Created: latency-svc-zvdqk
Aug 18 18:56:47.828: INFO: Got endpoints: latency-svc-zvdqk [387.595749ms]
Aug 18 18:56:47.845: INFO: Created: latency-svc-lg8bv
Aug 18 18:56:47.863: INFO: Got endpoints: latency-svc-lg8bv [393.123029ms]
Aug 18 18:56:47.870: INFO: Created: latency-svc-d8bgf
Aug 18 18:56:47.888: INFO: Got endpoints: latency-svc-d8bgf [392.270156ms]
Aug 18 18:56:47.898: INFO: Created: latency-svc-w9gbd
Aug 18 18:56:47.914: INFO: Got endpoints: latency-svc-w9gbd [396.377244ms]
Aug 18 18:56:47.920: INFO: Created: latency-svc-7nn2l
Aug 18 18:56:47.939: INFO: Got endpoints: latency-svc-7nn2l [399.676548ms]
Aug 18 18:56:47.946: INFO: Created: latency-svc-gj82r
Aug 18 18:56:47.965: INFO: Got endpoints: latency-svc-gj82r [390.406348ms]
Aug 18 18:56:47.981: INFO: Created: latency-svc-dh8ll
Aug 18 18:56:48.021: INFO: Created: latency-svc-ps2j4
Aug 18 18:56:48.048: INFO: Got endpoints: latency-svc-ps2j4 [431.151785ms]
Aug 18 18:56:48.049: INFO: Got endpoints: latency-svc-dh8ll [455.380508ms]
Aug 18 18:56:48.053: INFO: Created: latency-svc-5658w
Aug 18 18:56:48.068: INFO: Got endpoints: latency-svc-5658w [429.813008ms]
Aug 18 18:56:48.088: INFO: Created: latency-svc-kj7cf
Aug 18 18:56:48.103: INFO: Got endpoints: latency-svc-kj7cf [443.353455ms]
Aug 18 18:56:48.122: INFO: Created: latency-svc-nkrz2
Aug 18 18:56:48.141: INFO: Got endpoints: latency-svc-nkrz2 [439.78202ms]
Aug 18 18:56:48.154: INFO: Created: latency-svc-jws9s
Aug 18 18:56:48.171: INFO: Got endpoints: latency-svc-jws9s [454.626906ms]
Aug 18 18:56:48.189: INFO: Created: latency-svc-gds5n
Aug 18 18:56:48.205: INFO: Got endpoints: latency-svc-gds5n [457.265033ms]
Aug 18 18:56:48.234: INFO: Created: latency-svc-2l7xj
Aug 18 18:56:48.251: INFO: Got endpoints: latency-svc-2l7xj [476.801008ms]
Aug 18 18:56:48.273: INFO: Created: latency-svc-28jcl
Aug 18 18:56:48.293: INFO: Got endpoints: latency-svc-28jcl [492.220056ms]
Aug 18 18:56:48.309: INFO: Created: latency-svc-7wn2t
Aug 18 18:56:48.328: INFO: Got endpoints: latency-svc-7wn2t [499.368873ms]
Aug 18 18:56:48.375: INFO: Created: latency-svc-9rmzm
Aug 18 18:56:48.390: INFO: Got endpoints: latency-svc-9rmzm [527.440123ms]
Aug 18 18:56:48.411: INFO: Created: latency-svc-kdkh2
Aug 18 18:56:48.431: INFO: Got endpoints: latency-svc-kdkh2 [543.059372ms]
Aug 18 18:56:48.461: INFO: Created: latency-svc-l922d
Aug 18 18:56:48.480: INFO: Got endpoints: latency-svc-l922d [566.563386ms]
Aug 18 18:56:48.507: INFO: Created: latency-svc-7zh2r
Aug 18 18:56:48.524: INFO: Got endpoints: latency-svc-7zh2r [584.206854ms]
Aug 18 18:56:48.549: INFO: Created: latency-svc-m9nhx
Aug 18 18:56:48.567: INFO: Got endpoints: latency-svc-m9nhx [601.932425ms]
Aug 18 18:56:48.596: INFO: Created: latency-svc-vj9q6
Aug 18 18:56:48.612: INFO: Got endpoints: latency-svc-vj9q6 [563.2187ms]
Aug 18 18:56:48.630: INFO: Created: latency-svc-b4s9w
Aug 18 18:56:48.648: INFO: Got endpoints: latency-svc-b4s9w [599.617729ms]
Aug 18 18:56:48.663: INFO: Created: latency-svc-rrwff
Aug 18 18:56:48.678: INFO: Got endpoints: latency-svc-rrwff [609.65651ms]
Aug 18 18:56:48.686: INFO: Created: latency-svc-n9kdt
Aug 18 18:56:48.701: INFO: Got endpoints: latency-svc-n9kdt [598.752556ms]
Aug 18 18:56:48.719: INFO: Created: latency-svc-ft45f
Aug 18 18:56:48.735: INFO: Got endpoints: latency-svc-ft45f [593.828027ms]
Aug 18 18:56:48.750: INFO: Created: latency-svc-rxdxf
Aug 18 18:56:48.767: INFO: Got endpoints: latency-svc-rxdxf [65.433988ms]
Aug 18 18:56:48.778: INFO: Created: latency-svc-sdq76
Aug 18 18:56:48.793: INFO: Got endpoints: latency-svc-sdq76 [621.763972ms]
Aug 18 18:56:48.810: INFO: Created: latency-svc-8tvd7
Aug 18 18:56:48.824: INFO: Got endpoints: latency-svc-8tvd7 [619.203503ms]
Aug 18 18:56:48.836: INFO: Created: latency-svc-4pbjk
Aug 18 18:56:48.853: INFO: Got endpoints: latency-svc-4pbjk [601.316051ms]
Aug 18 18:56:48.863: INFO: Created: latency-svc-qh5rx
Aug 18 18:56:48.886: INFO: Got endpoints: latency-svc-qh5rx [592.803105ms]
Aug 18 18:56:48.901: INFO: Created: latency-svc-gvd65
Aug 18 18:56:48.917: INFO: Got endpoints: latency-svc-gvd65 [589.129255ms]
Aug 18 18:56:48.942: INFO: Created: latency-svc-4fwr4
Aug 18 18:56:48.965: INFO: Got endpoints: latency-svc-4fwr4 [574.508918ms]
Aug 18 18:56:48.982: INFO: Created: latency-svc-l5lt9
Aug 18 18:56:48.997: INFO: Created: latency-svc-hh929
Aug 18 18:56:49.003: INFO: Got endpoints: latency-svc-l5lt9 [572.573435ms]
Aug 18 18:56:49.012: INFO: Got endpoints: latency-svc-hh929 [531.613264ms]
Aug 18 18:56:49.039: INFO: Created: latency-svc-88gfg
Aug 18 18:56:49.056: INFO: Got endpoints: latency-svc-88gfg [532.571458ms]
Aug 18 18:56:49.067: INFO: Created: latency-svc-2j7t9
Aug 18 18:56:49.086: INFO: Got endpoints: latency-svc-2j7t9 [518.868069ms]
Aug 18 18:56:49.101: INFO: Created: latency-svc-fxxbh
Aug 18 18:56:49.119: INFO: Got endpoints: latency-svc-fxxbh [507.447037ms]
Aug 18 18:56:49.132: INFO: Created: latency-svc-wj5z9
Aug 18 18:56:49.149: INFO: Got endpoints: latency-svc-wj5z9 [500.444299ms]
Aug 18 18:56:49.171: INFO: Created: latency-svc-5wz5z
Aug 18 18:56:49.186: INFO: Got endpoints: latency-svc-5wz5z [507.928033ms]
Aug 18 18:56:49.249: INFO: Created: latency-svc-l48vq
Aug 18 18:56:49.249: INFO: Created: latency-svc-bg2sm
Aug 18 18:56:49.274: INFO: Created: latency-svc-bs66z
Aug 18 18:56:49.278: INFO: Got endpoints: latency-svc-bg2sm [542.625459ms]
Aug 18 18:56:49.278: INFO: Got endpoints: latency-svc-l48vq [511.231803ms]
Aug 18 18:56:49.293: INFO: Got endpoints: latency-svc-bs66z [499.871663ms]
Aug 18 18:56:49.310: INFO: Created: latency-svc-ggk65
Aug 18 18:56:49.328: INFO: Got endpoints: latency-svc-ggk65 [504.145225ms]
Aug 18 18:56:49.361: INFO: Created: latency-svc-zzrxg
Aug 18 18:56:49.379: INFO: Got endpoints: latency-svc-zzrxg [526.656401ms]
Aug 18 18:56:49.388: INFO: Created: latency-svc-px44q
Aug 18 18:56:49.407: INFO: Got endpoints: latency-svc-px44q [520.687072ms]
Aug 18 18:56:49.425: INFO: Created: latency-svc-vsr7c
Aug 18 18:56:49.440: INFO: Got endpoints: latency-svc-vsr7c [523.487181ms]
Aug 18 18:56:49.455: INFO: Created: latency-svc-jm845
Aug 18 18:56:49.473: INFO: Got endpoints: latency-svc-jm845 [508.298025ms]
Aug 18 18:56:49.482: INFO: Created: latency-svc-5f625
Aug 18 18:56:49.500: INFO: Got endpoints: latency-svc-5f625 [496.831896ms]
Aug 18 18:56:49.509: INFO: Created: latency-svc-gj6gl
Aug 18 18:56:49.527: INFO: Got endpoints: latency-svc-gj6gl [515.066503ms]
Aug 18 18:56:49.541: INFO: Created: latency-svc-dk8wt
Aug 18 18:56:49.561: INFO: Got endpoints: latency-svc-dk8wt [504.606255ms]
Aug 18 18:56:49.573: INFO: Created: latency-svc-dh5jf
Aug 18 18:56:49.592: INFO: Got endpoints: latency-svc-dh5jf [506.400708ms]
Aug 18 18:56:49.610: INFO: Created: latency-svc-x25bj
Aug 18 18:56:49.628: INFO: Got endpoints: latency-svc-x25bj [508.298007ms]
Aug 18 18:56:49.647: INFO: Created: latency-svc-tr7l8
Aug 18 18:56:49.669: INFO: Got endpoints: latency-svc-tr7l8 [519.637706ms]
Aug 18 18:56:49.685: INFO: Created: latency-svc-5fqk8
Aug 18 18:56:49.707: INFO: Got endpoints: latency-svc-5fqk8 [520.551646ms]
Aug 18 18:56:49.733: INFO: Created: latency-svc-rwg22
Aug 18 18:56:49.755: INFO: Got endpoints: latency-svc-rwg22 [477.251435ms]
Aug 18 18:56:49.757: INFO: Created: latency-svc-pqbtj
Aug 18 18:56:49.777: INFO: Got endpoints: latency-svc-pqbtj [498.737533ms]
Aug 18 18:56:49.798: INFO: Created: latency-svc-6pb72
Aug 18 18:56:49.816: INFO: Got endpoints: latency-svc-6pb72 [523.157066ms]
Aug 18 18:56:49.836: INFO: Created: latency-svc-2nczc
Aug 18 18:56:49.852: INFO: Got endpoints: latency-svc-2nczc [523.074735ms]
Aug 18 18:56:49.863: INFO: Created: latency-svc-l2trf
Aug 18 18:56:49.878: INFO: Got endpoints: latency-svc-l2trf [498.370289ms]
Aug 18 18:56:49.925: INFO: Created: latency-svc-phj2d
Aug 18 18:56:49.943: INFO: Got endpoints: latency-svc-phj2d [536.002792ms]
Aug 18 18:56:50.218: INFO: Created: latency-svc-hr8rc
Aug 18 18:56:50.233: INFO: Got endpoints: latency-svc-hr8rc [792.213812ms]
Aug 18 18:56:50.279: INFO: Created: latency-svc-q6hh8
Aug 18 18:56:50.299: INFO: Got endpoints: latency-svc-q6hh8 [825.332253ms]
Aug 18 18:56:50.340: INFO: Created: latency-svc-twjgn
Aug 18 18:56:50.357: INFO: Got endpoints: latency-svc-twjgn [856.323018ms]
Aug 18 18:56:50.438: INFO: Created: latency-svc-crxpc
Aug 18 18:56:50.438: INFO: Created: latency-svc-gkzw4
Aug 18 18:56:50.457: INFO: Got endpoints: latency-svc-gkzw4 [895.513992ms]
Aug 18 18:56:50.458: INFO: Got endpoints: latency-svc-crxpc [930.707501ms]
Aug 18 18:56:50.482: INFO: Created: latency-svc-pvzbf
Aug 18 18:56:50.501: INFO: Got endpoints: latency-svc-pvzbf [907.705812ms]
Aug 18 18:56:50.531: INFO: Created: latency-svc-r98p7
Aug 18 18:56:50.550: INFO: Got endpoints: latency-svc-r98p7 [922.737684ms]
Aug 18 18:56:50.560: INFO: Created: latency-svc-fllj7
Aug 18 18:56:50.576: INFO: Got endpoints: latency-svc-fllj7 [907.801965ms]
Aug 18 18:56:50.612: INFO: Created: latency-svc-qr2rg
Aug 18 18:56:50.634: INFO: Got endpoints: latency-svc-qr2rg [927.452132ms]
Aug 18 18:56:50.645: INFO: Created: latency-svc-fmwmb
Aug 18 18:56:50.670: INFO: Created: latency-svc-2x9zb
Aug 18 18:56:50.674: INFO: Got endpoints: latency-svc-fmwmb [918.981194ms]
Aug 18 18:56:50.687: INFO: Got endpoints: latency-svc-2x9zb [910.633776ms]
Aug 18 18:56:50.714: INFO: Created: latency-svc-t7fh5
Aug 18 18:56:50.734: INFO: Got endpoints: latency-svc-t7fh5 [918.291307ms]
Aug 18 18:56:50.749: INFO: Created: latency-svc-kbkj2
Aug 18 18:56:50.766: INFO: Got endpoints: latency-svc-kbkj2 [914.322656ms]
Aug 18 18:56:50.783: INFO: Created: latency-svc-46xgc
Aug 18 18:56:50.799: INFO: Got endpoints: latency-svc-46xgc [920.975734ms]
Aug 18 18:56:50.813: INFO: Created: latency-svc-zhbcq
Aug 18 18:56:50.834: INFO: Got endpoints: latency-svc-zhbcq [890.725936ms]
Aug 18 18:56:50.840: INFO: Created: latency-svc-p4ftl
Aug 18 18:56:50.859: INFO: Got endpoints: latency-svc-p4ftl [625.747316ms]
Aug 18 18:56:50.871: INFO: Created: latency-svc-chdt9
Aug 18 18:56:50.887: INFO: Got endpoints: latency-svc-chdt9 [588.715538ms]
Aug 18 18:56:50.909: INFO: Created: latency-svc-nfqrx
Aug 18 18:56:50.923: INFO: Got endpoints: latency-svc-nfqrx [566.278313ms]
Aug 18 18:56:50.937: INFO: Created: latency-svc-h4gtm
Aug 18 18:56:50.957: INFO: Got endpoints: latency-svc-h4gtm [500.042788ms]
Aug 18 18:56:50.977: INFO: Created: latency-svc-cpsc8
Aug 18 18:56:50.994: INFO: Got endpoints: latency-svc-cpsc8 [536.282942ms]
Aug 18 18:56:51.006: INFO: Created: latency-svc-4xqqj
Aug 18 18:56:51.034: INFO: Created: latency-svc-jb668
Aug 18 18:56:51.041: INFO: Got endpoints: latency-svc-4xqqj [540.510565ms]
Aug 18 18:56:51.053: INFO: Got endpoints: latency-svc-jb668 [502.756389ms]
Aug 18 18:56:51.061: INFO: Created: latency-svc-4cqpt
Aug 18 18:56:51.071: INFO: Got endpoints: latency-svc-4cqpt [494.046373ms]
Aug 18 18:56:51.083: INFO: Created: latency-svc-qlrt9
Aug 18 18:56:51.099: INFO: Got endpoints: latency-svc-qlrt9 [465.096618ms]
Aug 18 18:56:51.112: INFO: Created: latency-svc-h5m5l
Aug 18 18:56:51.127: INFO: Got endpoints: latency-svc-h5m5l [452.424789ms]
Aug 18 18:56:51.149: INFO: Created: latency-svc-4tj5d
Aug 18 18:56:51.165: INFO: Got endpoints: latency-svc-4tj5d [477.996144ms]
Aug 18 18:56:51.183: INFO: Created: latency-svc-ddx87
Aug 18 18:56:51.202: INFO: Got endpoints: latency-svc-ddx87 [467.237097ms]
Aug 18 18:56:51.222: INFO: Created: latency-svc-5cwxs
Aug 18 18:56:51.238: INFO: Got endpoints: latency-svc-5cwxs [471.833105ms]
Aug 18 18:56:51.249: INFO: Created: latency-svc-lw47r
Aug 18 18:56:51.262: INFO: Got endpoints: latency-svc-lw47r [462.997238ms]
Aug 18 18:56:51.279: INFO: Created: latency-svc-5xbbz
Aug 18 18:56:51.294: INFO: Got endpoints: latency-svc-5xbbz [460.18897ms]
Aug 18 18:56:51.299: INFO: Created: latency-svc-szssz
Aug 18 18:56:51.317: INFO: Got endpoints: latency-svc-szssz [458.624162ms]
Aug 18 18:56:51.324: INFO: Created: latency-svc-srnk9
Aug 18 18:56:51.340: INFO: Got endpoints: latency-svc-srnk9 [452.505599ms]
Aug 18 18:56:51.354: INFO: Created: latency-svc-g6srt
Aug 18 18:56:51.369: INFO: Got endpoints: latency-svc-g6srt [445.49286ms]
Aug 18 18:56:51.383: INFO: Created: latency-svc-ghmjz
Aug 18 18:56:51.405: INFO: Got endpoints: latency-svc-ghmjz [447.775532ms]
Aug 18 18:56:51.411: INFO: Created: latency-svc-7fws7
Aug 18 18:56:51.437: INFO: Got endpoints: latency-svc-7fws7 [442.452649ms]
Aug 18 18:56:51.441: INFO: Created: latency-svc-n7hr9
Aug 18 18:56:51.465: INFO: Got endpoints: latency-svc-n7hr9 [424.311485ms]
Aug 18 18:56:51.470: INFO: Created: latency-svc-5nqf5
Aug 18 18:56:51.492: INFO: Got endpoints: latency-svc-5nqf5 [438.555732ms]
Aug 18 18:56:51.500: INFO: Created: latency-svc-9gzgd
Aug 18 18:56:51.517: INFO: Got endpoints: latency-svc-9gzgd [446.327367ms]
Aug 18 18:56:51.524: INFO: Created: latency-svc-68n2z
Aug 18 18:56:51.543: INFO: Got endpoints: latency-svc-68n2z [443.632475ms]
Aug 18 18:56:51.554: INFO: Created: latency-svc-mgxn5
Aug 18 18:56:51.572: INFO: Got endpoints: latency-svc-mgxn5 [445.240233ms]
Aug 18 18:56:51.582: INFO: Created: latency-svc-5f9q7
Aug 18 18:56:51.597: INFO: Got endpoints: latency-svc-5f9q7 [431.576348ms]
Aug 18 18:56:51.601: INFO: Created: latency-svc-7s64w
Aug 18 18:56:51.617: INFO: Got endpoints: latency-svc-7s64w [415.126453ms]
Aug 18 18:56:51.636: INFO: Created: latency-svc-p4fdb
Aug 18 18:56:51.657: INFO: Got endpoints: latency-svc-p4fdb [418.611728ms]
Aug 18 18:56:51.665: INFO: Created: latency-svc-26z5v
Aug 18 18:56:51.682: INFO: Got endpoints: latency-svc-26z5v [420.259217ms]
Aug 18 18:56:51.692: INFO: Created: latency-svc-g8lf6
Aug 18 18:56:51.708: INFO: Got endpoints: latency-svc-g8lf6 [413.814333ms]
Aug 18 18:56:51.717: INFO: Created: latency-svc-z5hmj
Aug 18 18:56:51.733: INFO: Got endpoints: latency-svc-z5hmj [415.510052ms]
Aug 18 18:56:51.748: INFO: Created: latency-svc-bqp88
Aug 18 18:56:51.764: INFO: Got endpoints: latency-svc-bqp88 [424.282303ms]
Aug 18 18:56:51.765: INFO: Created: latency-svc-bc58d
Aug 18 18:56:51.790: INFO: Got endpoints: latency-svc-bc58d [421.337228ms]
Aug 18 18:56:51.791: INFO: Created: latency-svc-jxr9l
Aug 18 18:56:51.809: INFO: Got endpoints: latency-svc-jxr9l [403.922492ms]
Aug 18 18:56:51.817: INFO: Created: latency-svc-bq7bm
Aug 18 18:56:51.828: INFO: Got endpoints: latency-svc-bq7bm [391.178871ms]
Aug 18 18:56:51.838: INFO: Created: latency-svc-22jxj
Aug 18 18:56:51.855: INFO: Got endpoints: latency-svc-22jxj [389.446688ms]
Aug 18 18:56:51.861: INFO: Created: latency-svc-pxk5z
Aug 18 18:56:51.877: INFO: Got endpoints: latency-svc-pxk5z [385.096211ms]
Aug 18 18:56:51.887: INFO: Created: latency-svc-w7g5d
Aug 18 18:56:51.908: INFO: Created: latency-svc-bbdvm
Aug 18 18:56:51.915: INFO: Got endpoints: latency-svc-w7g5d [397.8547ms]
Aug 18 18:56:51.925: INFO: Got endpoints: latency-svc-bbdvm [382.237195ms]
Aug 18 18:56:51.938: INFO: Created: latency-svc-t8fzl
Aug 18 18:56:51.956: INFO: Got endpoints: latency-svc-t8fzl [383.948201ms]
Aug 18 18:56:51.958: INFO: Created: latency-svc-4m2m4
Aug 18 18:56:51.975: INFO: Got endpoints: latency-svc-4m2m4 [377.728188ms]
Aug 18 18:56:51.985: INFO: Created: latency-svc-ctwgp
Aug 18 18:56:52.005: INFO: Got endpoints: latency-svc-ctwgp [387.829375ms]
Aug 18 18:56:52.011: INFO: Created: latency-svc-qnbmk
Aug 18 18:56:52.025: INFO: Got endpoints: latency-svc-qnbmk [368.473596ms]
Aug 18 18:56:52.034: INFO: Created: latency-svc-b9njx
Aug 18 18:56:52.047: INFO: Got endpoints: latency-svc-b9njx [364.53827ms]
Aug 18 18:56:52.056: INFO: Created: latency-svc-52bfs
Aug 18 18:56:52.074: INFO: Got endpoints: latency-svc-52bfs [365.231318ms]
Aug 18 18:56:52.078: INFO: Created: latency-svc-gm6h7
Aug 18 18:56:52.093: INFO: Got endpoints: latency-svc-gm6h7 [360.451879ms]
Aug 18 18:56:52.098: INFO: Created: latency-svc-6ttdp
Aug 18 18:56:52.114: INFO: Got endpoints: latency-svc-6ttdp [349.138836ms]
Aug 18 18:56:52.151: INFO: Created: latency-svc-8r8lq
Aug 18 18:56:52.152: INFO: Created: latency-svc-n9bf5
Aug 18 18:56:52.169: INFO: Got endpoints: latency-svc-8r8lq [360.340518ms]
Aug 18 18:56:52.170: INFO: Created: latency-svc-dqg2g
Aug 18 18:56:52.176: INFO: Got endpoints: latency-svc-n9bf5 [386.250521ms]
Aug 18 18:56:52.186: INFO: Got endpoints: latency-svc-dqg2g [357.948502ms]
Aug 18 18:56:52.193: INFO: Created: latency-svc-csc4z
Aug 18 18:56:52.210: INFO: Got endpoints: latency-svc-csc4z [355.029627ms]
Aug 18 18:56:52.220: INFO: Created: latency-svc-tl5rp
Aug 18 18:56:52.238: INFO: Got endpoints: latency-svc-tl5rp [361.440683ms]
Aug 18 18:56:52.251: INFO: Created: latency-svc-zbmgs
Aug 18 18:56:52.273: INFO: Got endpoints: latency-svc-zbmgs [358.188555ms]
Aug 18 18:56:52.283: INFO: Created: latency-svc-j8hcq
Aug 18 18:56:52.311: INFO: Got endpoints: latency-svc-j8hcq [385.138406ms]
Aug 18 18:56:52.311: INFO: Created: latency-svc-q4vh2
Aug 18 18:56:52.325: INFO: Got endpoints: latency-svc-q4vh2 [369.259085ms]
Aug 18 18:56:52.326: INFO: Latencies: [53.323621ms 65.433988ms 72.923765ms 102.689931ms 135.281292ms 159.873405ms 174.734445ms 203.198906ms 236.207578ms 261.23927ms 284.637615ms 318.230183ms 345.782276ms 349.138836ms 354.939941ms 355.029627ms 357.948502ms 358.188555ms 358.563449ms 360.340518ms 360.427599ms 360.451879ms 361.440683ms 361.792284ms 362.01574ms 362.602938ms 362.898791ms 363.479845ms 363.818602ms 363.834535ms 364.218394ms 364.53827ms 365.231318ms 365.607026ms 365.787472ms 366.703179ms 366.969353ms 368.052504ms 368.319411ms 368.473596ms 368.568993ms 369.259085ms 369.492115ms 370.669094ms 371.180425ms 371.471802ms 371.721237ms 372.069659ms 373.08678ms 374.701701ms 375.3773ms 375.380695ms 375.456772ms 375.783213ms 376.597704ms 377.728188ms 379.25677ms 379.676005ms 380.463043ms 380.579988ms 380.868884ms 382.237195ms 382.411766ms 383.948201ms 385.060246ms 385.096211ms 385.138406ms 386.250521ms 387.595749ms 387.829375ms 389.394036ms 389.446688ms 389.65898ms 389.986694ms 390.406348ms 391.178871ms 392.270156ms 393.123029ms 396.377244ms 397.8547ms 399.676548ms 403.922492ms 407.53638ms 413.814333ms 415.126453ms 415.510052ms 418.611728ms 420.259217ms 421.337228ms 424.282303ms 424.311485ms 429.813008ms 431.151785ms 431.576348ms 438.555732ms 439.78202ms 442.452649ms 443.353455ms 443.632475ms 445.240233ms 445.49286ms 446.327367ms 447.775532ms 452.424789ms 452.505599ms 454.626906ms 455.380508ms 457.265033ms 458.624162ms 460.18897ms 462.997238ms 465.096618ms 467.237097ms 471.833105ms 476.801008ms 477.251435ms 477.996144ms 492.220056ms 494.046373ms 496.831896ms 498.370289ms 498.737533ms 499.368873ms 499.871663ms 500.042788ms 500.444299ms 502.756389ms 504.145225ms 504.606255ms 506.400708ms 507.447037ms 507.928033ms 508.298007ms 508.298025ms 511.231803ms 515.066503ms 518.868069ms 519.637706ms 520.551646ms 520.687072ms 523.074735ms 523.157066ms 523.487181ms 526.656401ms 527.440123ms 531.613264ms 532.571458ms 536.002792ms 536.282942ms 540.510565ms 542.625459ms 543.059372ms 563.2187ms 566.278313ms 566.563386ms 572.573435ms 574.508918ms 584.206854ms 586.915136ms 588.715538ms 589.129255ms 592.273637ms 592.803105ms 593.433272ms 593.828027ms 598.752556ms 599.273459ms 599.617729ms 601.316051ms 601.932425ms 604.288351ms 609.65651ms 610.094118ms 616.432798ms 617.27663ms 617.809249ms 619.203503ms 620.182793ms 621.763972ms 623.454732ms 624.462903ms 624.58606ms 624.76128ms 625.747316ms 630.069765ms 792.213812ms 825.332253ms 856.323018ms 890.725936ms 895.513992ms 907.705812ms 907.801965ms 910.633776ms 914.322656ms 918.291307ms 918.981194ms 920.975734ms 922.737684ms 927.452132ms 930.707501ms]
Aug 18 18:56:52.326: INFO: 50 %ile: 445.49286ms
Aug 18 18:56:52.326: INFO: 90 %ile: 624.462903ms
Aug 18 18:56:52.326: INFO: 99 %ile: 927.452132ms
Aug 18 18:56:52.326: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:52.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8769" for this suite.

• [SLOW TEST:9.044 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":253,"skipped":4248,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:52.370: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 18 18:56:52.643: INFO: Waiting up to 5m0s for pod "pod-74fae8cb-ba1d-4904-8955-f785abc4bc42" in namespace "emptydir-7608" to be "success or failure"
Aug 18 18:56:52.657: INFO: Pod "pod-74fae8cb-ba1d-4904-8955-f785abc4bc42": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050304ms
Aug 18 18:56:54.671: INFO: Pod "pod-74fae8cb-ba1d-4904-8955-f785abc4bc42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027834884s
STEP: Saw pod success
Aug 18 18:56:54.671: INFO: Pod "pod-74fae8cb-ba1d-4904-8955-f785abc4bc42" satisfied condition "success or failure"
Aug 18 18:56:54.686: INFO: Trying to get logs from node 10.13.3.84 pod pod-74fae8cb-ba1d-4904-8955-f785abc4bc42 container test-container: <nil>
STEP: delete the pod
Aug 18 18:56:54.762: INFO: Waiting for pod pod-74fae8cb-ba1d-4904-8955-f785abc4bc42 to disappear
Aug 18 18:56:54.773: INFO: Pod pod-74fae8cb-ba1d-4904-8955-f785abc4bc42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:56:54.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7608" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":4249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:56:54.825: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 18 18:57:05.882: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0818 18:57:05.882338      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 18 18:57:05.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5991" for this suite.

• [SLOW TEST:11.101 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":255,"skipped":4289,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:57:05.926: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9036.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9036.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:57:10.535: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.558: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.582: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.603: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.672: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.692: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.712: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.732: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:10.777: INFO: Lookups using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local]

Aug 18 18:57:15.807: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:15.829: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:15.849: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:15.872: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:15.930: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:15.949: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:16.185: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:16.204: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:16.245: INFO: Lookups using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local]

Aug 18 18:57:20.798: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:20.816: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:20.831: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:20.849: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:20.905: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:20.968: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:20.988: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:21.011: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:21.278: INFO: Lookups using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local]

Aug 18 18:57:25.805: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.824: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.843: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.863: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.928: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.949: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.970: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:25.987: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:26.079: INFO: Lookups using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local]

Aug 18 18:57:30.798: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:30.815: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:30.833: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:30.856: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:31.125: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:31.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:31.166: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:31.190: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:31.227: INFO: Lookups using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local]

Aug 18 18:57:35.857: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:35.892: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:35.914: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:35.934: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:36.001: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:36.029: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:36.050: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:36.068: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local from pod dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3: the server could not find the requested resource (get pods dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3)
Aug 18 18:57:36.111: INFO: Lookups using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9036.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9036.svc.cluster.local jessie_udp@dns-test-service-2.dns-9036.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9036.svc.cluster.local]

Aug 18 18:57:41.001: INFO: DNS probes using dns-9036/dns-test-cc0cccec-3429-49fc-b51f-5c54472c0dc3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:57:41.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9036" for this suite.

• [SLOW TEST:35.278 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":256,"skipped":4295,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:57:41.205: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:57:41.483: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c7fc74a3-35b2-4c48-85a0-248db8ecb502" in namespace "security-context-test-6527" to be "success or failure"
Aug 18 18:57:41.496: INFO: Pod "busybox-privileged-false-c7fc74a3-35b2-4c48-85a0-248db8ecb502": Phase="Pending", Reason="", readiness=false. Elapsed: 12.668623ms
Aug 18 18:57:43.513: INFO: Pod "busybox-privileged-false-c7fc74a3-35b2-4c48-85a0-248db8ecb502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029931651s
Aug 18 18:57:43.513: INFO: Pod "busybox-privileged-false-c7fc74a3-35b2-4c48-85a0-248db8ecb502" satisfied condition "success or failure"
Aug 18 18:57:43.576: INFO: Got logs for pod "busybox-privileged-false-c7fc74a3-35b2-4c48-85a0-248db8ecb502": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:57:43.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6527" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4305,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:57:43.641: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-578.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-578.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-578.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-578.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-578.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-578.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-578.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-578.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-578.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 231.73.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.73.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.73.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.73.231_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-578.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-578.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-578.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-578.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-578.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-578.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-578.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-578.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-578.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-578.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 231.73.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.73.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.73.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.73.231_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 18 18:57:48.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-578.svc.cluster.local from pod dns-578/dns-test-c8390d06-1c93-45b5-8e63-658c68a20784: the server could not find the requested resource (get pods dns-test-c8390d06-1c93-45b5-8e63-658c68a20784)
Aug 18 18:57:48.339: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-578.svc.cluster.local from pod dns-578/dns-test-c8390d06-1c93-45b5-8e63-658c68a20784: the server could not find the requested resource (get pods dns-test-c8390d06-1c93-45b5-8e63-658c68a20784)
Aug 18 18:57:48.461: INFO: Lookups using dns-578/dns-test-c8390d06-1c93-45b5-8e63-658c68a20784 failed for: [jessie_udp@_http._tcp.dns-test-service.dns-578.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-578.svc.cluster.local]

Aug 18 18:57:54.137: INFO: DNS probes using dns-578/dns-test-c8390d06-1c93-45b5-8e63-658c68a20784 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:57:54.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-578" for this suite.

• [SLOW TEST:11.105 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":258,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:57:54.748: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:58:08.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1067" for this suite.

• [SLOW TEST:13.642 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":259,"skipped":4339,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:58:08.390: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:58:10.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7597" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4343,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:58:10.900: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 18:58:11.819: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 18:58:14.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:58:15.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4336" for this suite.
STEP: Destroying namespace "webhook-4336-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":261,"skipped":4344,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:58:15.806: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 18 18:58:16.355: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58798 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 18 18:58:16.355: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58798 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 18 18:58:26.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58850 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 18 18:58:26.393: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58850 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 18 18:58:36.424: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58875 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 18 18:58:36.425: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58875 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 18 18:58:46.468: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58901 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 18 18:58:46.468: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-a 6d71e69e-ee89-4cf2-a9df-ba5798d0328a 58901 0 2020-08-18 18:58:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 18 18:58:56.499: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-b 4156d60f-ec0a-42fa-9969-c8a7b7d0962a 58941 0 2020-08-18 18:58:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 18 18:58:56.499: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-b 4156d60f-ec0a-42fa-9969-c8a7b7d0962a 58941 0 2020-08-18 18:58:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 18 18:59:06.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-b 4156d60f-ec0a-42fa-9969-c8a7b7d0962a 58967 0 2020-08-18 18:58:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 18 18:59:06.535: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8705 /api/v1/namespaces/watch-8705/configmaps/e2e-watch-test-configmap-b 4156d60f-ec0a-42fa-9969-c8a7b7d0962a 58967 0 2020-08-18 18:58:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8705" for this suite.

• [SLOW TEST:60.791 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":262,"skipped":4347,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:16.597: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Aug 18 18:59:16.875: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 18 18:59:16.923: INFO: Waiting for terminating namespaces to be deleted...
Aug 18 18:59:16.940: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.114 before test
Aug 18 18:59:17.054: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-9fl8l from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 18 18:59:17.054: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:59:17.054: INFO: ibm-master-proxy-static-10.13.3.114 from kube-system started at 2020-08-18 15:51:09 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:59:17.054: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:59:17.054: INFO: metrics-server-797d668946-ltqcd from kube-system started at 2020-08-18 15:52:24 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container metrics-server ready: true, restart count 0
Aug 18 18:59:17.054: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 18 18:59:17.054: INFO: ibm-keepalived-watcher-q59v8 from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:59:17.054: INFO: calico-kube-controllers-5754cfb59d-8hh6k from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 18 18:59:17.054: INFO: coredns-6567db4fff-6tjgz from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:59:17.054: INFO: coredns-6567db4fff-78ghm from kube-system started at 2020-08-18 17:45:40 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:59:17.054: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-k9jqb from kube-system started at 2020-08-18 15:56:04 +0000 UTC (4 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:59:17.054: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:59:17.054: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:59:17.054: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:59:17.054: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-pw6pj from ibm-system started at 2020-08-18 17:41:38 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:59:17.054: INFO: vpn-f66c45467-kh4hm from kube-system started at 2020-08-18 16:16:37 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container vpn ready: true, restart count 0
Aug 18 18:59:17.054: INFO: calico-node-2ngjr from kube-system started at 2020-08-18 15:51:11 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.054: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:59:17.054: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.115 before test
Aug 18 18:59:17.201: INFO: ibm-file-plugin-ff7c989f9-fsrmg from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 18 18:59:17.201: INFO: sonobuoy-e2e-job-715a84e433104f63 from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container e2e ready: true, restart count 0
Aug 18 18:59:17.201: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 18 18:59:17.201: INFO: public-crbstv947f0fav3stm5itg-alb1-7cc5bbb5bb-rb2hb from kube-system started at 2020-08-18 17:41:38 +0000 UTC (4 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 18 18:59:17.201: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 18 18:59:17.201: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 18 18:59:17.201: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 18 18:59:17.201: INFO: ibm-master-proxy-static-10.13.3.115 from kube-system started at 2020-08-18 15:51:10 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:59:17.201: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:59:17.201: INFO: calico-node-j6sx8 from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container calico-node ready: true, restart count 0
Aug 18 18:59:17.201: INFO: dashboard-metrics-scraper-5789d44f58-nsshl from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Aug 18 18:59:17.201: INFO: catalog-operator-67646bfcdb-77lhs from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 18 18:59:17.201: INFO: olm-operator-787498c9b7-9cmpc from ibm-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container olm-operator ready: true, restart count 0
Aug 18 18:59:17.201: INFO: ibm-cloud-provider-ip-149-81-70-235-7cd5779b89-g6ck9 from ibm-system started at 2020-08-18 15:52:58 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container ibm-cloud-provider-ip-149-81-70-235 ready: true, restart count 0
Aug 18 18:59:17.201: INFO: coredns-6567db4fff-wxj8p from kube-system started at 2020-08-18 16:17:00 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container coredns ready: true, restart count 0
Aug 18 18:59:17.201: INFO: ibm-keepalived-watcher-zktgc from kube-system started at 2020-08-18 15:51:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:59:17.201: INFO: kubernetes-dashboard-984c5c57-bl4dx from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 18 18:59:17.201: INFO: coredns-autoscaler-649976fbf4-q69hh from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container autoscaler ready: true, restart count 0
Aug 18 18:59:17.201: INFO: sonobuoy from sonobuoy started at 2020-08-18 17:39:12 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 18 18:59:17.201: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-cpwhn from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 18 18:59:17.201: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:59:17.201: INFO: ibm-storage-watcher-56b6fd445c-dqt8p from kube-system started at 2020-08-18 15:51:30 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.201: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 18 18:59:17.201: INFO: 
Logging pods the kubelet thinks is on node 10.13.3.84 before test
Aug 18 18:59:17.753: INFO: ibm-master-proxy-static-10.13.3.84 from kube-system started at 2020-08-18 15:51:51 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.753: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 18 18:59:17.753: INFO: 	Container pause ready: true, restart count 0
Aug 18 18:59:17.753: INFO: ibm-keepalived-watcher-228kf from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.753: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 18 18:59:17.753: INFO: addon-catalog-source-62ppj from ibm-system started at 2020-08-18 15:52:32 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.753: INFO: 	Container configmap-registry-server ready: true, restart count 0
Aug 18 18:59:17.753: INFO: sonobuoy-systemd-logs-daemon-set-42a3e47b88254098-kdbbz from sonobuoy started at 2020-08-18 17:39:19 +0000 UTC (2 container statuses recorded)
Aug 18 18:59:17.753: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 18 18:59:17.753: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 18 18:59:17.753: INFO: calico-node-b68l4 from kube-system started at 2020-08-18 15:51:52 +0000 UTC (1 container statuses recorded)
Aug 18 18:59:17.753: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-12cc5fca-694a-4bbb-ad48-ec0c869c013f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-12cc5fca-694a-4bbb-ad48-ec0c869c013f off the node 10.13.3.84
STEP: verifying the node doesn't have the label kubernetes.io/e2e-12cc5fca-694a-4bbb-ad48-ec0c869c013f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:32.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6752" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:15.841 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":263,"skipped":4348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:32.438: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-5547
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5547
STEP: Deleting pre-stop pod
Aug 18 18:59:41.880: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:41.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5547" for this suite.

• [SLOW TEST:9.524 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":264,"skipped":4371,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:41.962: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 18:59:42.223: INFO: Creating deployment "test-recreate-deployment"
Aug 18 18:59:42.237: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 18 18:59:42.267: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Aug 18 18:59:44.301: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 18 18:59:44.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373982, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373982, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373982, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733373982, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 18 18:59:46.749: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 18 18:59:46.779: INFO: Updating deployment test-recreate-deployment
Aug 18 18:59:46.780: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Aug 18 18:59:47.272: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6502 /apis/apps/v1/namespaces/deployment-6502/deployments/test-recreate-deployment 05762fc8-1e95-4d7a-ac61-bbe3e9d75f46 59279 2 2020-08-18 18:59:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038ea608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-18 18:59:47 +0000 UTC,LastTransitionTime:2020-08-18 18:59:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-08-18 18:59:47 +0000 UTC,LastTransitionTime:2020-08-18 18:59:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 18 18:59:47.293: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6502 /apis/apps/v1/namespaces/deployment-6502/replicasets/test-recreate-deployment-5f94c574ff 68ecde99-b4c3-4249-9153-db7e3e769c51 59272 1 2020-08-18 18:59:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 05762fc8-1e95-4d7a-ac61-bbe3e9d75f46 0xc0038ea9b7 0xc0038ea9b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038eaa18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:59:47.293: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 18 18:59:47.294: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-6502 /apis/apps/v1/namespaces/deployment-6502/replicasets/test-recreate-deployment-799c574856 1a526412-c653-47bd-923c-3fc089627d24 59264 2 2020-08-18 18:59:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 05762fc8-1e95-4d7a-ac61-bbe3e9d75f46 0xc0038eaa87 0xc0038eaa88}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038eaaf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 18 18:59:47.309: INFO: Pod "test-recreate-deployment-5f94c574ff-8ts2h" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-8ts2h test-recreate-deployment-5f94c574ff- deployment-6502 /api/v1/namespaces/deployment-6502/pods/test-recreate-deployment-5f94c574ff-8ts2h f8c803fa-cb34-49b8-9204-a71ca648d8a5 59274 0 2020-08-18 18:59:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 68ecde99-b4c3-4249-9153-db7e3e769c51 0xc0038eaf97 0xc0038eaf98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zrzjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zrzjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zrzjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.13.3.84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:59:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:59:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:59:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-18 18:59:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.13.3.84,PodIP:,StartTime:2020-08-18 18:59:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:47.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6502" for this suite.

• [SLOW TEST:5.399 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":265,"skipped":4376,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:47.361: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 18 18:59:52.484: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3772 pod-service-account-47354241-1239-41c4-a2ea-d81a92410d42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 18 18:59:52.852: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3772 pod-service-account-47354241-1239-41c4-a2ea-d81a92410d42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 18 18:59:53.191: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3772 pod-service-account-47354241-1239-41c4-a2ea-d81a92410d42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:53.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3772" for this suite.

• [SLOW TEST:6.165 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":266,"skipped":4381,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:53.527: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0818 18:59:55.346405      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 18 18:59:55.346: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:55.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8729" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":267,"skipped":4397,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:55.403: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 18 18:59:56.048: INFO: Waiting up to 5m0s for pod "pod-2ee2697d-f636-4bac-9e56-2c823d12cc89" in namespace "emptydir-4416" to be "success or failure"
Aug 18 18:59:56.062: INFO: Pod "pod-2ee2697d-f636-4bac-9e56-2c823d12cc89": Phase="Pending", Reason="", readiness=false. Elapsed: 14.100269ms
Aug 18 18:59:58.076: INFO: Pod "pod-2ee2697d-f636-4bac-9e56-2c823d12cc89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028018501s
STEP: Saw pod success
Aug 18 18:59:58.076: INFO: Pod "pod-2ee2697d-f636-4bac-9e56-2c823d12cc89" satisfied condition "success or failure"
Aug 18 18:59:58.307: INFO: Trying to get logs from node 10.13.3.84 pod pod-2ee2697d-f636-4bac-9e56-2c823d12cc89 container test-container: <nil>
STEP: delete the pod
Aug 18 18:59:58.401: INFO: Waiting for pod pod-2ee2697d-f636-4bac-9e56-2c823d12cc89 to disappear
Aug 18 18:59:58.415: INFO: Pod pod-2ee2697d-f636-4bac-9e56-2c823d12cc89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 18:59:58.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4416" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":268,"skipped":4402,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 18:59:58.465: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Aug 18 18:59:58.850: INFO: Waiting up to 5m0s for pod "client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33" in namespace "containers-634" to be "success or failure"
Aug 18 18:59:58.865: INFO: Pod "client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33": Phase="Pending", Reason="", readiness=false. Elapsed: 14.820119ms
Aug 18 19:00:00.879: INFO: Pod "client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33": Phase="Running", Reason="", readiness=true. Elapsed: 2.028925541s
Aug 18 19:00:02.897: INFO: Pod "client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046538813s
STEP: Saw pod success
Aug 18 19:00:02.897: INFO: Pod "client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33" satisfied condition "success or failure"
Aug 18 19:00:02.912: INFO: Trying to get logs from node 10.13.3.84 pod client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33 container test-container: <nil>
STEP: delete the pod
Aug 18 19:00:02.999: INFO: Waiting for pod client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33 to disappear
Aug 18 19:00:03.014: INFO: Pod client-containers-8bb5c15f-9934-4303-b3cc-541757e54d33 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:00:03.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-634" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":269,"skipped":4419,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:00:03.082: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7692
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Aug 18 19:00:03.358: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:00:26.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7692" for this suite.

• [SLOW TEST:23.112 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":270,"skipped":4425,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:00:26.196: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-2264899a-66b5-4dba-8e48-4cfc9b5e7209
STEP: Creating a pod to test consume configMaps
Aug 18 19:00:26.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532" in namespace "projected-7646" to be "success or failure"
Aug 18 19:00:26.524: INFO: Pod "pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532": Phase="Pending", Reason="", readiness=false. Elapsed: 14.786634ms
Aug 18 19:00:28.540: INFO: Pod "pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031481077s
Aug 18 19:00:30.557: INFO: Pod "pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048283831s
STEP: Saw pod success
Aug 18 19:00:30.557: INFO: Pod "pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532" satisfied condition "success or failure"
Aug 18 19:00:30.575: INFO: Trying to get logs from node 10.13.3.84 pod pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 18 19:00:30.659: INFO: Waiting for pod pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532 to disappear
Aug 18 19:00:30.674: INFO: Pod pod-projected-configmaps-1edd4cd8-2c9b-4bab-994c-6e1477031532 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:00:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7646" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":271,"skipped":4425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:00:30.738: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Aug 18 19:00:31.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603" in namespace "projected-3518" to be "success or failure"
Aug 18 19:00:31.345: INFO: Pod "downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603": Phase="Pending", Reason="", readiness=false. Elapsed: 24.925993ms
Aug 18 19:00:33.365: INFO: Pod "downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044272165s
Aug 18 19:00:35.380: INFO: Pod "downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059821204s
STEP: Saw pod success
Aug 18 19:00:35.380: INFO: Pod "downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603" satisfied condition "success or failure"
Aug 18 19:00:35.395: INFO: Trying to get logs from node 10.13.3.84 pod downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603 container client-container: <nil>
STEP: delete the pod
Aug 18 19:00:35.488: INFO: Waiting for pod downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603 to disappear
Aug 18 19:00:35.502: INFO: Pod downwardapi-volume-d232d638-a7dd-420f-be65-15187506b603 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:00:35.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3518" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4457,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:00:35.565: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-38
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-3aff52d3-b47f-4ce4-9817-6714f67ce406
STEP: Creating a pod to test consume secrets
Aug 18 19:00:35.901: INFO: Waiting up to 5m0s for pod "pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764" in namespace "secrets-38" to be "success or failure"
Aug 18 19:00:35.915: INFO: Pod "pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764": Phase="Pending", Reason="", readiness=false. Elapsed: 13.621437ms
Aug 18 19:00:38.143: INFO: Pod "pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.242183962s
STEP: Saw pod success
Aug 18 19:00:38.143: INFO: Pod "pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764" satisfied condition "success or failure"
Aug 18 19:00:38.160: INFO: Trying to get logs from node 10.13.3.84 pod pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764 container secret-volume-test: <nil>
STEP: delete the pod
Aug 18 19:00:38.259: INFO: Waiting for pod pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764 to disappear
Aug 18 19:00:38.273: INFO: Pod pod-secrets-c9b68eaf-a878-41c8-8d55-f8427ca47764 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:00:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-38" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4471,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:00:38.343: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 18 19:00:39.897: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 18 19:00:41.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733374039, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733374039, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63733374039, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63733374039, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 18 19:00:45.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 19:00:45.041: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8369-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:00:51.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9745" for this suite.
STEP: Destroying namespace "webhook-9745-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":274,"skipped":4473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:00:52.825: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 18 19:00:53.410: INFO: Number of nodes with available pods: 0
Aug 18 19:00:53.410: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 19:00:54.448: INFO: Number of nodes with available pods: 0
Aug 18 19:00:54.448: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 19:00:55.442: INFO: Number of nodes with available pods: 1
Aug 18 19:00:55.442: INFO: Node 10.13.3.114 is running more than one daemon pod
Aug 18 19:00:56.443: INFO: Number of nodes with available pods: 3
Aug 18 19:00:56.443: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 18 19:00:56.534: INFO: Number of nodes with available pods: 2
Aug 18 19:00:56.534: INFO: Node 10.13.3.115 is running more than one daemon pod
Aug 18 19:00:57.568: INFO: Number of nodes with available pods: 2
Aug 18 19:00:57.568: INFO: Node 10.13.3.115 is running more than one daemon pod
Aug 18 19:00:58.572: INFO: Number of nodes with available pods: 3
Aug 18 19:00:58.572: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7215, will wait for the garbage collector to delete the pods
Aug 18 19:00:58.734: INFO: Deleting DaemonSet.extensions daemon-set took: 55.242499ms
Aug 18 19:00:58.934: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.383446ms
Aug 18 19:01:11.449: INFO: Number of nodes with available pods: 0
Aug 18 19:01:11.449: INFO: Number of running nodes: 0, number of available pods: 0
Aug 18 19:01:11.470: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7215/daemonsets","resourceVersion":"60099"},"items":null}

Aug 18 19:01:11.483: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7215/pods","resourceVersion":"60099"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:01:11.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7215" for this suite.

• [SLOW TEST:18.781 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":275,"skipped":4507,"failed":0}
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:01:11.605: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:01:14.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1870" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":276,"skipped":4507,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:01:14.406: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8418
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-8418
Aug 18 19:01:14.771: INFO: Found 0 stateful pods, waiting for 1
Aug 18 19:01:24.787: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Aug 18 19:01:24.884: INFO: Deleting all statefulset in ns statefulset-8418
Aug 18 19:01:24.901: INFO: Scaling statefulset ss to 0
Aug 18 19:01:34.965: INFO: Waiting for statefulset status.replicas updated to 0
Aug 18 19:01:34.981: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:01:35.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8418" for this suite.

• [SLOW TEST:20.723 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":277,"skipped":4521,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:01:35.129: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 19:01:35.450: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-48dd8004-da2d-4b2a-9615-2702b43a3ba1" in namespace "security-context-test-5125" to be "success or failure"
Aug 18 19:01:35.464: INFO: Pod "busybox-readonly-false-48dd8004-da2d-4b2a-9615-2702b43a3ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.05982ms
Aug 18 19:01:37.479: INFO: Pod "busybox-readonly-false-48dd8004-da2d-4b2a-9615-2702b43a3ba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028597869s
Aug 18 19:01:39.492: INFO: Pod "busybox-readonly-false-48dd8004-da2d-4b2a-9615-2702b43a3ba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041893122s
Aug 18 19:01:39.492: INFO: Pod "busybox-readonly-false-48dd8004-da2d-4b2a-9615-2702b43a3ba1" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:01:39.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5125" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:01:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4151
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4151
I0818 19:01:39.926055      26 runners.go:189] Created replication controller with name: externalname-service, namespace: services-4151, replica count: 2
Aug 18 19:01:42.976: INFO: Creating new exec pod
I0818 19:01:42.976528      26 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 18 19:01:46.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-4151 execpod9jdgl -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 18 19:01:46.330: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 18 19:01:46.330: INFO: stdout: ""
Aug 18 19:01:46.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-222924798 exec --namespace=services-4151 execpod9jdgl -- /bin/sh -x -c nc -zv -t -w 2 172.21.47.28 80'
Aug 18 19:01:46.577: INFO: stderr: "+ nc -zv -t -w 2 172.21.47.28 80\nConnection to 172.21.47.28 80 port [tcp/http] succeeded!\n"
Aug 18 19:01:46.577: INFO: stdout: ""
Aug 18 19:01:46.577: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:01:46.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4151" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.232 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":279,"skipped":4548,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Aug 18 19:01:46.772: INFO: >>> kubeConfig: /tmp/kubeconfig-222924798
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Aug 18 19:01:47.115: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"28646868-0c4e-4fdd-8df7-ba0572bf50e8", Controller:(*bool)(0xc0063424fe), BlockOwnerDeletion:(*bool)(0xc0063424ff)}}
Aug 18 19:01:47.131: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5702bce3-f747-42c2-a458-42842288175e", Controller:(*bool)(0xc0063d68e6), BlockOwnerDeletion:(*bool)(0xc0063d68e7)}}
Aug 18 19:01:47.150: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a2176cef-2432-4694-b3be-e5d3d3a5a1eb", Controller:(*bool)(0xc006342786), BlockOwnerDeletion:(*bool)(0xc006342787)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Aug 18 19:01:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8219" for this suite.

• [SLOW TEST:5.865 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":280,"skipped":4563,"failed":0}
SAug 18 19:01:52.638: INFO: Running AfterSuite actions on all nodes
Aug 18 19:01:52.638: INFO: Running AfterSuite actions on node 1
Aug 18 19:01:52.638: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4564,"failed":0}

Ran 280 of 4844 Specs in 4928.243 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4564 Skipped
PASS

Ginkgo ran 1 suite in 1h22m9.78951753s
Test Suite Passed
